{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669a3790",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab provides hands-on practice building a complete **Search Engine** from scratch using **TF-IDF (Term Frequency-Inverse Document Frequency)**, one of the most common techniques in Information Retrieval. You'll work with real datasets, implement TF-IDF vectorization, measure document similarity using cosine similarity, and build a functional search engine that can retrieve and rank documents based on user queries. This lab demonstrates the practical application of IR concepts in building production-ready search systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54002715",
   "metadata": {},
   "source": [
    "> A 2015 survey showed that 83% of text-based recommender systems in digital libraries used TF-IDF.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Setup and Imports** - Installing dependencies and importing libraries\n",
    "2. **Dataset Loading** - Loading the 20 Newsgroups dataset from scikit-learn\n",
    "3. **Text Preprocessing** - Preparing documents for vectorization\n",
    "4. **TF-IDF Vectorization** - Converting documents and queries into numerical vectors\n",
    "5. **Building the Search Engine**:\n",
    "   - **Retrieval**: Finding the most similar documents to a query using cosine similarity\n",
    "   - **Ranking**: Ordering documents by relevance score\n",
    "   - **Classification**: Classifying queries into one of the 20 categories\n",
    "6. **Testing the Search Engine** - Querying and evaluating results\n",
    "7. **Understanding Results** - Interpreting search results and similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a535e",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Recognize the term-document matrix produced by the TF-IDF algorithm\n",
    "- Understand how similarity is measured between text vectors (cosine similarity)\n",
    "- **Work with real datasets**.\n",
    "- **Implement a TF-IDF-based search engine** using:\n",
    "  - Scikit-learn's `TfidfVectorizer`\n",
    "  - Cosine similarity for document ranking\n",
    "  - Sparse matrices for efficient storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff080ff",
   "metadata": {},
   "source": [
    "## Glossary of Terms\n",
    "\n",
    "**Information Retrieval (IR)**: The task of finding information (usually documents) that satisfies an information need from within large collections.\n",
    "\n",
    "**Corpus**: A collection of documents. In IR, this is the entire set of documents we search through.\n",
    "\n",
    "**Query**: A user's information need expressed in natural language (e.g., \"What is machine learning?\").\n",
    "\n",
    "**Document**: A unit of information in the corpus (e.g., a web page, article, or text passage).\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)**: A numerical statistic that reflects how important a word is to a document in a collection. As you learned in the vectorization lesson:\n",
    "- **TF (Term Frequency)**: How often a term appears in a document\n",
    "- **IDF (Inverse Document Frequency)**: How rare or common a term is across the entire corpus\n",
    "- **TF-IDF**: TF Ã— IDF, giving higher weight to terms that are frequent in a document but rare in the corpus\n",
    "\n",
    "**Vector Space Model**: A model where documents and queries are represented as vectors in a high-dimensional space. Similarity is measured using the angle between vectors (cosine similarity).\n",
    "\n",
    "**Cosine Similarity**: A measure of similarity between two vectors. It measures the cosine of the angle between them, ranging from -1 to 1 (or 0 to 1 for non-negative vectors like TF-IDF).\n",
    "\n",
    "**Sparse Matrix**: A matrix where most elements are zero. TF-IDF vectors are typically sparse because most words don't appear in most documents.\n",
    "\n",
    "**Retrieval**: The process of finding and ranking documents in response to a query.\n",
    "\n",
    "**Ranking**: Ordering retrieved documents by their relevance score (highest to lowest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451be65",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [20 Newsgroups Dataset](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)\n",
    "- [Scikit-learn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "- [Scikit-learn cosine_similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73007603",
   "metadata": {},
   "source": [
    "## The Complete Pipeline\n",
    "\n",
    "1. **Dataset Loading**: Load the 20 Newsgroups dataset from scikit-learn\n",
    "2. **Text Preprocessing**: Prepare documents for vectorization (as learned in previous lessons)\n",
    "3. **TF-IDF Vectorization**: Convert documents and queries into numerical vectors\n",
    "4. Model:\n",
    "   1. **Retrieval**: Find the most similar documents to a query using **cosine similarity**\n",
    "   2. **Classification**: Classify a query into one of the 20 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e643e",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "We group imports by category following Python best practices. All libraries used here are part of the standard scikit-learn ecosystem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install joblib==1.5.3 numpy==1.26.4 pandas==2.3.3 scikit-learn==1.8.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3172c8",
   "metadata": {},
   "source": [
    "## Loading the 20 Newsgroups Dataset\n",
    "\n",
    "We'll use scikit-learn's **20 Newsgroups dataset**, a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. This is a classic dataset for text classification and information retrieval experiments.\n",
    "\n",
    "**About the Dataset:**\n",
    "- **20 categories** of newsgroups (e.g., comp.graphics, rec.sport.baseball, sci.med)\n",
    "- Each document is a newsgroup post with subject and body text\n",
    "- Documents are organized by topic, which we'll use to create relevance judgments\n",
    "\n",
    "We'll load the 20 Newsgroups dataset using scikit-learn's `fetch_20newsgroups` function. This dataset contains newsgroup posts organized into 20 categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6705ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_20newsgroups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 20 Newsgroups dataset\n",
    "# We'll use the training set as our document collection\n",
    "# remove=('headers', 'footers', 'quotes') removes metadata to focus on content\n",
    "print(\"Loading 20 Newsgroups dataset...\")\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de287d",
   "metadata": {},
   "source": [
    "### Exercise 1: Explore the Dataset\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- The distribution of documents across categories\n",
    "- The content of sample documents\n",
    "- Notice that documents are organized by topic (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'text': newsgroups.data,\n",
    "    'category': newsgroups.target,\n",
    "    'category_name': [newsgroups.target_names[newsgroups.target[i]] for i in range(len(newsgroups.target))]\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e998e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145aece",
   "metadata": {},
   "source": [
    "### Drop columns and keep `text` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f92865",
   "metadata": {},
   "source": [
    "## Building the Search Engine\n",
    "\n",
    "Now we'll build a TF-IDF-based search engine. The process involves:\n",
    "\n",
    "1. **Creating TF-IDF vectors** for all documents\n",
    "2. **Implementing a retrieval function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c31e6",
   "metadata": {},
   "source": [
    "### How TF-IDF enables retrieval\n",
    "\n",
    "- TF-IDF weights words by their importance: rare words that appear frequently in a document get high scores\n",
    "- When a user searches for \"machine learning\", documents with high TF-IDF scores for those terms are likely relevant\n",
    "- The query is also converted to a TF-IDF vector, then compared to all document vectors\n",
    "\n",
    "**Cosine Similarity for Document Matching**\n",
    "\n",
    "To find relevant documents, we need to measure **similarity** between the query vector and document vectors. \n",
    "\n",
    "**Cosine Similarity** measures the angle between two vectors:\n",
    "- It ranges from -1 to 1 (or 0 to 1 for non-negative vectors like TF-IDF)\n",
    "- **1.0** = vectors point in the same direction (very similar)\n",
    "- **0.0** = vectors are perpendicular (no similarity)\n",
    "- **-1.0** = vectors point in opposite directions (very dissimilar)\n",
    "\n",
    "**Why cosine similarity?**\n",
    "- It measures similarity in **direction**, not magnitude\n",
    "- A long document and a short document about the same topic will have similar directions (high cosine similarity)\n",
    "- It's robust to document length differences\n",
    "- Works well with sparse TF-IDF vectors\n",
    "\n",
    "**The retrieval process:**\n",
    "1. Convert query to TF-IDF vector (using the same vocabulary as documents)\n",
    "2. Compute cosine similarity between query vector and all document vectors\n",
    "3. Rank documents by similarity score (highest first)\n",
    "4. Return top-k most similar documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa43bc",
   "metadata": {},
   "source": [
    "### Task 1: Creating TF-IDF vectors for all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create TF-IDF vectors for all documents\n",
    "# Initialize the vectorizer\n",
    "# We use default settings, but you can customize:\n",
    "# - max_features: limit vocabulary size\n",
    "# - stop_words: remove common words ('english')\n",
    "# - ngram_range: use unigrams and bigrams\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,      # Convert to lowercase\n",
    "    stop_words='english', # Remove English stop words\n",
    "    max_features=5000,   # Limit vocabulary to top 5000 terms\n",
    "    ngram_range=(1, 2)   # Use both unigrams and bigrams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a74b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on all documents and transform them\n",
    "# This learns the vocabulary and IDF from the corpus\n",
    "document_vectors = vectorizer.fit_transform(df['text'])\n",
    "document_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ee14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Document vectors shape: {document_vectors.shape}\")\n",
    "print(f\"  - {document_vectors.shape[0]} documents\")\n",
    "print(f\"  - {document_vectors.shape[1]} features (terms in vocabulary)\")\n",
    "print(\"\\nThis is a sparse matrix. Let's check sparsity:\")\n",
    "print(f\"  - Non-zero elements: {document_vectors.nnz:,}\")\n",
    "print(f\"  - Total elements: {document_vectors.shape[0] * document_vectors.shape[1]:,}\")\n",
    "print(f\"  - Sparsity: {(1 - document_vectors.nnz / (document_vectors.shape[0] * document_vectors.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da4cf8",
   "metadata": {},
   "source": [
    "> Notice how the sparse matrix efficiently stores only non-zero values. This is why TF-IDF scales well to large document collections.\n",
    "\n",
    "- Document vectors shape: (~11,000, 5000) - thousands of documents, 5000 features\n",
    "- High sparsity (typically 95-99%) - most values are zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70896f34",
   "metadata": {},
   "source": [
    "Notice how there is a lot of junk in the text, when we print the vocabulary from `200:400`, this may or may not be useful, depending on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some vocabulary terms\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(vocab[200:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d87d0f",
   "metadata": {},
   "source": [
    "### Task 2: Implementing a retrieval function\n",
    "\n",
    "- Converts a query to a TF-IDF vector\n",
    "- Computes cosine similarity with all document vectors\n",
    "- Returns the top-k most similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Implement the retrieval function\n",
    "def retrieve_documents(query_text, top_k=10):\n",
    "    # Transform query to TF-IDF vector using the same vectorizer\n",
    "    query_vector = vectorizer.transform([query_text])\n",
    "    \n",
    "    # Compute cosine similarity between query and all documents\n",
    "    # cosine_similarity returns a matrix of shape (1, num_documents)\n",
    "    similarities = cosine_similarity(query_vector, document_vectors).flatten()\n",
    "    \n",
    "    # Get indices of top-k documents (sorted by similarity, descending)\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # Return a dataframe with the top-k results\n",
    "    df_results = df.iloc[top_indices].copy()\n",
    "    df_results['similarity'] = similarities[top_indices]\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ded750",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = retrieve_documents(query_text='computer', top_k=3)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed705f",
   "metadata": {},
   "source": [
    "> **Note:** how the top three results are all related to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6035e97",
   "metadata": {},
   "source": [
    "## **Student Exercise**: build a search engine on the `CVs` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d706a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae1ef66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3ae72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03cf2d36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036631f",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4134f187",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- **TF-IDF** is a simple but effective method for information retrieval\n",
    "- **Sparse matrices** make TF-IDF scalable to large document collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428874e6",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "- Explore other scikit-learn datasets (e.g., `fetch_20newsgroups` with different subsets)\n",
    "- Try advanced techniques like BM25 (can be implemented with sklearn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
