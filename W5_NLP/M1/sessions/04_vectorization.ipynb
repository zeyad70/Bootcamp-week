{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9862c4",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers **vectorization**, the process of converting text into numerical features that machine learning models can process. We explore tokenization (defining the smallest units of text), n-grams (unigrams, bigrams, trigrams), and how to use scikit-learn's vectorizer classes to transform text into numerical representations. Understanding sparse matrices and their importance for text data is crucial for efficient text processing in NLP pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47915be0",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understand why we need to convert text into numerical features\n",
    "- Understand the smallest unit of meaning (token); and Unigrams, Bigrams, Trigrams, etc.\n",
    "- Learn how to vectorize text using the vectorizer classes in `scikit-learn`\n",
    "- Recognize sparse matrices and why they are essential for text data\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Tokenization** - Defining the smallest units of text processing\n",
    "2. **Collocation and N-grams** - Unigrams, bigrams, trigrams, and word sequences\n",
    "3. **Bag of Words (BoW)** - Simple word counting approach\n",
    "4. **TF-IDF (Term Frequency-Inverse Document Frequency)** - Weighted word importance\n",
    "5. **Scikit-learn Vectorizers** - Using `CountVectorizer` and `TfidfVectorizer`\n",
    "6. **Sparse Matrices** - Understanding why sparse representations are essential for text data\n",
    "7. **Practical Examples** - Vectorizing real text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy==1.26.4 pandas==2.3.3 scikit-learn==1.8.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaac155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "# (none needed for this notebook)\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771be55b",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Before dealing with text, we have to first define what a *word* is.\n",
    "\n",
    "**Tokenization:** segment text into its smallest units of processing (atoms).\n",
    "\n",
    "Tokens can be words, sub-words, characters, or even bytes. They could also be spaces or tabs (for coding). It all depends on how we're going to make use of this unit later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a453ea20",
   "metadata": {},
   "source": [
    "## Collocation\n",
    "\n",
    "**Unigram**: A single token considered in isolation. This is the simplest form (1-gram).\n",
    "\n",
    "**Collocation:** is a sequence of words that occur together unusually very often.\n",
    "\n",
    "Examples of Bigrams (2-gram):\n",
    "\n",
    "- `\"United States\"`\n",
    "- `\"fellow citizens\"`\n",
    "- `\"Federal Government\"`\n",
    "- `\"General Government\"`\n",
    "- `\"Vice President\"`\n",
    "- `\"God bless\"`\n",
    "- `\"White Whale\"`\n",
    "\n",
    "A Bigram in Arabic could be:\n",
    "\n",
    "- `\"الذكاء الاصطناعي\"`\n",
    "\n",
    "A Trigram in Arabic could be:\n",
    "\n",
    "- `\"ما شاء الله\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1b5a1",
   "metadata": {},
   "source": [
    "## Sparse Representation\n",
    "\n",
    "When we convert text documents into vectors, we create a **document-term matrix** where:\n",
    "- Each row represents a document\n",
    "- Each column represents a unique word (term) in the vocabulary\n",
    "- Each cell contains the count or weight of that word in that document\n",
    "\n",
    "**Why are these matrices sparse?**\n",
    "\n",
    "In natural language, each document contains only a small fraction of all possible words. For example:\n",
    "- A corpus might have 10,000 unique words (vocabulary size)\n",
    "- A single document might use only 100-200 of those words\n",
    "- This means 98-99% of the vector entries are zero\n",
    "\n",
    "**Sparse matrices** are data structures that efficiently store only non-zero values, saving massive amounts of memory. Instead of storing millions of zeros, we store only the few non-zero values and their positions.\n",
    "\n",
    "**Benefits:**\n",
    "- **Memory efficiency**: Can handle corpora with millions of documents and hundreds of thousands of words\n",
    "- **Computational efficiency**: Operations skip zero values, making computations faster\n",
    "- **Scalability**: Enables working with large-scale text data that wouldn't fit in memory otherwise\n",
    "\n",
    "This is why text vectorization techniques (BoW, TF-IDF) are designed to work with sparse representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d94ab",
   "metadata": {},
   "source": [
    "We will use a corpus with two distinct \"topics\": **Food** and **Computing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small corpus with distinct topics for clarity\n",
    "corpus = [\n",
    "    \"The server is down and needs a reboot.\",      # Computing\n",
    "    \"I love a hamburger with cheese and fries.\",   # Food\n",
    "    \"The new code has a bug in the server.\",       # Computing\n",
    "    \"Can I order a cheese burger with fries?\",     # Food\n",
    "    \"Reboot the server to fix the code bug.\"       # Computing\n",
    "]\n",
    "\n",
    "categories = [\n",
    "    \"Computing\",\n",
    "    \"Food\",\n",
    "    \"Computing\",\n",
    "    \"Food\",\n",
    "    \"Computing\"\n",
    "]\n",
    "\n",
    "print(f\"Corpus size: {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec0858",
   "metadata": {},
   "source": [
    "Vectorization enables three main types of NLP tasks where word frequency and distribution matter more than precise meaning:\n",
    "\n",
    "1. **Text Classification**: Assigning documents to predefined categories (e.g., spam/not spam, positive/negative sentiment)\n",
    "2. **Information Retrieval**: Finding relevant documents from a collection based on a query (e.g., search engines)\n",
    "3. **Topic Modeling**: Discovering hidden topics in a collection of unlabeled documents (unsupervised learning)\n",
    "\n",
    "For these tasks, we use statistical techniques that capture word patterns:\n",
    "\n",
    "1. **Bag of Words (BoW):** represent documents as a vector of word counts.\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency):** represent documents as a vector of word frequencies weighted by their importance.\n",
    "\n",
    "Let's explore these vectorization techniques and understand how they support these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c8e27",
   "metadata": {},
   "source": [
    "### 1. Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa320b8",
   "metadata": {},
   "source": [
    "**Concept:** The simplest method. We discard grammar and order, keeping only the **count** of each word. The document becomes a fixed-length vector of numbers.\n",
    "\n",
    "**Key Class:** `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db283658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1. Initialize the Vectorizer\n",
    "# stop_words='english' removes common words like 'the', 'is', 'and'\n",
    "vectorizer_bow = CountVectorizer(stop_words='english')\n",
    "\n",
    "# 2. Fit and Transform the corpus\n",
    "X_bow = vectorizer_bow.fit_transform(corpus)\n",
    "\n",
    "# --- Visualization & Analysis ---\n",
    "\n",
    "# Get the vocabulary (the \"bag\")\n",
    "feature_names = vectorizer_bow.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=feature_names)\n",
    "\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce5c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_range=(1, 2) means we want both unigrams AND bigrams\n",
    "vectorizer_ngram = CountVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "X_ngram = vectorizer_ngram.fit_transform(corpus)\n",
    "\n",
    "feature_names_ngram = vectorizer_ngram.get_feature_names_out()\n",
    "\n",
    "pd.DataFrame(X_ngram.toarray(), columns=feature_names_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db06d9c",
   "metadata": {},
   "source": [
    "## 2. TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a7c570",
   "metadata": {},
   "source": [
    "In information retrieval, **TF-IDF** (term frequency–inverse document frequency), **is a measure of importance of a word to a document in a collection or corpus, adjusted for the fact that some words appear more frequently in general**.\n",
    "\n",
    "It was often used as a weighting factor in searches of information retrieval, text mining, and user modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164d23a",
   "metadata": {},
   "source": [
    "<img src=\"https://mallahyari.github.io/ml_tutorial/images/tfidf_ex3.png\">\n",
    "\n",
    "Image Source: [mallahyari](https://mallahyari.github.io/ml_tutorial/tfidf/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0760240",
   "metadata": {},
   "source": [
    "Let's break it down:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dac8d0",
   "metadata": {},
   "source": [
    "#### Term Frequency (TF)\n",
    "\n",
    "**TF** measures how common a term $t$ is in a document $d$.\n",
    "\n",
    "`TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)`\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{n(t, d)}{\\sum_{t' \\in d} n(t', d)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $t$ is the term\n",
    "* $d$ is the document\n",
    "* $n(t, d)$ is the number of times term $t$ appears in document $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5197a",
   "metadata": {},
   "source": [
    "#### Inverse Document Frequency (IDF)\n",
    "\n",
    "**IDF** measures how specific a term is to certain documents. Hence, it is the inverse-frequency.\n",
    "\n",
    "`IDF(t) = log_e(Total number of documents / Number of documents with term t in it)`\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log \\left(\\frac{|D|}{\\sum_{d \\in D} n(t, d)} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $D$ is the set of all documents in the corpus\n",
    "* $|D|$ is just the number of documents in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b0fae",
   "metadata": {},
   "source": [
    "#### TF + IDF\n",
    "\n",
    "**TF-IDF**: combines the TF and IDF scores to give a measure of the overall importance of a term in a document (and hence how representative it is). Higher score indicates that some term `t` is more specific (IDF) and more occurring (TF) to some document `d` than other documents in the corpus:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\cdot \\text{IDF}(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c3a60",
   "metadata": {},
   "source": [
    "##### Example\n",
    "\n",
    "Imagine the term $t$ appears 20 times in a document that contains a total of 100 words. Term Frequency (TF) of $t$ can be calculated as follow:\n",
    "\n",
    "$$\n",
    "TF= \\frac{20}{100} = 0.2\n",
    "$$\n",
    "\n",
    "Assume a collection of related documents contains 10,000 documents. If 100 documents out of 10,000 documents contain the term $t$, then, Inverse Document Frequency (IDF) of $t$ can be calculated as follows:\n",
    "\n",
    "$$\n",
    "IDF = log \\frac{10000}{100} = 2\n",
    "$$\n",
    "\n",
    "Using these two quantities, we can calculate TF-IDF score of the term $t$ for the document:\n",
    "\n",
    "$$\n",
    "\\textit{TF-IDF} = 0.2 * 2 = 0.4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991c426",
   "metadata": {},
   "source": [
    "**Analogy:** Think of TF-IDF like a restaurant review system:\n",
    "- **TF (Term Frequency)**: How many times a word appears in a review (like how many times \"delicious\" appears)\n",
    "- **IDF (Inverse Document Frequency)**: How rare the word is across all reviews (if \"delicious\" appears in every review, it's not distinctive)\n",
    "- **TF-IDF**: Combines both—words that appear often in one review but rarely in others are most informative\n",
    "\n",
    "**Key Class:** `TfidfVectorizer`\n",
    "\n",
    "#### Understanding TF-IDF: Compute-by-Hand Example\n",
    "\n",
    "Let's calculate TF-IDF step-by-step for a simple example to understand how it works.\n",
    "\n",
    "**Step 1: Define our corpus**\n",
    "\n",
    "```python\n",
    "# Simple corpus for demonstration\n",
    "documents = [\n",
    "    \"machine learning is fun\",      # Document 1\n",
    "    \"machine learning is hard\",     # Document 2\n",
    "    \"python is fun\"                 # Document 3\n",
    "]\n",
    "```\n",
    "\n",
    "**Step 2: Calculate Term Frequency (TF)**\n",
    "\n",
    "TF measures how common a term is in a document:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
    "$$\n",
    "\n",
    "Let's calculate TF for each word in each document:\n",
    "\n",
    "| Document | Word | Count | Total Words | TF |\n",
    "|----------|------|-------|-------------|-----|\n",
    "| Doc 1 | machine | 1 | 4 | 1/4 = 0.25 |\n",
    "| Doc 1 | learning | 1 | 4 | 1/4 = 0.25 |\n",
    "| Doc 1 | is | 1 | 4 | 1/4 = 0.25 |\n",
    "| Doc 1 | fun | 1 | 4 | 1/4 = 0.25 |\n",
    "| Doc 2 | machine | 1 | 4 | 1/4 = 0.25 |\n",
    "| Doc 2 | learning | 1 | 4 | 1/4 = 0.25 |\n",
    "| Doc 2 | is | 1 | 4 | 1/4 = 0.25 |\n",
    "| Doc 2 | hard | 1 | 4 | 1/4 = 0.25 |\n",
    "| Doc 3 | python | 1 | 3 | 1/3 = 0.33 |\n",
    "| Doc 3 | is | 1 | 3 | 1/3 = 0.33 |\n",
    "| Doc 3 | fun | 1 | 3 | 1/3 = 0.33 |\n",
    "\n",
    "**Step 3: Calculate Inverse Document Frequency (IDF)**\n",
    "\n",
    "IDF measures how rare/common a term is across the corpus:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right)\n",
    "$$\n",
    "\n",
    "Let's count how many documents contain each word:\n",
    "\n",
    "| Word | Documents Containing It | IDF Calculation | IDF |\n",
    "|------|------------------------|-----------------|-----|\n",
    "| machine | 2 (Doc 1, Doc 2) | log(3/2) = log(1.5) | 0.405 |\n",
    "| learning | 2 (Doc 1, Doc 2) | log(3/2) = log(1.5) | 0.405 |\n",
    "| is | 3 (all documents) | log(3/3) = log(1) | 0.000 |\n",
    "| fun | 2 (Doc 1, Doc 3) | log(3/2) = log(1.5) | 0.405 |\n",
    "| hard | 1 (Doc 2 only) | log(3/1) = log(3) | 1.099 |\n",
    "| python | 1 (Doc 3 only) | log(3/1) = log(3) | 1.099 |\n",
    "\n",
    "**Key insight:** Words that appear in many documents (like \"is\") get low IDF scores. Words that appear in few documents (like \"hard\" or \"python\") get high IDF scores.\n",
    "\n",
    "**Step 4: Calculate TF-IDF**\n",
    "\n",
    "TF-IDF combines TF and IDF:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "Let's calculate TF-IDF for each word in each document:\n",
    "\n",
    "| Document | Word | TF | IDF | TF-IDF |\n",
    "|----------|------|----|-----|--------|\n",
    "| Doc 1 | machine | 0.25 | 0.405 | 0.25 × 0.405 = **0.101** |\n",
    "| Doc 1 | learning | 0.25 | 0.405 | 0.25 × 0.405 = **0.101** |\n",
    "| Doc 1 | is | 0.25 | 0.000 | 0.25 × 0.000 = **0.000** |\n",
    "| Doc 1 | fun | 0.25 | 0.405 | 0.25 × 0.405 = **0.101** |\n",
    "| Doc 2 | machine | 0.25 | 0.405 | 0.25 × 0.405 = **0.101** |\n",
    "| Doc 2 | learning | 0.25 | 0.405 | 0.25 × 0.405 = **0.101** |\n",
    "| Doc 2 | is | 0.25 | 0.000 | 0.25 × 0.000 = **0.000** |\n",
    "| Doc 2 | hard | 0.25 | 1.099 | 0.25 × 1.099 = **0.275** |\n",
    "| Doc 3 | python | 0.33 | 1.099 | 0.33 × 1.099 = **0.363** |\n",
    "| Doc 3 | is | 0.33 | 0.000 | 0.33 × 0.000 = **0.000** |\n",
    "| Doc 3 | fun | 0.33 | 0.405 | 0.33 × 0.405 = **0.134** |\n",
    "\n",
    "**Observations:**\n",
    "- \"is\" appears in all documents, so it gets TF-IDF = 0 (not distinctive)\n",
    "- \"hard\" and \"python\" appear in only one document each, so they get high TF-IDF scores (very distinctive)\n",
    "- \"machine\" and \"learning\" appear in 2 documents, so they get moderate TF-IDF scores\n",
    "\n",
    "**Why TF-IDF works:**\n",
    "- Common words (like \"is\", \"the\") appear everywhere → low IDF → low TF-IDF → filtered out\n",
    "- Distinctive words (like \"python\", \"hard\") appear in few documents → high IDF → high TF-IDF → emphasized\n",
    "\n",
    "#### Using TF-IDF with Scikit-learn\n",
    "\n",
    "Now let's see how to use TF-IDF in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd49acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TfidfVectorizer (similar to CountVectorizer)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(corpus)\n",
    "\n",
    "# Get feature names\n",
    "feature_names_tfidf = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "df_tfidf = pd.DataFrame(\n",
    "    np.round(X_tfidf.toarray(), 3),  # Round to 3 decimal places\n",
    "    columns=feature_names_tfidf\n",
    ")\n",
    "\n",
    "print(\"TF-IDF Representation:\")\n",
    "print(\"(Higher values = more distinctive/important words)\")\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755266ca",
   "metadata": {},
   "source": [
    "**What to look for:**\n",
    "\n",
    "* Look at the column `server`. You will see counts like `1` or `2` for the computing sentences, and `0` for the food sentences.\n",
    "* The matrix is **sparse** (mostly zeros), which is typical in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b18726",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Vectorization** is the process of converting text into numerical features that machine learning models can process.\n",
    "\n",
    "- **Tokenization** defines the smallest units of text processing (atoms) - tokens can be words, sub-words, characters, or bytes depending on the use case.\n",
    "\n",
    "- **N-grams** are sequences of n tokens:\n",
    "  - **Unigrams** (1-gram): Single tokens\n",
    "  - **Bigrams** (2-gram): Pairs of consecutive tokens\n",
    "  - **Trigrams** (3-gram): Triplets of consecutive tokens\n",
    "  - N-grams capture word order and context\n",
    "\n",
    "- **Bag of Words (BoW)** is a simple vectorization approach that counts word occurrences, ignoring word order.\n",
    "\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)** weights words by their importance:\n",
    "  - **TF**: How frequent a term is in a document\n",
    "  - **IDF**: How rare/common a term is across the corpus\n",
    "  - Terms frequent in a document but rare in the corpus get high weights\n",
    "\n",
    "- **Scikit-learn vectorizers** (`CountVectorizer`, `TfidfVectorizer`) provide efficient, configurable text vectorization with support for:\n",
    "  - N-gram extraction\n",
    "  - Stop word removal\n",
    "  - Vocabulary size limits\n",
    "  - Custom tokenization\n",
    "\n",
    "- **Sparse matrices** are essential for text data because:\n",
    "  - Most documents contain only a small fraction of the vocabulary\n",
    "  - Sparse representations save memory and computation\n",
    "  - Most matrix elements are zeros\n",
    "\n",
    "- Understanding vectorization is fundamental to building effective NLP pipelines and machine learning models for text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "W5_NLP (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
