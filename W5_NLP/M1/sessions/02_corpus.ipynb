{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc5693e",
   "metadata": {},
   "source": [
    "# Text Data and Exploratory Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook introduces the concept of **corpora** (collections of text documents) as the fundamental infrastructure for NLP work. We explore what corpora are, different types of corpora, and the importance of **Exploratory Data Analysis (EDA)** for text data. Understanding your corpus through EDA is crucial before preprocessing, as it helps identify data quality issues, vocabulary characteristics, and preprocessing needs that will guide your entire NLP pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b3d26",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Explain the significance of corpora as the fundamental infrastructure for training language models, conducting statistical analysis, and benchmarking NLP systems\n",
    "- Understand what exploratory data analysis (EDA) is and why it's crucial before preprocessing\n",
    "- Learn what to assess in EDA for text data: data quality, vocabulary characteristics, and preprocessing needs\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **What is a Corpus?** - Definition and examples of text corpora\n",
    "2. **Types of Corpora** - News, social media, literature, scientific corpora\n",
    "3. **Working with Corpora** - Practical examples of corpus structure\n",
    "4. **Real-World Example Corpora** - Famous corpora used in NLP research\n",
    "5. **Exploratory Data Analysis (EDA) for Text** - Understanding your data before preprocessing\n",
    "6. **EDA Components**:\n",
    "   - Data quality assessment (class distribution, text length, language detection, duplicates)\n",
    "   - Vocabulary characteristics (vocabulary size, word frequency patterns, class-specific patterns)\n",
    "   - Preprocessing needs identification (noise patterns, normalization needs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71815a4",
   "metadata": {},
   "source": [
    "In NLP, a **corpus** (Latin for \"body\") is a collection of text documents. Each document in the corpus represents a single text unit (like a book, article, tweet, or sentence), and the collection of all these documents forms the corpus.\n",
    "\n",
    "It is the raw dataset you work with. Whether it's a folder of PDF invoices, a scraping of Wikipedia, or a database of tweets, it is your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704508b4",
   "metadata": {},
   "source": [
    "![Corpora - Corpus - Docuemnts](../../assets/corpora.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c388c",
   "metadata": {},
   "source": [
    "## Examples of Corpora\n",
    "\n",
    "- **News corpus**: A collection of news articles from various sources\n",
    "- **Social media corpus**: A collection of tweets, posts, or comments\n",
    "- **Literature corpus**: A collection of novels, poems, or plays\n",
    "- **Scientific corpus**: A collection of research papers or abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69061dd3",
   "metadata": {},
   "source": [
    "Let's see some examples of working with corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed corpus (different types of documents)\n",
    "mixed_corpus = {\n",
    "    \"emails\": [\n",
    "        \"Subject: Meeting tomorrow at 3 PM\",\n",
    "        \"Subject: Project update required\"\n",
    "    ],\n",
    "    \"tweets\": [\n",
    "        \"Just finished reading an amazing book! #reading\",\n",
    "        \"Beautiful sunset today ðŸŒ…\"\n",
    "    ],\n",
    "    \"articles\": [\n",
    "        \"Scientists discover new species in the Amazon rainforest...\",\n",
    "        \"Technology advances reshape modern education...\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for doc_type, docs in mixed_corpus.items():\n",
    "    print(f\"\\n{doc_type.upper()} ({len(docs)} documents):\")\n",
    "    for doc in docs:\n",
    "        print(f\"  - {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2666f",
   "metadata": {},
   "source": [
    "Notice how tech type of document have a different **structure**, **writing tone**, and **vocabulary**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2c473",
   "metadata": {},
   "source": [
    "![A document parsed into an abstract syntax tree](https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/AST_Document.svg/1280px-AST_Document.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecdb468",
   "metadata": {},
   "source": [
    "### Real-World Example Corpora\n",
    "\n",
    "Let's explore some famous corpora used in NLP research and applications. Understanding these corpora helps us appreciate the scale and diversity of text data used in NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: Corpus Sizes and Characteristics\n",
    "# This table shows the scale differences between different types of corpora\n",
    "\n",
    "# Real-world corpus statistics (approximate values)\n",
    "corpus_comparison = {\n",
    "    'Corpus Name': [\n",
    "        'Brown Corpus',\n",
    "        'Reuters-21578',\n",
    "        'Wikipedia (English)',\n",
    "        'Common Crawl',\n",
    "        'Arabic Wikipedia',\n",
    "        'Twitter (daily)',\n",
    "        'Google Books'\n",
    "    ],\n",
    "    'Documents': [\n",
    "        '500',\n",
    "        '21,578',\n",
    "        '6 million',\n",
    "        'Billions',\n",
    "        '1.2 million',\n",
    "        '500 million',\n",
    "        '25 million books'\n",
    "    ],\n",
    "    'Words (Approx)': [\n",
    "        '1 million',\n",
    "        '3 million',\n",
    "        '6 billion',\n",
    "        'Trillions',\n",
    "        '500 million',\n",
    "        'Billions',\n",
    "        'Hundreds of billions'\n",
    "    ],\n",
    "    'Source': [\n",
    "        'Brown University, 1961',\n",
    "        'Reuters News, 1987',\n",
    "        'Wikipedia Foundation',\n",
    "        'Web crawls',\n",
    "        'Wikipedia Foundation',\n",
    "        'Twitter API',\n",
    "        'Google Books Project'\n",
    "    ],\n",
    "    'Primary Use': [\n",
    "        'Historical linguistics, benchmarks',\n",
    "        'Text classification',\n",
    "        'Language models, embeddings',\n",
    "        'Large-scale training',\n",
    "        'Arabic NLP',\n",
    "        'Sentiment, trends',\n",
    "        'Historical language analysis'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(corpus_comparison)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4c30f",
   "metadata": {},
   "source": [
    "- Note: These are approximate values. Actual sizes may vary\n",
    "- Large corpora like Common Crawl and social media are continuously growing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5d602",
   "metadata": {},
   "source": [
    "## Why Corpora Matter?\n",
    "\n",
    "Corpora serve as the foundation for:\n",
    "\n",
    "1. **Training language models**\n",
    "2. **Building domain-specific NLP applications**\n",
    "3. **Statistical analysis of language patterns**\n",
    "4. **Linguistic research and analysis**\n",
    "\n",
    "---\n",
    "\n",
    "## Exploratory Data Analysis (EDA) for Text Data\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is the process of understanding your data *before* you start preprocessing or modeling. For text data, EDA helps you:\n",
    "\n",
    "- **Identify data quality issues** early (wrong language, duplicates, outliers)\n",
    "- **Understand vocabulary characteristics** (size, frequency patterns, class-specific words)\n",
    "- **Determine preprocessing needs** (what noise to remove, what to normalize)\n",
    "- **Make informed decisions** about your NLP pipeline\n",
    "\n",
    "> **Analogy**: EDA is like inspecting ingredients before cooking. You check if vegetables are fresh, if you have the right quantities, and if anything needs cleaning. Similarly, EDA helps you understand your text data before \"cooking\" (preprocessing and modeling).\n",
    "\n",
    "### Why EDA Matters for Text\n",
    "\n",
    "**Without EDA**, you might:\n",
    "- Preprocess incorrectly (e.g., remove important punctuation)\n",
    "- Miss data quality issues (duplicates, wrong language)\n",
    "- Choose wrong preprocessing steps (e.g., stemming when lemmatization is better)\n",
    "- Build models on biased or poor-quality data\n",
    "\n",
    "**With EDA**, you:\n",
    "- Make informed preprocessing decisions\n",
    "- Catch data issues early\n",
    "- Understand what your model will learn from\n",
    "- Validate that preprocessing improved your data\n",
    "\n",
    "### EDA Components for Text Data\n",
    "\n",
    "#### 1. Data Quality Assessment\n",
    "\n",
    "**Class Distribution**: Are categories balanced or imbalanced?\n",
    "- **Balanced**: Equal number of examples per class (e.g., 50% positive, 50% negative)\n",
    "- **Imbalanced**: One class dominates (e.g., 90% positive, 10% negative)\n",
    "- **Why it matters**: Imbalanced data can bias models toward the majority class\n",
    "\n",
    "**Text Length Distribution**: How long are your texts?\n",
    "- Very short texts (1-5 words) might be incomplete or noisy\n",
    "- Very long texts (1000+ words) might be concatenated or need chunking\n",
    "- Most texts should fall within a reasonable range\n",
    "\n",
    "**Language Detection**: Is the text in the expected language?\n",
    "- Mixed languages need special handling\n",
    "- Wrong language indicates data collection issues\n",
    "\n",
    "**Duplicate Detection**: Are there duplicate documents?\n",
    "- Duplicates can bias training (model sees same example multiple times)\n",
    "- Need to identify and remove duplicates before training\n",
    "\n",
    "#### 2. Vocabulary Characteristics\n",
    "\n",
    "**Vocabulary Size**: How many unique words?\n",
    "- Small vocabulary (< 1000 words): Limited diversity, might need more data\n",
    "- Large vocabulary (> 100,000 words): High diversity, might need dimensionality reduction\n",
    "- Affects vectorization complexity and model performance\n",
    "\n",
    "**Word Frequency Patterns**: Which words are most common?\n",
    "- Very common words (appear in >80% of documents) might be stop words\n",
    "- Rare words (appear in <2 documents) might be typos or noise\n",
    "- Helps identify stop words to remove\n",
    "\n",
    "**Class-Specific Patterns**: Do certain words appear more in certain classes?\n",
    "- Words that appear mostly in one class are good features for classification\n",
    "- Helps with feature selection and model interpretation\n",
    "\n",
    "#### 3. Preprocessing Needs Identification\n",
    "\n",
    "**Noise Patterns**: What needs to be removed?\n",
    "- URLs, emails, hashtags, mentions (@user)\n",
    "- HTML tags, special characters\n",
    "- Identified through EDA by examining sample texts\n",
    "\n",
    "**Normalization Needs**: What needs to be standardized?\n",
    "- Case variations (UPPERCASE, lowercase, Title Case)\n",
    "- Contractions (\"don't\" vs \"do not\")\n",
    "- Elongations (\"loooove\" vs \"love\")\n",
    "- Diacritics (in Arabic: \"Ø§Ù„ÙƒØªØ§Ø¨\" vs \"Ø§Ù„ÙƒØªØ§Ø¨\")\n",
    "\n",
    "**Outliers**: Unusually long or short texts\n",
    "- Very long texts might be concatenated (need splitting)\n",
    "- Very short texts might be incomplete (need filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dfbbe4",
   "metadata": {},
   "source": [
    "### Example: EDA Workflow\n",
    "\n",
    "Let's see a practical example of EDA on a sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: EDA on a sample text dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load sample data (this would be your actual dataset)\n",
    "sample_data = {\n",
    "    'text': [\n",
    "        'I love this product! It is amazing.',\n",
    "        'This is terrible. Worst purchase ever.',\n",
    "        'I love this product! It is amazing.',  # Duplicate\n",
    "        'Ù…Ù…ØªØ§Ø² Ø±Ø§Ø¦Ø¹',  # Arabic text\n",
    "        'The quick brown fox jumps over the lazy dog.',\n",
    "        'Bad product. Do not buy.',\n",
    "    ],\n",
    "    'label': ['positive', 'negative', 'positive', 'positive', 'neutral', 'negative']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "# 1. Class Distribution\n",
    "print(\"Class Distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nClass Balance: {df['label'].value_counts().std():.2f} (lower is more balanced)\")\n",
    "\n",
    "# 2. Text Length\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(\"\\nText Length Statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "# 3. Duplicate Detection\n",
    "duplicates = df.duplicated(subset=['text'])\n",
    "print(f\"\\nDuplicates found: {duplicates.sum()}\")\n",
    "\n",
    "# 4. Vocabulary Size\n",
    "all_words = ' '.join(df['text']).split()\n",
    "vocab_size = len(set(all_words))\n",
    "print(f\"\\nVocabulary Size: {vocab_size} unique words\")\n",
    "\n",
    "# 5. Most Common Words\n",
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "print(\"\\nTop 5 Most Common Words:\")\n",
    "for word, count in word_counts.most_common(5):\n",
    "    print(f\"  '{word}': {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca42d4",
   "metadata": {},
   "source": [
    "**Output Analysis:**\n",
    "- Class distribution shows if data is balanced\n",
    "- Text length helps identify outliers\n",
    "- Duplicates need to be removed\n",
    "- Vocabulary size affects vectorization\n",
    "- Common words help identify stop words\n",
    "\n",
    "> **Note**: In the lab (Session 7), you'll perform comprehensive EDA on the Arabic 100k Reviews dataset, including class distribution analysis, text length histograms, vocabulary analysis, and word frequency patterns by class.\n",
    "\n",
    "### EDA Best Practices\n",
    "\n",
    "1. **Always do EDA before preprocessing** - Understand your raw data first\n",
    "2. **Visualize distributions** - Histograms, bar charts help identify patterns\n",
    "3. **Examine samples** - Look at actual text examples, not just statistics\n",
    "4. **Compare before and after** - Validate that preprocessing improved your data\n",
    "5. **Document findings** - Keep notes on what you discovered and decisions you made\n",
    "\n",
    "### The EDA â†’ Preprocessing Connection\n",
    "\n",
    "EDA findings directly inform preprocessing decisions:\n",
    "\n",
    "| EDA Finding | Preprocessing Action |\n",
    "|------------|---------------------|\n",
    "| Many URLs found | Remove URLs with regex |\n",
    "| Mixed case (UPPERCASE, lowercase) | Normalize to lowercase |\n",
    "| Many duplicates | Remove duplicate documents |\n",
    "| Very long texts | Consider text splitting or truncation |\n",
    "| Very short texts | Filter out incomplete texts |\n",
    "| Common words in >80% of docs | Add to stop word list |\n",
    "| Rare words in <2 docs | Consider min_df filtering |\n",
    "| Mixed languages | Use language-specific preprocessing |\n",
    "| Many elongations (\"loooove\") | Normalize elongations |\n",
    "\n",
    "> **Remember**: EDA is not a one-time activity. You should do EDA:\n",
    "> - **Before preprocessing** (to understand raw data)\n",
    "> - **After preprocessing** (to validate improvements)\n",
    "> - **After vectorization** (to understand feature space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd974527",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- A **corpus** is a collection of text documents that serves as the fundamental infrastructure for NLP work, including training language models, statistical analysis, and benchmarking.\n",
    "- Different types of corpora exist (news, social media, literature, scientific) with varying structures, writing tones, and vocabularies.\n",
    "- Understanding your corpus structure and characteristics is essential for building effective NLP pipelines and avoiding biases in your models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
