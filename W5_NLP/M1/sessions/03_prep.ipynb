{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers **data preprocessing** for statistical NLP, focusing on the lossy transformation of raw text into clean, normalized tokens suitable for machine learning models. We explore the context in which statistical NLP is used (search, topic discovery, classification), the importance of Exploratory Data Analysis (EDA) before preprocessing, and the various preprocessing methods including cleaning (noise removal) and normalization (standardization). Understanding preprocessing order and trade-offs is crucial for building effective NLP pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Recognize the context in which statistical NLP is used\n",
    "- Learn the function of many lossy preprocessing methods used in statistical NLP\n",
    "- Recognize why preprocessing order matters and how to determine the correct sequence\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **The Goal of Statistical NLP** - Understanding the \"super-fast librarian\" approach\n",
    "2. **Exploratory Data Analysis (EDA) for Text** - Assessing data quality, vocabulary characteristics, and preprocessing needs\n",
    "3. **Preprocessing Overview** - Why preprocessing is necessary and its two main phases\n",
    "4. **Cleaning (Noise Removal)** - Removing non-textual elements, URLs, HTML tags, punctuation, stop words\n",
    "5. **Normalization** - Standardizing text (case, contractions, elongations, diacritics)\n",
    "6. **Preprocessing Order** - Why order matters and how to determine the correct sequence\n",
    "7. **Arabic-Specific Preprocessing** - Special considerations for Arabic text\n",
    "8. **Preprocessing Trade-offs** - Balancing information loss vs. efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The Goal\n",
    "\n",
    "Think **\"super-fast librarian\"**. Its goal isn't to understand the emotional nuance of a sentence; its goal is to organize, index, and retrieve documents based on the words they contain.\n",
    "\n",
    "* **Search:** Finding the one document out of a million that matches your query (e.g., Google Search, Ctrl+F).\n",
    "* **Topic Discovery:** Scanning huge archives to see what they are talking about (e.g., \"70% of these news articles are about 'Sports'\").\n",
    "* **Classification:** Sorting text into predefined buckets based on word statistics (e.g., Spam Filtering: Noticing that emails containing the words \"Winner\" and \"Cash\" 50 times are likely junk).\n",
    "\n",
    "> Example: Routing customer support tickets. If a ticket has the words \"refund,\" \"money,\" and \"charge,\" the math predicts it belongs to the \"Billing Department\" bucket. If it has \"crash,\" \"error,\" and \"screen,\" it goes to \"Tech Support.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) for Text\n",
    "\n",
    "Before preprocessing, it's essential to **understand your data**. EDA helps you:\n",
    "\n",
    "**1. Assess Data Quality**\n",
    "- **Class distribution**: Are categories balanced or imbalanced? (affects model training)\n",
    "- **Text length distribution**: Are there unusually long or short texts? (outliers that might need handling)\n",
    "- **Language detection**: Is the text in the expected language? (mixed languages need special handling)\n",
    "- **Duplicate detection**: Are there duplicate documents? (can bias training)\n",
    "\n",
    "**2. Understand Vocabulary Characteristics**\n",
    "- **Vocabulary size**: How many unique words? (affects vectorization complexity)\n",
    "- **Word frequency patterns**: Which words are most common? (helps identify stop words)\n",
    "- **Class-specific patterns**: Do certain words appear more in certain classes? (guides feature selection)\n",
    "\n",
    "**3. Identify Preprocessing Needs**\n",
    "- **Noise patterns**: URLs, emails, special characters that need removal\n",
    "- **Normalization needs**: Case variations, contractions, elongations\n",
    "- **Outliers**: Very long texts (might be concatenated), very short texts (might be incomplete)\n",
    "\n",
    "**Why EDA matters:**\n",
    "- **Informed decisions**: Choose preprocessing steps based on actual data characteristics, not assumptions\n",
    "- **Quality assurance**: Catch data issues early (wrong language, duplicates, extreme outliers)\n",
    "- **Baseline understanding**: Know your data before and after preprocessing to validate changes\n",
    "\n",
    "> **Note**: In the lab, you will practice EDA techniques including class distribution analysis, text length histograms, and vocabulary analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "Preprocessing is the critical, intentionally **lossy** process of stripping raw text down to its bare semantic bones (keywords) to prepare it for analysis. It generally consists of two phases: **Cleaning** (removing noise) and **Normalization** (standardizing text).\n",
    "\n",
    "### Why Preprocess?\n",
    "\n",
    "Raw data contains variations in case, punctuation, whitespace, and encoding. Preprocessing resolves these to ensure:\n",
    "\n",
    "* **Accuracy:** Improves matching, comparison, and analysis reliability.\n",
    "* **Efficiency:** Reduces vocabulary size and computational overhead.\n",
    "\n",
    "### The Pipeline: Key Steps\n",
    "\n",
    "**1. Cleaning (Noise Removal)**\n",
    "This phase removes non-textual or irrelevant elements to reduce the document to valid tokens.\n",
    "\n",
    "* Collapse multiple spaces, tabs, and newlines into single spaces.\n",
    "* Strip URLs, HTML tags, numbers, and special characters.\n",
    "* Eliminate punctuation (e.g., `.` `?` `!`) and standard \"stop words\" (filler words like \"the\", \"is\", \"at\") that carry little semantic weight.\n",
    "\n",
    "**2. Normalization (Canonicalization)**\n",
    "This phase transforms the remaining text into a single standard form.\n",
    "\n",
    "* **Casefold:** converting text to lowercase (English)\n",
    "* **Expansion:** `\"don't\" -> \"do not\"`\n",
    "* **Reduction:** `\"closing\", \"closed\", \"closes\" -> \"close\"`\n",
    "\n",
    "### Why Preprocessing Order Matters\n",
    "\n",
    "**The order of preprocessing steps is critical** because later steps depend on earlier ones. Applying steps in the wrong order can:\n",
    "- **Lose information**: Removing punctuation before extracting mentions (`@user`) means you can't identify mentions\n",
    "- **Break patterns**: Lowercasing before removing URLs might break URL detection patterns\n",
    "- **Create errors**: Removing stop words before expanding contractions loses context\n",
    "\n",
    "**General order principles:**\n",
    "1. **Extract structured information first** (URLs, emails, mentions) before removing punctuation\n",
    "2. **Normalize structure** (whitespace, case) before tokenization\n",
    "3. **Tokenize** before removing stop words (need word boundaries)\n",
    "4. **Stem/Lemmatize** after tokenization (operate on individual words)\n",
    "5. **Remove stop words last** (after all transformations are complete)\n",
    "\n",
    "**Example of wrong order:**\n",
    "- ‚ùå Remove punctuation ‚Üí Remove mentions: `\"@user\"` becomes `\"user\"` (mention lost)\n",
    "- ‚úÖ Remove mentions ‚Üí Remove punctuation: `\"@user\"` ‚Üí `\"\"` ‚Üí punctuation removed (correct)\n",
    "\n",
    "**Example of correct order:**\n",
    "1. Remove URLs and emails (they contain punctuation)\n",
    "2. Remove mentions/handles (they contain `@`)\n",
    "3. Normalize whitespace\n",
    "4. Lowercase\n",
    "5. Tokenize\n",
    "6. Expand contractions (if needed)\n",
    "7. Stem/Lemmatize\n",
    "8. Remove stop words\n",
    "\n",
    "> **Note**: In the lab, you will see how preprocessing order affects results and learn to design effective preprocessing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install farasapy==0.1.1 nltk==3.9.2 pandas==2.3.3 pyarabic==0.6.15 qalsadi==0.5.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# NLTK imports\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Arabic NLP libraries\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from pyarabic import number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Farasa segmenter and stemmer for Arabic text processing\n",
    "segmenter = FarasaSegmenter()\n",
    "stemmer = FarasaStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3rByOuVdsta",
    "outputId": "4acd8661-cffe-443e-a15c-b89183289875"
   },
   "outputs": [],
   "source": [
    "# Example 1: Text Normalization - Substitution Approach\n",
    "# Normalization prefers substitution over removal to preserve information\n",
    "# This example demonstrates case normalization and whitespace normalization\n",
    "\n",
    "# Original text with various formatting issues\n",
    "examples = [\n",
    "    \"  Hello    World  \",\n",
    "    \"HELLO\\t\\tWORLD\\n\\n\",\n",
    "    \"  hello   world  \",\n",
    "    \"Hello   World\"\n",
    "]\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text by substituting variations with canonical forms.\n",
    "    This preserves information better than removal.\n",
    "    \"\"\"\n",
    "    # Step 1: Case normalization - substitute all cases with lowercase\n",
    "    normalized = text.lower()\n",
    "    \n",
    "    # Step 2: Whitespace normalization - substitute multiple spaces/tabs/newlines with single space\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized)\n",
    "    \n",
    "    # Step 3: Remove leading/trailing whitespace\n",
    "    normalized = normalized.strip()\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "print(\"Normalization Examples (Substitution Approach):\\n\")\n",
    "print(\"=\" * 60)\n",
    "for i, example in enumerate(examples, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Original: '{example}'\")\n",
    "    normalized = normalize_text(example)\n",
    "    print(f\"Normalized: '{normalized}'\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Key Takeaway: All variations normalize to the same canonical form!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTNsTZf6dstb",
    "outputId": "3e6d9627-6ac8-4d33-945f-12ebd81b8c03"
   },
   "outputs": [],
   "source": [
    "# Example 2: Normalization with Substitution (URLs, Numbers, Punctuation)\n",
    "# Substitution approach: Replace with placeholders or normalized forms instead of removing\n",
    "\n",
    "# Sample text with URLs, numbers, and punctuation\n",
    "text = \"This is an example sentence with a URL (http://www.example.com) and a number (123).\"\n",
    "print('Original text:', text)\n",
    "\n",
    "# Step 1: Substitute URLs with a placeholder\n",
    "# Pattern: http\\S+ matches \"http\" followed by any non-whitespace characters\n",
    "# Substitute with a normalized placeholder\n",
    "text_normalized_urls = re.sub(r\"http\\S+\", \"[URL]\", text)\n",
    "print('After normalizing URLs:', text_normalized_urls)\n",
    "\n",
    "# Step 2: Substitute numbers with a placeholder (or convert to words)\n",
    "# Pattern: \\d+ matches one or more digits\n",
    "# Option 1: Substitute with placeholder\n",
    "text_normalized_numbers = re.sub(r\"\\d+\", \"[NUMBER]\", text_normalized_urls)\n",
    "print('After normalizing numbers (placeholder):', text_normalized_numbers)\n",
    "\n",
    "# Option 2: Keep numbers but normalize format (e.g., remove leading zeros)\n",
    "text_normalized_numbers_format = re.sub(r\"\\b0+(\\d+)\\b\", r\"\\1\", text_normalized_urls)\n",
    "print('After normalizing number format:', text_normalized_numbers_format)\n",
    "\n",
    "# Step 3: Normalize punctuation - substitute multiple punctuation with single space\n",
    "# Instead of removing, we normalize punctuation marks\n",
    "text_normalized_punct = re.sub(r'[^\\w\\s]+', ' ', text_normalized_numbers)\n",
    "print('After normalizing punctuation:', text_normalized_punct)\n",
    "\n",
    "# Step 4: Clean up extra whitespace (substitute multiple spaces with single space)\n",
    "final_text = re.sub(r'\\s+', ' ', text_normalized_punct).strip()\n",
    "print('Final normalized text:', final_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Note: Substitution preserves structure better than removal!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPRZjTqudstb",
    "outputId": "6567178a-3383-4993-d8df-5aa6d53fdc63"
   },
   "outputs": [],
   "source": [
    "# Example 3: Removing stop words\n",
    "# Stop words are common words that appear frequently but often don't add much semantic meaning\n",
    "# Examples: \"the\", \"is\", \"at\", \"which\", \"on\", etc.\n",
    "\n",
    "# Important Note: Removing stop words is NOT always beneficial!\n",
    "# - For tasks like sentiment analysis, stop words might be important (e.g., \"not\" is a stop word)\n",
    "# - For tasks like topic modeling or information retrieval, removing stop words can help\n",
    "# - Always consider your specific use case before removing stop words\n",
    "\n",
    "# Simple example with a custom stop word list\n",
    "stop_words = ['is', 'an', 'with', 'a', 'and', 'the', 'to', 'of']\n",
    "text = \"this is is is an example text with a a a lot of stop words that need to be removed\"\n",
    "\n",
    "print('Original text:', text)\n",
    "print('Number of words before:', len(text.split()))\n",
    "\n",
    "# Remove stop words\n",
    "words = text.split()\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "print('After removing stop words:', filtered_text)\n",
    "print('Number of words after:', len(filtered_words))\n",
    "print('\\nRemoved words:', [w for w in words if w in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3h2X_bYdstc",
    "outputId": "0222cda1-d6d3-4c82-d66a-5e0b60d5e0b7"
   },
   "outputs": [],
   "source": [
    "# Example 4: Using NLTK's built-in stop word lists\n",
    "# NLTK provides pre-compiled stop word lists for many languages\n",
    "# These are more comprehensive than custom lists and are maintained by the community\n",
    "\n",
    "# English stop words\n",
    "# NLTK's English stop word list contains 179 common English words\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "print(f\"English stop words count: {len(english_stop_words)}\")\n",
    "print(f\"Sample English stop words: {list(english_stop_words)[:20]}\")  # Show first 20\n",
    "\n",
    "# Arabic stop words\n",
    "# NLTK's Arabic stop word list contains 701 common Arabic words\n",
    "arabic_stop_words = set(stopwords.words('arabic'))\n",
    "print(f\"\\nArabic stop words count: {len(arabic_stop_words)}\")\n",
    "print(f\"Sample Arabic stop words: {list(arabic_stop_words)[:20]}\")  # Show first 20\n",
    "\n",
    "# Example: Using stop words to filter text\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "words = sample_text.lower().split()\n",
    "print(f\"\\nOriginal text: {sample_text}\")\n",
    "print(f\"Words: {words}\")\n",
    "\n",
    "filtered = [w for w in words if w not in english_stop_words]\n",
    "print(f\"After removing stop words: {filtered}\")\n",
    "print(f\"Filtered text: {' '.join(filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lU_e3Njfdstc",
    "outputId": "f60ca97d-feb5-438a-9607-e8f59d867f3e"
   },
   "outputs": [],
   "source": [
    "# Example 5: Expanding contractions\n",
    "# Contractions are shortened forms of words (e.g., \"don't\" = \"do not\")\n",
    "# Expanding contractions can help with:\n",
    "# - Better word matching (both \"don't\" and \"do not\" become the same)\n",
    "# - More consistent tokenization\n",
    "# - Better understanding of the actual words used\n",
    "\n",
    "# Sample text with contractions\n",
    "sentence_with_contractions = \"I ain't going to the store because I ain't got no money.\"\n",
    "print('Original text:', sentence_with_contractions)\n",
    "\n",
    "# Dictionary mapping contractions to their expanded forms\n",
    "# Note: This is a simplified example. In practice, you might use libraries\n",
    "# like 'contractions' package which handles edge cases better\n",
    "contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Expand contractions word by word\n",
    "words = sentence_with_contractions.split()\n",
    "expanded_words = []\n",
    "for word in words:\n",
    "    # Remove punctuation from word for lookup, but preserve it\n",
    "    word_clean = word.rstrip('.,!?;:')\n",
    "    if word_clean.lower() in contractions:\n",
    "        expanded = contractions[word_clean.lower()]\n",
    "        # Preserve original punctuation\n",
    "        if word != word_clean:\n",
    "            expanded += word[len(word_clean):]\n",
    "        expanded_words.append(expanded)\n",
    "    else:\n",
    "        expanded_words.append(word)\n",
    "\n",
    "expanded_text = ' '.join(expanded_words)\n",
    "print('After expanding contractions:', expanded_text)\n",
    "\n",
    "# Note: The current implementation has a limitation - \"ain't\" appears twice\n",
    "# but gets expanded the same way. More sophisticated approaches handle\n",
    "# context-dependent contractions better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation and Stemming\n",
    "\n",
    "**Segmentation**: helps identify prefixes, suffixes, and roots.\n",
    "\n",
    "**Stemming**: reduces words to their core meaning (root).\n",
    "\n",
    "- Both are essential for Arabic NLP tasks such as:\n",
    "  - Search engines\n",
    "  - Topic modeling\n",
    "  - Text classification\n",
    "  - Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arabic Text Normalization with PyArabic\n",
    "\n",
    "[**PyArabic**](https://github.com/linuxscout/pyarabic) is a comprehensive library for Arabic text preprocessing and normalization. It provides many features for handling Arabic text's unique characteristics.\n",
    "\n",
    "**Key Features:**\n",
    "- ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ≠ÿ±ŸàŸÅ (Character classification)\n",
    "- ÿ™ŸÅÿ±ŸäŸÇ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ Ÿàÿ≠ÿØÿßÿ™ (Text segmentation into sentences or words)\n",
    "- ÿ≠ÿ∞ŸÅ ÿßŸÑÿ≠ÿ±ŸÉÿßÿ™ (Removing diacritics)\n",
    "- ÿ™ŸÜŸÖŸäÿ∑ ÿßŸÑÿ≠ÿ±ŸàŸÅ (Character normalization - unifying forms like alif-lam, hamzas)\n",
    "- ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿ£ÿπÿØÿßÿØ ÿ•ŸÑŸâ ŸÉŸÑŸÖÿßÿ™ (Converting numbers to words)\n",
    "- And many more...\n",
    "\n",
    "Let's see PyArabic in action with practical examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numbers to Arabic words\n",
    "numbers = [123, 4567, 1000000]\n",
    "print(\"Converting numbers to Arabic words:\")\n",
    "for num in numbers:\n",
    "    try:\n",
    "        # PyArabic number module converts numbers to words\n",
    "        arabic_words = number.number2text(num)\n",
    "        print(f\"  {num} ‚Üí {arabic_words}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {num} ‚Üí (conversion not available: {e})\")\n",
    "\n",
    "# Convert text numbers to words\n",
    "text_with_numbers = \"ŸÑÿØŸä 5 ŸÉÿ™ÿ® Ÿà 10 ÿ£ŸÇŸÑÿßŸÖ\"\n",
    "print(f\"\\nOriginal text with numbers: {text_with_numbers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cMFv2Htdst4"
   },
   "source": [
    "### Arabic Segmentation and Stemming\n",
    "\n",
    "[**FarasaPy**](https://github.com/MagedSaeed/farasapy) is an Arabic NLP toolkit serving the following tasks:\n",
    "\n",
    "- Segmentation (ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÉŸÑŸÖÿ© ÿ•ŸÑŸâ ÿ£ÿ¨ÿ≤ÿßÿ°)\n",
    "- Stemming (ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑÿ¨ÿ∞ÿ±)\n",
    "- Named Entity Recognition (NER)\n",
    "- Part Of Speech tagging (POS tagging) (Ÿàÿ≥ŸÖ ÿßŸÑÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßŸÑŸÉŸÑÿßŸÖ)\n",
    "- Diacritization (ÿ™ÿ¥ŸÉŸäŸÑ ÿßŸÑŸÉŸÑŸÖÿßÿ™)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Arabic Word Segmentation (ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÉŸÑŸÖÿ©)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Segmentation splits Arabic words into their morphological components.\")\n",
    "print(\"This is crucial because Arabic words often combine multiple morphemes.\\n\")\n",
    "\n",
    "# Sample Arabic words and sentences\n",
    "arabic_words = [\n",
    "    \"ÿßŸÑŸÉÿ™ÿßÿ®\",           # The book\n",
    "    \"ÿ®ÿßŸÑŸÖÿØÿ±ÿ≥ÿ©\",         # At the school\n",
    "    \"Ÿäÿ∞Ÿáÿ®ŸàŸÜ\",           # They go\n",
    "    \"ŸÉÿ™ÿ®ÿ™Ÿáÿß\",           # I wrote it (feminine)\n",
    "]\n",
    "\n",
    "print(\"Word Segmentation Examples:\")\n",
    "for word in arabic_words:\n",
    "    segmented = segmenter.segment(word)\n",
    "    print(f\"  '{word}' ‚Üí {segmented}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Arabic Stemming (ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑÿ¨ÿ∞ÿ±)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Stemming extracts the root (ÿ¨ÿ∞ÿ±) of Arabic words.\")\n",
    "print(\"Arabic roots are typically 3-letter roots that convey core meaning.\\n\")\n",
    "\n",
    "# Words to stem\n",
    "words_to_stem = [\n",
    "    \"ŸÉÿßÿ™ÿ®\",\n",
    "    \"ŸÖŸÉÿ™ÿ®ÿ©\",\n",
    "    \"ŸäŸÉÿ™ÿ®\",\n",
    "    \"ŸÉÿ™ÿ®\",\n",
    "    \"ÿßŸÑŸÉÿ™ÿßÿ®ÿ©\",\n",
    "]\n",
    "\n",
    "print(\"Stemming Examples:\")\n",
    "for word in words_to_stem:\n",
    "    stem = stemmer.stem(word)\n",
    "    print(f\"  '{word}' ‚Üí {stem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Combining Segmentation and Stemming\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Combined workflow\n",
    "text = \"ÿßŸÑÿ∑ŸÑÿßÿ® ŸäÿØÿ±ÿ≥ŸàŸÜ ŸÅŸä ÿßŸÑŸÖŸÉÿ™ÿ®ÿ©\"\n",
    "print(f\"Original text: '{text}'\")\n",
    "\n",
    "# First segment\n",
    "segmented = segmenter.segment(text)\n",
    "print(f\"After segmentation: {segmented}\")\n",
    "\n",
    "# Then stem\n",
    "stemmed = stemmer.stem(text)\n",
    "print(f\"After stemming: {stemmed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Statistical NLP** serves as a \"super-fast librarian\" for search, topic discovery, and classification tasks, focusing on organizing and retrieving documents based on word statistics.\n",
    "\n",
    "- **Exploratory Data Analysis (EDA)** is essential before preprocessing to:\n",
    "  - Assess data quality (class distribution, text length, language, duplicates)\n",
    "  - Understand vocabulary characteristics (size, frequency patterns, class-specific patterns)\n",
    "  - Identify preprocessing needs (noise patterns, normalization requirements)\n",
    "\n",
    "- **Preprocessing is intentionally lossy** - it strips text down to semantic keywords, consisting of two phases:\n",
    "  - **Cleaning**: Removing noise (URLs, HTML tags, punctuation, stop words)\n",
    "  - **Normalization**: Standardizing text (case, contractions, elongations, diacritics)\n",
    "\n",
    "- **Preprocessing order matters critically**:\n",
    "  1. Extract structured information first (URLs, emails, mentions)\n",
    "  2. Normalize structure (whitespace, case)\n",
    "  3. Tokenize\n",
    "  4. Expand contractions (if needed)\n",
    "  5. Stem/Lemmatize\n",
    "  6. Remove stop words last\n",
    "\n",
    "- Wrong preprocessing order can lose information, break patterns, or create errors.\n",
    "\n",
    "- **Arabic text** requires special preprocessing considerations including diacritic removal, elongation handling, and Arabic-specific stemming/lemmatization tools (Farasa, PyArabic, Qalsadi).\n",
    "\n",
    "- Preprocessing involves **trade-offs** between information retention and computational efficiency - choose steps based on your specific use case and data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [PyArabic Documentation](https://github.com/linuxscout/pyarabic)\n",
    "- [FarasaPy Documentation](https://github.com/MagedSaeed/farasapy)\n",
    "\n",
    "### Corpora & Data\n",
    "- [Brown Corpus Overview](https://en.wikipedia.org/wiki/Brown_Corpus)\n",
    "- [Reuters-21578 Dataset](https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html)\n",
    "- [Common Crawl](https://commoncrawl.org/)\n",
    "- [Arabic Wikipedia Dumps](https://dumps.wikimedia.org/arwiki/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tjHPb5LQdstf"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "w5-nlp (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
