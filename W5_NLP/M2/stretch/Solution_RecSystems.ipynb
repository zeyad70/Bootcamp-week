{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b45cb763",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "b45cb763",
        "outputId": "d5aab987-025c-4b8b-d504-322d41624f21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device initialized: cpu\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# This defines the 'device' variable to avoid the NameError\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device initialized: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (ratings.csv uploaded to Colab)\n",
        "try:\n",
        "    df = pd.read_csv('ratings.csv')\n",
        "    df = df[['userId', 'movieId', 'rating']]\n",
        "\n",
        "    # Encoding IDs to ensure they are sequential (required for Embedding layers)\n",
        "    user_encoder = LabelEncoder()\n",
        "    movie_encoder = LabelEncoder()\n",
        "\n",
        "    df['user_idx'] = user_encoder.fit_transform(df['userId'])\n",
        "    df['movie_idx'] = movie_encoder.fit_transform(df['movieId'])\n",
        "\n",
        "    num_users = df['user_idx'].nunique()\n",
        "    num_movies = df['movie_idx'].nunique()\n",
        "\n",
        "    print(f\"Number of Unique Users: {num_users}\")\n",
        "    print(f\"Number of Unique Movies: {num_movies}\")\n",
        "    print(\"âœ… Data preprocessing completed successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\" Error: ratings.csv not found. Please upload it to the side panel.\")"
      ],
      "metadata": {
        "id": "Z1Gx0yopjFWB",
        "outputId": "59085ea4-12b1-4deb-8c1b-3e84ba06ebae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Z1Gx0yopjFWB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Unique Users: 7107\n",
            "Number of Unique Movies: 17174\n",
            "âœ… Data preprocessing completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CollaborativeFilteringNet(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_size=50):\n",
        "        super(CollaborativeFilteringNet, self).__init__()\n",
        "\n",
        "        # Creating Embedding layers for Users and Items\n",
        "        # This treats User IDs and Movie IDs as tokens similar to NLP word tokens\n",
        "        self.user_embeddings = nn.Embedding(n_users, emb_size)\n",
        "        self.item_embeddings = nn.Embedding(n_items, emb_size)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.user_embeddings.weight.data.uniform_(0, 0.05)\n",
        "        self.item_embeddings.weight.data.uniform_(0, 0.05)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        # Retrieve the embedding vectors for the given indices\n",
        "        user_vecs = self.user_embeddings(user_indices)\n",
        "        item_vecs = self.item_embeddings(item_indices)\n",
        "\n",
        "        # Perform dot product to calculate the similarity/predicted rating\n",
        "        dot_product = (user_vecs * item_vecs).sum(1)\n",
        "        return dot_product\n",
        "\n",
        "# Instantiate the model\n",
        "model = CollaborativeFilteringNet(num_users, num_movies).to(device)\n",
        "print(\"âœ… Embedding-based model architecture defined.\")"
      ],
      "metadata": {
        "id": "mFgdwHF3jUuJ",
        "outputId": "fdcdc8f0-04f5-49c9-d5ac-8471c2e06e13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mFgdwHF3jUuJ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Embedding-based model architecture defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into 80% training and 20% testing\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch Tensors and move to device (CPU or GPU)\n",
        "train_u = torch.LongTensor(train.user_idx.values).to(device)\n",
        "train_m = torch.LongTensor(train.movie_idx.values).to(device)\n",
        "train_r = torch.FloatTensor(train.rating.values).to(device)\n",
        "\n",
        "test_u = torch.LongTensor(test.user_idx.values).to(device)\n",
        "test_m = torch.LongTensor(test.movie_idx.values).to(device)\n",
        "test_r = torch.FloatTensor(test.rating.values).to(device)\n",
        "\n",
        "print(\"âœ… Data converted to Tensors and ready for training.\")"
      ],
      "metadata": {
        "id": "3Lb4diJLkSHx",
        "outputId": "c7de9f87-ee3a-40de-d5b9-b032357c4128",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3Lb4diJLkSHx",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data converted to Tensors and ready for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss() # Mean Squared Error Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "print(\"ðŸš€ Starting Training Loop...\")\n",
        "epochs = 10\n",
        "batch_size = 64 # Processing in batches for efficiency\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass: using a subset for quick demonstration\n",
        "    # You can use the full set if the computer resources allow\n",
        "    predictions = model(train_u[:100000], train_m[:100000])\n",
        "    loss = criterion(predictions, train_r[:100000])\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Training Loss (MSE): {loss.item():.4f}\")\n",
        "\n",
        "print(\"ðŸŽ‰ Training finished successfully!\")"
      ],
      "metadata": {
        "id": "Gm4y0C_MkXyf",
        "outputId": "07434f8b-5d5f-4645-802f-7bec0a773833",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Gm4y0C_MkXyf",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting Training Loop...\n",
            "Epoch 2/10 | Training Loss (MSE): 13.2538\n",
            "Epoch 4/10 | Training Loss (MSE): 12.6384\n",
            "Epoch 6/10 | Training Loss (MSE): 11.7753\n",
            "Epoch 8/10 | Training Loss (MSE): 10.6832\n",
            "Epoch 10/10 | Training Loss (MSE): 9.3912\n",
            "ðŸŽ‰ Training finished successfully!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}