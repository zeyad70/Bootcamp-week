{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "# Fine Tuning Pre-Trained Text Classifier Models\n",
    "\n",
    "Having explored how to build text processing pipelines and classifiers from the ground up, you are now ready to leverage more advanced and efficient techniques. Instead of training a model from scratch, which can be computationally intensive and require vast amounts of data, you'll use a pre-trained model. This approach utilizes a model that has already learned rich language patterns from enormous datasets, giving you a powerful head start through transfer learning.\n",
    "\n",
    "In this lab, you will focus on fine-tuning **DistilBERT**, a lighter and faster version of the formidable BERT model, to classify recipe titles. This process demonstrates how to adapt a general purpose language model for a specialized task. You'll also see how tools from the Hugging Face ecosystem streamline many of the manual data preparation steps, such as tokenization and padding.\n",
    "\n",
    "This lab will guide you through the following essential steps:\n",
    "\n",
    "* Loading the pre-trained DistilBERT model along with its specific tokenizer.\n",
    "* Preparing the recipe dataset using a custom `Dataset` class and an automated `DataCollatorWithPadding` for efficient batching.\n",
    "* Implementing two fine-tuning strategies: one where you update the entire model and another, more efficient method where you only train the final few layers.\n",
    "* Comparing the performance of both methods to evaluate the trade-offs between accuracy and computational cost.\n",
    "* Testing your model(s) on new, unseen recipe titles to assess its generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-8901-2345-67890abcdef1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-a7b8-9012-3456-7890abcdef12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 99\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431dd49-014b-4bcf-b5b2-4ff9c81b18ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-b8c9-0123-4567-890abcdef123",
   "metadata": {},
   "source": [
    "## Revisiting Recipe Dataset\n",
    "\n",
    "You will re-use the recipe dataset from the previous lab. As a reminder, this is a specialized subset of the large [Food.com Recipes and Interactions](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions), containing titles for recipes that have been clearly classified as either fruit-based or vegetable-based.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "* Load the `recipes_fruit_veg.csv` file into a pandas DataFrame.\n",
    "* Create a numerical `label` column from the text categories, mapping `'fruit'` to `0` and `'vegetable'` to `1`.\n",
    "* Extract the recipe names and numerical labels into two separate lists, `texts` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-c9d0-1234-5678-90abcdef1234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the filtered dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"recipes_fruit_veg.csv\")\n",
    "\n",
    "# Create the numerical 'label' column: 0 for 'fruit', 1 for 'vegetable'\n",
    "df['label'] = 1\n",
    "df.loc[df['category'] == 'fruit', 'label'] = 0\n",
    "\n",
    "# Extract the recipe names and labels into lists\n",
    "df_clean = df.dropna(subset=['name'])\n",
    "texts = df_clean['name'].tolist()\n",
    "labels = df_clean['label'].tolist()\n",
    "\n",
    "# Verify the dataset size and class distribution\n",
    "print(f\"Total samples for classification:  {len(texts)}\")\n",
    "print(f\"Fruit recipes:                     {labels.count(0)}, {round(labels.count(0)/(labels.count(0) + labels.count(1)) *100,1)} %\")\n",
    "print(f\"Vegetable recipes:                 {labels.count(1)}, {round(labels.count(1)/(labels.count(0) + labels.count(1)) *100,1)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4cf44-b22f-4afb-ad60-7d05b56a6142",
   "metadata": {},
   "source": [
    "### Previewing the `name`and `label` Columns\n",
    "\n",
    "Your data is now structured with the `name` and `label` columns.\n",
    "\n",
    "* Run the cell below to review a random sample of these training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5cd252-58b7-4373-b92b-986a3085af25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the number of random samples to display.\n",
    "num_samples = 10\n",
    "\n",
    "# Display a sample of name and label pairs.\n",
    "display(df[['name', 'label']].sample(num_samples, random_state=25).style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9-d0e1-2345-6789-0abcdef12345",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained Transformer\n",
    "\n",
    "In the previous lab, you built every part of your text classifier from scratch. The core difference in this lab is that you will replace several components you previously had to build yourself with highly optimized tools from the Hugging Face ecosystem.\n",
    "\n",
    "Specifically, you will be using the [DistilBERT](https://huggingface.co/distilbert-base-uncased) model. This involves loading two key components that are designed to work together:\n",
    "\n",
    "* **The Pre-trained Model**: This is a powerful neural network, DistilBERT, that has already learned to understand language from a massive amount of text. Its role is to provide a strong foundation of language understanding that you will adapt for your recipe classification task.\n",
    "\n",
    "* **The Tokenizer**: This is the bridge between your raw text and the model. It will translate your recipe titles into the specific numerical format the model was trained on. Each pre-trained model has its own specific tokenizer, and it is crucial to use the one that matches your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89cb5c-3c46-4a2a-be61-3ed033372701",
   "metadata": {},
   "source": [
    "* Execute the cell below to download the base DistilBERT model and tokenizer from the Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29e54b-f761-4803-8d35-a1c11a91e835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name=\"distilbert-base-uncased\"\n",
    "model_path=\"./distilbert-local-base\"\n",
    "\n",
    "# Ensure the model is downloaded\n",
    "helper_utils.download_bert(model_name, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f9a6c-ea33-4ef9-bc0b-3e890bdc6808",
   "metadata": {},
   "source": [
    "* Load the pre-trained transformer.\n",
    "    * `num_classes=2`: Attaches a new randomly initialized classification head with 2 output labels, preparing the model for your binary classification task.\n",
    "    \n",
    "**Note**: You will see a warning that some weights were \"newly initialized.\" This is expected. It confirms that you have successfully loaded the pre-trained DistilBERT base and attached a new, untrained classification head.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d708b3e-b4f4-4093-957d-4862f93e7500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_model, bert_tokenizer = helper_utils.load_bert(model_path, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2a3-4567-8901-cdef12345678",
   "metadata": {},
   "source": [
    "## Preparing Data for Training\n",
    "\n",
    "Now that you have your model, tokenizer, and data lists ready, the next step is to structure this data into the objects PyTorch requires for training. This process is simpler than in the previous lab because many of the manual steps you performed before, such as cleaning text with the `preprocess_text` function and building a custom `Vocabulary` class, are no longer necessary.\n",
    "\n",
    "The Hugging Face tokenizer handles this work for you. It performs the text cleaning, tokenization, and numerical conversion automatically inside the custom `Dataset` class you are about to create. You will define this `Dataset` to wrap your data and then use `DataLoaders` to create iterable batches.\n",
    "\n",
    "### `RecipeDataset` Dataset Class\n",
    "\n",
    "You will start by defining a `RecipeDataset` class, the purpose of which is to use your tokenizer to convert a single raw text sample into the required numerical tensors on the fly, right when the model needs it.\n",
    "\n",
    "* Define the `RecipeDataset` which will serve as a container for your data and manage the on the fly tokenization process.\n",
    "    * `__init__`: Initializes the dataset by storing your `texts`, `labels`, and the `tokenizer`.\n",
    "    * `__len__`: Returns the total number of samples in your dataset.\n",
    "    * `__getitem__`: This is the core method where the on the fly processing occurs. For each text sample, the single call to the `tokenizer` performs all the complex preprocessing steps you previously handled manually. It cleans the text, tokenizes it into sub words, converts tokens to numerical IDs using its built in vocabulary, and creates an attention mask. The method then combines these tensors with the correct label into a dictionary, ready for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2-a3b4-5678-9012-def123456789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RecipeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for text classification.\n",
    "\n",
    "    This Dataset class stores raw texts and their corresponding labels. It is\n",
    "    designed to work efficiently with a Hugging Face tokenizer, performing\n",
    "    tokenization on the fly for each sample when it is requested.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        \"\"\"\n",
    "        Initializes the RecipeDataset.\n",
    "\n",
    "        Args:\n",
    "            texts: A list of raw text strings.\n",
    "            labels: A list of integer labels corresponding to the texts.\n",
    "            tokenizer: A Hugging Face tokenizer instance for processing text.\n",
    "        \"\"\"\n",
    "        # Store the list of raw text strings.\n",
    "        self.texts = texts\n",
    "        # Store the list of integer labels.\n",
    "        self.labels = labels\n",
    "        # Store the tokenizer instance that will process the text.\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        # Return the size of the dataset based on the number of texts.\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves and processes one sample from the dataset.\n",
    "\n",
    "        For a given index, this method fetches the corresponding text and label,\n",
    "        tokenizes the text, and returns a dictionary of tensors.\n",
    "\n",
    "        Args:\n",
    "            idx: The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the tokenized inputs ('input_ids',\n",
    "            'attention_mask') and the 'labels' as tensors.\n",
    "        \"\"\"\n",
    "        # Get the raw text and label for the specified index.\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text, handling tasks like cleaning, numerical conversion,\n",
    "        # and truncation. Padding is handled later by a DataCollator.\n",
    "        encoding = self.tokenizer(text, truncation=True, max_length=512)\n",
    "\n",
    "        # Add the label to the encoding dictionary and convert it to a tensor.\n",
    "        encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        # Return the dictionary containing all processed data for the sample.\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd2f9b-3c53-45f6-b567-401e753f8c1d",
   "metadata": {},
   "source": [
    "* Create an instance of your `RecipeDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b02f7-e93a-4627-ab95-0874e0ccb6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the full dataset\n",
    "full_dataset = RecipeDataset(texts, labels, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3-b4c5-6789-0123-ef1234567890",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "* Divide your `full_dataset` into an 80% training set and a 20% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceb6f9-7e94-48ce-9ad2-f0dd9ff9bf1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the full dataset into an 80% training set and a 20% validation set.\n",
    "train_dataset, val_dataset = helper_utils.create_dataset_splits(\n",
    "    full_dataset, \n",
    "    train_split_percentage=0.8\n",
    ")\n",
    "\n",
    "# Print the number of samples in each set to verify the split.\n",
    "print(f\"Training samples:   {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7571826-d397-40a9-88bf-ce2d0bf7c5bb",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "In the previous lab, you addressed the challenge of batching variable-length text by writing custom `collate_fn` functions to manually pad sequences or create offsets. The Hugging Face `DataCollatorWithPadding` function automates this complex step for you. \n",
    "\n",
    "* Use [DataCollatorWithPadding](https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorWithPadding) and pass it your `bert_tokenizer`. It will automatically handle the dynamic padding of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491f4cc-d195-4d30-a6e6-300b041b941f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data collator handles dynamic padding for each batch\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d851d26-3b71-4c12-b13a-2a5b92fa56fb",
   "metadata": {},
   "source": [
    "* Create two `DataLoader` instances, `train_loader` and `val_loader`.\n",
    "    * `collate_fn=data_collator`: Passing your `data_collator` to create dynamically padded batches instead of the default PyTorch behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4-c5d6-7890-1234-f12345678901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the number of samples to process in each batch.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for the training set with `data_collator`\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=data_collator\n",
    "                         )\n",
    "\n",
    "# Create the DataLoader for the validation set with `data_collator`\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        collate_fn=data_collator\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5-d6e7-8901-2345-123456789012",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With the pre-trained DistilBERT model loaded and the `DataLoaders` fully configured, the foundational work is complete. You are now ready to begin the fine-tuning process.\n",
    "\n",
    "### Addressing Class Imbalance\n",
    "\n",
    "* Calculate class weights to address the data imbalance in your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a679256-2401-4833-bf38-278b92c2f29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract all labels from the training set to calculate class weights for handling imbalance.\n",
    "train_labels_list = [train_dataset.dataset.labels[i] for i in train_dataset.indices]\n",
    "    \n",
    "    \n",
    "# Use scikit-learn's utility to automatically calculate class weights.\n",
    "class_weights = compute_class_weight(\n",
    "    # The strategy for calculating weights. 'balanced' is automatic.\n",
    "    class_weight='balanced',\n",
    "    # The array of unique class labels (e.g., [0, 1]).\n",
    "    classes=np.unique(train_labels_list),\n",
    "    # The list of all training labels, used to count class frequencies.\n",
    "    y=train_labels_list\n",
    ")\n",
    "\n",
    "# Convert the NumPy array of weights into a PyTorch tensor of type float\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Print the final weights to verify the calculation.\n",
    "print(\"Calculated Class Weights:\")\n",
    "print(f\"  - Fruit (Class 0):     {class_weights[0]:.2f}\")\n",
    "print(f\"  - Vegetable (Class 1): {class_weights[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4040e-7743-42c9-a291-6e57f78eb278",
   "metadata": {},
   "source": [
    "### Configuring the Loss Function\n",
    "\n",
    "* Define `nn.CrossEntropyLoss` as your loss function, and pass your previously calculated `class_weights` tensor to the weight parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986c70f-6303-45f9-a3d3-fef84093db08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the CrossEntropyLoss function with the calculated `class_weights`.\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d9e65-e090-4ccf-81cc-02ae6f35905f",
   "metadata": {},
   "source": [
    "### Baseline Approach: Fine-Tuning the Entire Model\n",
    "\n",
    "First, you will take the standard approach: fine-tuning the *entire* DistilBERT model. This means that every parameter, from the initial embedding layers to the final classification layer, will have its weights updated during training. Keep in mind that we are beginning the training using the pre-trained weights and will continue to further train the model.\n",
    "\n",
    "This method adapts the whole model to the recipe classification task and will serve as your performance baseline. You will use the `training_loop` function to run the training process and see how well this approach works.\n",
    "\n",
    "* For each batch, it explicitly unpacks the `input_ids`, `attention_mask`, and `labels` required by the model.\n",
    "* It then fine-tunes all layers of the DistilBERT model on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24bfdc-dd97-412c-b8a8-adc1e2fe6335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Uncomment if you want to see the training loop function\n",
    "\n",
    "# helper_utils.display_function(helper_utils.training_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0069b-358c-492f-a2e1-6076edb1499f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the total number of epochs.\n",
    "num_epochs = 3\n",
    "\n",
    "# Call the training loop to start the full fine-tuning process.\n",
    "full_finetuned_bert, full_results = helper_utils.training_loop(\n",
    "    bert_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    loss_function, \n",
    "    num_epochs, \n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3ee10-4956-4d1e-b9c5-1f84ca3faf5d",
   "metadata": {},
   "source": [
    "* Print the validation metrics from the `results_bert` dictionary to review the performance of your fine-tuned model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9028914-941b-44dd-90c9-5154737b395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results \n",
    "helper_utils.print_final_results(full_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea9a30",
   "metadata": {},
   "source": [
    "### An Efficient Alternative: Partial Fine-Tuning\n",
    "\n",
    "While fine-tuning the entire model is effective, it can be computationally expensive. Now, you will explore a more efficient strategy known as **partial fine-tuning**. Instead of training the entire model, you will strategically freeze the majority of the model's layers and train only those most effective for adapting to the new task.\n",
    "\n",
    "First take a look at the architecture of the DistilBERT model you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf653b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb6392-edba-4eaf-8256-49e36011e1cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The decision of which layers to freeze is based on how transformers learn hierarchically:\n",
    "\n",
    "* **Earlier Layers**: \n",
    "The layers closer to the input learn general language features, such as grammar and basic word relationships. \n",
    "Since these features are useful for almost any task, they are often kept frozen. \n",
    "In your DistilBERT model, these are the `embeddings` and the **first four** `TransformerBlock` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ff223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "print(\"\\nEmbeddings: \\n\")\n",
    "print(bert_model.distilbert.embeddings)\n",
    "\n",
    "# first four TransformerBlock layers\n",
    "print(\"\\nFirst four TransformerBlock layers: \\n\")\n",
    "print(bert_model.distilbert.transformer.layer[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a48f28",
   "metadata": {},
   "source": [
    "* **Later Layers**: The layers closer to the output learn more complex and abstract features that become more specialized to the data they are trained on. These are the layers you typically want to unfreeze to adapt the model to the nuances of your new task. In your model, these are the **last two** `TransformerBlock` layers and the final classification layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef8017-d932-45c7-8cc6-4a18ac411750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last two TransformerBlock layers\n",
    "print(\"\\nLast two TransformerBlock layers: \\n\")\n",
    "print(bert_model.distilbert.transformer.layer[4:6])\n",
    "\n",
    "# final classification layers\n",
    "print(\"\\nFinal Classifier Layer: \\n\")\n",
    "print(bert_model.pre_classifier)\n",
    "print(bert_model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be458123",
   "metadata": {},
   "source": [
    "For the task at hand, you will unfreeze and train the **final classifier head** and the **last two transformer layers** (**later layers**). This allows the model to adjust its high level feature extraction to the nuances of recipe classification, while still leveraging the robust, general language understanding from its frozen layers.\n",
    "\n",
    "This approach tests a key hypothesis: can you achieve comparable performance to the baseline while saving significant computational resources?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e008a-6227-4c7e-8531-4bf3819de99d",
   "metadata": {},
   "source": [
    "* Your first step is to freeze all parameters in the model by setting their `requires_grad` attribute to `False`. This prevents their weights from being updated during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af696ef3-96bb-46b5-9105-23404548d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze ALL model parameters first\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80236d-7d5a-41df-8c5f-585dab144a12",
   "metadata": {},
   "source": [
    "* Next, you will unfreeze the **last two transformer layers** to make them trainable by setting their `requires_grad` attribute back to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c28eb-a782-452d-ab94-78b1dced3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last 2 transformer layers\n",
    "# Set the number of final transformer layers to unfreeze and train.\n",
    "layers_to_train = 2 \n",
    "\n",
    "# Access the list of all transformer layers in the DistilBERT model.\n",
    "transformer_layers = bert_model.distilbert.transformer.layer\n",
    "\n",
    "# Loop backwards from the end of the layer list for the number of layers you want to train.\n",
    "for i in range(layers_to_train):\n",
    "    # Select a layer using negative indexing (e.g., -1 for the last, -2 for the second to last).\n",
    "    layer_to_unfreeze = transformer_layers[-(i+1)]\n",
    "    \n",
    "    # Iterate through all parameters of the selected layer.\n",
    "    for param in layer_to_unfreeze.parameters():\n",
    "        # Set requires_grad to True to make the parameter trainable.\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1f7ba-8c52-476f-a6b2-3670f9424089",
   "metadata": {},
   "source": [
    "* The final step is to unfreeze the model's classification head, which consists of the `pre_classifier` and `classifier` layers, to ensure it can be trained on your new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15155c0f-1f80-4dcd-92a9-a9db4c10a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the classifier head\n",
    "# The final layers of the model must be made trainable to adapt to the new task.\n",
    "\n",
    "# For DistilBERT, this head consists of two linear layers.\n",
    "# Unfreeze the pre_classifier layer.\n",
    "for param in bert_model.pre_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze the final classifier layer.\n",
    "for param in bert_model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70356d0d-533a-4cb2-beb1-7db2369679a1",
   "metadata": {},
   "source": [
    "With your partial fine-tuning strategy configured, you will now execute the `training_loop` function to start the training process. This will handle the entire training process and return the trained model along with a dictionary of the final validation metrics.\n",
    "\n",
    "* For each batch, it explicitly unpacks the `input_ids`, `attention_mask`, and `labels` required by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a48d3-28de-43d8-a90b-e9fcbc69a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment if you want to see the training loop function\n",
    "\n",
    "# helper_utils.display_function(helper_utils.training_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c8be4-15b0-41a8-b524-01ea310cdfa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the total number of epochs.\n",
    "num_epochs = 3\n",
    "\n",
    "# Call the training loop to start the partial fine-tuning process.\n",
    "partial_finetuned_bert, partial_results = helper_utils.training_loop(\n",
    "    bert_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    loss_function, \n",
    "    num_epochs, \n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd1f46-ef99-4c96-ba12-4aae8e40b2fe",
   "metadata": {},
   "source": [
    "* Print the validation metrics from the `results_bert` dictionary to review the performance of your fine-tuned model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee852c32-854c-4f0c-a355-8c1d46edeab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results \n",
    "helper_utils.print_final_results(partial_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aada1cd-91c9-4176-a360-be65609fe5ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Comparing Fine-Tuning Approaches\n",
    "\n",
    "* Directly compare the performance of the two approaches: the full fine-tuning baseline and the efficient partial fine-tuning method.\n",
    "    * `full_results`: Contains the metrics from the full fine-tuning of the entire model.\n",
    "    * `partial_results`: Contains the metrics from the partial fine-tuning, where only the last two transformer layers and the classifier were trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80690400-0910-4b5c-acb7-00a495f78ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare your results\n",
    "helper_utils.display_results(full_results, partial_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4826c5-0a9a-4c40-a5e2-26e417338af5",
   "metadata": {},
   "source": [
    "Based on these results, it's clear that both models perform almost identically after just `3` epochs. As you must have noticed, this more efficient approach took less time to train since it updated far fewer parameters. This perfectly illustrates the core benefit of partial fine-tuning: achieving comparable, if not better, performance while saving valuable time and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8-a9b0-1234-5678-456789012345",
   "metadata": {},
   "source": [
    "## Testing the Fine-tuned BERT Model on New Examples\n",
    "\n",
    "Now for the final test. It's time to see how your fine-tuned model performs on completely new, unseen data. This is the best way to get a qualitative feel for how well your model has learned to generalize.\n",
    "\n",
    "* Define a `test_products` list containing a mix of new recipe titles. This list includes straightforward examples as well as more challenging ones to see where the model excels and where it might struggle.\n",
    "    * Feel free to add your own recipe titles to this list to test the model even further!\n",
    " \n",
    "**Note**: Remember, the model's predictions are based *only* on the words in the recipe's `name`. It was never shown the ingredients list, so it has no knowledge of whether fruits or vegetables are the dominant ingredient. A recipe's name can sometimes be misleading, and the model's classification will reflect only what it has learned from the title's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375825f-0e7e-4464-8a58-ac6485b65206",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_products = [\n",
    "    \"Blueberry Muffins\",                  # Expected: Fruit\n",
    "    \"Spinach and Feta Stuffed Chicken\",   # Expected: Vegetable\n",
    "    \"Classic Carrot Cake with Frosting\",  # Expected: Vegetable\n",
    "    \"Tomato and Basil Bruschetta\",        # Expected: Vegetable\n",
    "    \"Avocado Toast\",                      # Expected: Fruit\n",
    "    \"Zucchini Bread with Walnuts\",        # Expected: Vegetable\n",
    "    \"Lemon and Herb Roasted Chicken\",     # Expected: Fruit\n",
    "    \"Strawberry Rhubarb Pie\",             # Expected: Fruit\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fbe0f-e2d5-4dc6-8557-7d242efc5b3f",
   "metadata": {},
   "source": [
    "* Finally, loop through the `test_products` list to run the prediction for each recipe and see the model's final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975aa376-091d-429d-916e-54bfe71c5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment if you want to see the predict category function\n",
    "\n",
    "# helper_utils.display_function(helper_utils.predict_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9-b0c1-2345-6789-567890123456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through each test product\n",
    "for product in test_products:\n",
    "    # Call the prediction function with the required arguments\n",
    "    category = helper_utils.predict_category(\n",
    "        partial_finetuned_bert, # Try it with `full_finetuned_bert` as well.\n",
    "        bert_tokenizer,\n",
    "        product,\n",
    "        device\n",
    "    )\n",
    "    # Print the results\n",
    "    print(f\"Product: '{product}'\\nPredicted: {category}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf08ebd-51a5-44a9-948d-ebb669a98b12",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing this lab! You have successfully moved beyond building models from scratch and have now fine-tuned a state-of-the-art transformer model for a custom text classification task.\n",
    "\n",
    "You began by loading a pre-trained DistilBERT model and saw firsthand how it simplifies the entire text-to-tensor pipeline. The main takeaway from your experiments is the effectiveness of **partial fine-tuning**. You demonstrated that by strategically freezing most of the model's layers and only updating the final, task-specific ones, you can achieve performance comparable to or even slightly better than fully fine-tuning the entire model. This insight is immensely valuable for practical applications, as it allows for significant savings in training time and computational resources without sacrificing quality.\n",
    "\n",
    "The skills you've developed here, loading and adapting pre-trained models, managing data with modern tools, and strategically choosing which parts of a model to trainâ€”are the building blocks for tackling a wide range of complex NLP challenges, from sentiment analysis to machine translation and beyond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
