{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0IKDtFpSTKK"
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "You've learned how to process text by tokenizing it and converting it into tensors, a necessary step for any neural network. However, the integer IDs assigned to words don't inherently capture their meaning or relationships. The number for \"cat\" is no more related to the number for \"dog\" than it is to the one for \"banana\". This is where **word embeddings** come in. Embeddings are dense vector representations that map words into a multi-dimensional space where semantic relationships can be measured mathematically.\n",
    "\n",
    "In this lab, you'll get hands-on experience with the core concepts behind embeddings and see how they are implemented in PyTorch.\n",
    "\n",
    "* You will start by loading **pre-trained GloVe embeddings** to explore how they capture semantic similarity and solve analogies like `king - man + woman ≈ queen`.\n",
    "* Next, you'll **visualize** these high-dimensional vectors in 2D space to see how related concepts form distinct clusters.\n",
    "* Then, you will **build a simple embedding model from scratch**, defining your own vocabulary and training it to learn word relationships on a specific task.\n",
    "* Finally, you will investigate the limitations of static embeddings with words that have multiple meanings and see how **contextual models like BERT** provide a more dynamic and powerful solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXoTg7vdSTKM",
    "outputId": "f7440be6-f0a6-49cd-8692-5710cf984cca",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "As you already know, at their core, machine learning models understand numbers, not words. **Word Embeddings** are the solution to this problem for text-based models. They are numerical vector representations of words that are designed to capture their semantic meaning, relationships, and context.\n",
    "\n",
    "There are two main categories of word embeddings:\n",
    "\n",
    "* **Static Embeddings**: This is the classic approach where each word in the vocabulary is mapped to a single, fixed vector. Models like **Word2Vec** and **GloVe** use this method. The vector for the word \"Apple\" is the same, regardless of whether you're talking about the fruit or the technology company.\n",
    "\n",
    "* **Dynamic (or Contextual) Embeddings**: This is a more advanced approach where the vector for a word changes based on the sentence it's in. Models like **BERT** excel at this, generating different vectors for \"Apple\" depending on the surrounding context.\n",
    "\n",
    "You’ll start by looking at a powerful pre-trained static embedding model called **GloVe**.\n",
    "\n",
    "### GloVe: Static Embeddings\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/projects/glove/), which stands for **Global Vectors for Word Representation**, is one of the most popular pre-trained static embedding models. It's an unsupervised learning algorithm that learns word vectors by analyzing a massive corpus of text and computing aggregated global word-word co-occurrence statistics, essentially learning from how frequently words appear near each other.\n",
    "\n",
    "The creators of GloVe provide several pre-trained models, each trained on different large-scale datasets:\n",
    "\n",
    "* **glove.6B**: Trained on Wikipedia and Gigaword text, containing 6 billion tokens and a 400,000-word vocabulary.\n",
    "\n",
    "* **glove.42B** & **glove.840B**: Trained on a massive web dataset (Common Crawl) with 42 billion and 840 billion tokens, respectively. These have much larger vocabularies (1.9M and 2.2M words).\n",
    "\n",
    "* **glove.twitter.27B**: A model trained specifically on 2 billion tweets with 27 billion tokens.\n",
    "\n",
    "While the larger models trained on Common Crawl offer vast vocabularies, the **glove.6B** model is an extremely popular starting point for many general-purpose tasks due to its manageable size and powerful performance.\n",
    "\n",
    "You will be using this **glove.6B** version. Run the next cells to download and load the pre-trained GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the data for the GloVe 6B 100d model\n",
    "helper_utils.download_glove6B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the path to the 100d GloVe file\n",
    "glove_file = './glove_data/glove.6B.100d.txt'\n",
    "\n",
    "# Load the pre-trained word vectors from the file\n",
    "glove_embeddings = helper_utils.load_glove_embeddings(glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Similarity with GloVe\n",
    "\n",
    "One of the most powerful features of word embeddings is their ability to capture **semantic similarity**. This means that words with similar meanings will have vector representations that are close to each other in the vector space.\n",
    "\n",
    "You can measure this closeness using a metric called **cosine similarity**. This metric calculates the cosine of the angle between two vectors. A higher cosine similarity (closer to 1) indicates that the words are more semantically related, while a lower value (closer to 0 or -1) suggests they are not. For example, you would expect the vector for \"cat\" to have a high cosine similarity with \"dog,\" but a very low similarity with \"car\".\n",
    "\n",
    "Beyond simple similarity, these embeddings can also capture more complex relationships and analogies. The most famous example is that the vector relationship between `king` and `man` is similar to that between `queen` and `woman`. You can test this with vector arithmetic: `king - man + woman ≈ queen`.\n",
    "\n",
    "* Define the `find_closest_words` function, which takes a target vector, the entire embedding dictionary, an optional list of words to exclude, and a `top_n` parameter. Its goal is to return a list of the top `n` most semantically similar words along with their similarity scores.\n",
    "* To find the closest semantic matches, this function uses a highly efficient vectorized approach to calculate the cosine similarity between the target vector and all other word vectors in a single operation. It sorts the similarity scores to find the top `n` matches and returns them with their corresponding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_closest_words(embedding, embeddings_dict, exclude_words=[], top_n=5):\n",
    "    \"\"\"\n",
    "    Finds the N most semantically similar words to a given vector and their scores.\n",
    "\n",
    "    Args:\n",
    "        embedding: The vector representation of the target word.\n",
    "        embeddings_dict: A dictionary mapping words to their embedding vectors.\n",
    "        exclude_words: A list of words to exclude from the search.\n",
    "        top_n: The number of most similar words to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains a word and its\n",
    "        cosine similarity score, sorted in descending order of similarity.\n",
    "        Returns None if the vocabulary is empty after exclusions.\n",
    "    \"\"\"\n",
    "    # Filter the vocabulary to remove any words in the exclude list.\n",
    "    filtered_words = [word for word in embeddings_dict.keys() if word not in exclude_words]\n",
    "    \n",
    "    # Handle the edge case where the filtered vocabulary is empty.\n",
    "    if not filtered_words:\n",
    "        return None\n",
    "        \n",
    "    # Create a matrix of all word vectors for efficient computation.\n",
    "    embedding_matrix = np.array([embeddings_dict[word] for word in filtered_words])\n",
    "    \n",
    "    # Reshape the target embedding to a 2D array for the similarity function.\n",
    "    target_embedding = embedding.reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity between the target and all other words.\n",
    "    similarity_scores = cosine_similarity(target_embedding, embedding_matrix)\n",
    "    \n",
    "    # Get the indices of the top N words with the highest similarity scores.\n",
    "    closest_word_indices = np.argsort(similarity_scores[0])[::-1][:top_n]\n",
    "    \n",
    "    # Create a list of (word, score) tuples for the top N closest words.\n",
    "    return [(filtered_words[i], similarity_scores[0][i]) for i in closest_word_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, check if the words \"king,\" \"man,\" and \"woman\" exist in the GloVe vocabulary and retrieve their corresponding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure the words exist in the glove_embeddings\n",
    "if all(word in glove_embeddings for word in ['king', 'man', 'woman']):\n",
    "    king = glove_embeddings['king']\n",
    "    man = glove_embeddings['man']\n",
    "    woman = glove_embeddings['woman']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, perform the vector calculation `king - man + woman` to find the resulting embedding for the analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The resulting vector for the analogy\n",
    "result_embedding = king - man + woman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, use the `find_closest_words` function to search the vocabulary for the list of words that are semantically closest to the calculated `result_embedding`.\n",
    "    * The original words (`king`, `man`, `woman`) are excluded from the search. This prevents the function from simply returning one of the input words, which would not be a meaningful answer to the analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set top N words\n",
    "top_n = 5\n",
    "\n",
    "# Find the top N closest words, making sure to exclude the inputs\n",
    "closest_words_with_scores = find_closest_words(\n",
    "    result_embedding, \n",
    "    glove_embeddings, \n",
    "    exclude_words=['king', 'man', 'woman'],\n",
    "    top_n=top_n\n",
    ")\n",
    "\n",
    "# Check if any words were returned\n",
    "if closest_words_with_scores:\n",
    "    # Unpack the top result (word and score)\n",
    "    top_word, top_score = closest_words_with_scores[0]\n",
    "\n",
    "    # Print the top result in the original format with its score\n",
    "    print(f\"king - man + woman ≈ {top_word} (Score: {top_score:.4f})\")\n",
    "\n",
    "    # Print the other 4 results\n",
    "    if len(closest_words_with_scores) > 1:\n",
    "        print(f\"\\n--- Other Top {top_n-1} Results ---\")\n",
    "        # Loop through the rest of the list of tuples\n",
    "        for word, score in closest_words_with_scores[1:]:\n",
    "            print(f\"{word} (Score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This result is a powerful demonstration of how word embeddings capture deep linguistic relationships, allowing you to solve complex analogies with simple vector math. To further see how these vectors group related concepts, you'll now visualize them.\n",
    "\n",
    "Feel free to experiment with other analogies to see what other relationships the model has learned. For example, you could try:\n",
    "\n",
    "* `france - paris + tokyo` (to find the country for a given capital)\n",
    "\n",
    "* `walking - walk + swim` (to find the progressive tense of a verb)\n",
    "\n",
    "* `uncle - man + woman` (to find the female equivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your analogy\n",
    "\n",
    "# Your analogy will follow this format:\n",
    "# word1 - word2 + word3 = ???\n",
    "\n",
    "# Define your words\n",
    "word1 = ''\n",
    "word2 = ''\n",
    "word3 = ''\n",
    "\n",
    "# Set how many top results you want to see\n",
    "top_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A list of the words used in the analogy\n",
    "analogy_words = [word1, word2, word3]\n",
    "\n",
    "# Check if all the words exist in the embeddings dictionary\n",
    "if all(word in glove_embeddings for word in analogy_words):\n",
    "    # Get the embedding vector for each word\n",
    "    embedding1 = glove_embeddings[word1]\n",
    "    embedding2 = glove_embeddings[word2]\n",
    "    embedding3 = glove_embeddings[word3]\n",
    "\n",
    "    # Perform the vector arithmetic to find the resulting embedding\n",
    "    result_embedding = embedding1 - embedding2 + embedding3\n",
    "\n",
    "    # Find the top N closest words to the result, excluding the input words\n",
    "    closest_words_with_scores = find_closest_words(\n",
    "        result_embedding, \n",
    "        glove_embeddings, \n",
    "        exclude_words=analogy_words,\n",
    "        top_n=top_n\n",
    "    )\n",
    "\n",
    "    # Check if the function returned any similar words\n",
    "    if closest_words_with_scores:\n",
    "        # Get the top word and its similarity score\n",
    "        top_word, top_score = closest_words_with_scores[0]\n",
    "\n",
    "        # Print the analogy and its top result\n",
    "        print(f\"'{word1}' - '{word2}' + '{word3}' ≈ '{top_word}' (Score: {top_score:.4f})\")\n",
    "\n",
    "        # Check if there are other results to display\n",
    "        if len(closest_words_with_scores) > 1:\n",
    "            print(f\"\\n--- Other Top {len(closest_words_with_scores) - 1} Results ---\")\n",
    "            # Loop through the rest of the results and print them\n",
    "            for word, score in closest_words_with_scores[1:]:\n",
    "                print(f\"{word} (Score: {score:.4f})\")\n",
    "    else:\n",
    "        # Message if no similar words were found\n",
    "        print(\"Could not find any similar words for the given analogy.\")\n",
    "\n",
    "else:\n",
    "    # Find and report which words are missing from the vocabulary\n",
    "    missing_words = [word for word in analogy_words if word not in glove_embeddings]\n",
    "    print(f\"Error: The following word(s) were not found in the vocabulary: {missing_words}\")\n",
    "    print(\"Please try different words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing GloVe Embeddings\n",
    "\n",
    "So far, you've confirmed these semantic relationships numerically with vector arithmetic. Another powerful way to understand the structure of these embeddings is to visualize them. However, plotting a vector is not straightforward.\n",
    "\n",
    "Each word embedding is a vector with a specific number of **dimensions**. For the GloVe model you have loaded, that is 100 dimensions. Each of these dimensions captures a different abstract feature of the word's meaning and its relationships with other words. While having many dimensions allows the model to store rich, nuanced information, it also creates a challenge: it is impossible for us to visualize a 100-dimensional space directly.\n",
    "\n",
    "To solve this, you can use a powerful dimensionality reduction technique called **Principal Component Analysis (PCA)**. PCA analyzes the data to find the directions of maximum variance and projects the original high dimensional vectors onto a new, lower dimensional space. In simple terms, it helps you squish the 100 dimensions down to just two, an x and y coordinate, while preserving as much of the original semantic structure as possible.\n",
    "\n",
    "By doing this, you can create a 2D plot to see how GloVe groups related concepts.\n",
    "\n",
    "* To see this in action, define a `words_to_visualize` list containing the words you'll visualize.\n",
    "    * Notice that the words in this list can be grouped into clear categories: types of vehicles, types of pets and types of fruits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the vocabulary of words from different categories to visualize.\n",
    "words_to_visualize = ['car', 'bike', 'plane',      # Category: Vehicles\n",
    "                      'cat', 'dog', 'bird',        # Category: Pets\n",
    "                      'orange', 'apple', 'grape'   # Category: Fruits\n",
    "]\n",
    "\n",
    "# A dictionary grouping the same words from `words_to_visualize` by category for easy visualization\n",
    "visualization_dict = {\n",
    "    'Vehicle': ['car', 'bike', 'plane'],\n",
    "    'Pet': ['cat', 'dog', 'bird'],\n",
    "    'Fruit': ['orange', 'apple', 'grape']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, retrieve the GloVe embedding vector for each word in the words_to_visualize list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the vectors.\n",
    "embedding_vectors_list = []\n",
    "\n",
    "# Loop through each word in the `words_to_visualize` list.\n",
    "for word in words_to_visualize:\n",
    "    # Get the embedding for the word and add it to the list.\n",
    "    embedding_vectors_list.append(glove_embeddings[word])\n",
    "\n",
    "# Convert the list of vectors into a NumPy array.\n",
    "embedding_vectors = np.array(embedding_vectors_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use PCA to reduce the 100-dimensional embedding vectors down to just two dimensions so they can be plotted.\n",
    "    * The `n_components=2` parameter specifies that you want to reduce the data to a two-dimensional representation, creating an x and y coordinate for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the PCA model to reduce dimensions to 2\n",
    "reducer = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the embedding vectors to get 2D coordinates\n",
    "coords_2d = reducer.fit_transform(embedding_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the reduced 2D embeddings to see how the words are clustered.\n",
    "\n",
    "    * **Note**: If you have made any changes to the `words_to_visualize` list, make sure they are appropriately reflected in the `visualization_dict` dictionary as well, otherwise the plot will return an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.plot_embeddings(coords=coords_2d, \n",
    "                             labels=words_to_visualize,\n",
    "                             label_dict=visualization_dict,\n",
    "                             title='GloVe Pre-Trained Embeddings'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above visually confirms that the GloVe model understands word relationships. Even after compressing the vectors from 100 dimensions down to two, the words form distinct clusters based on their semantic category:\n",
    "\n",
    "* The **vehicles** (`car`, `bike`, `plane`) are grouped together.\n",
    "\n",
    "* The **pets** (`cat`, `dog`, `bird`) form a second distinct cluster.\n",
    "\n",
    "* The **fruits** (`orange`, `apple`, `grape`) create a third group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogNupnu5STKM"
   },
   "source": [
    "## Building Your Own Embeddings From Scratch\n",
    "While pre-trained models like GloVe are powerful, there are many cases where you need to train your own embeddings. This is especially true when working with a specialized vocabulary (e.g., medical or financial terms) that may not be present in the pre-trained models, or when you want to capture semantic relationships specific to your dataset.\n",
    "\n",
    "In this section, you will learn the fundamental process of training embeddings. You'll start with a small, custom vocabulary and use PyTorch to build a simple model that learns vector representations from scratch.\n",
    "\n",
    "### Defining the Vocabulary and Parameters\n",
    "\n",
    "Before creating the model, you need to establish the foundational pieces. This involves defining the specific vocabulary of words the model will learn and setting up key parameters for the embedding layer, such as the size of the vocabulary and the dimension of the embedding vectors.\n",
    "\n",
    "* Define the `vocabulary` list. This will contain all the words from different semantic categories that your model will learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocabulary = ['car', 'bike', 'plane', \n",
    "              'cat', 'dog', 'bird', \n",
    "              'orange', 'apple', 'grape']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create two essential mappings from your `vocabulary` list: a `word_to_idx` dictionary to convert words into numerical indexes, and an `idx_to_word` dictionary to convert those indexes back into words.\n",
    "    * This is similar to the `build_vocab` function from the previous lab, but it's more direct because you are starting with a pre-defined list of unique words. Notice that this time the indexing starts from **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create word-to-index mapping\n",
    "# Initialize an empty dictionary for the word-to-index mapping\n",
    "word_to_idx = {}\n",
    "\n",
    "# Loop through the vocabulary list with an index\n",
    "for i, word in enumerate(vocabulary):\n",
    "    # Assign each word to its corresponding index\n",
    "    word_to_idx[word] = i\n",
    "    \n",
    "# Create index-to-word mapping\n",
    "# Initialize an empty dictionary for the index-to-word mapping\n",
    "idx_to_word = {}\n",
    "\n",
    "# Loop through the items of the newly created word_to_idx dictionary\n",
    "for word, i in word_to_idx.items():\n",
    "    # Assign each index to its corresponding word\n",
    "    idx_to_word[i] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define the `vocab_size` and `embedding_dim` which are essential parameters for the model.\n",
    "    * `embedding_dim = 3`: Note that while you are using an embedding dimension of 3 for this simple example, a typical range for larger vocabularies is 100-300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the total number of unique words in your vocabulary\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Define the size of the embedding vector for each word\n",
    "embedding_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the word-to-index mapping to review it\n",
    "print(\"Vocabulary:\\tIndex:\")\n",
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"{word}:\\t\\t{idx}\")\n",
    "\n",
    "# Print the final parameters that will be used for the model\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Training Pairs\n",
    "\n",
    "Now that you've defined your vocabulary, the next step is to create the training data that will teach the model the relationships between these words. The goal is for the model to learn that words within the same category are similar.\n",
    "\n",
    "You can accomplish this with a simple prediction task. The model learns by being given an input word and trying to predict another word from that same category. This requires structuring your vocabulary into `(input_word, target_word)` pairs to serve as the training examples.\n",
    "\n",
    "* First, structure your vocabulary into distinct categories using a Python dictionary. This dictionary will map a category name (like 'Vehicles') to a list of words belonging to that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the vocabulary, grouped by semantic category.\n",
    "vocab_categories = {\n",
    "    'Vehicles': ['car', 'bike', 'plane'],\n",
    "    'Pets': ['cat', 'dog', 'bird'],\n",
    "    'Fruits': ['orange', 'apple', 'grape']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To teach the model that words like 'car', 'bike', and 'plane' are related, you need to create training pairs such as `('car', 'bike')`, `('car', 'plane')`, `('bike', 'plane')`, and so on for each category.\n",
    "* Instead of writing out every combination by hand, you can use Python's `itertools.permutations` function to generate these pairs automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the training pairs.\n",
    "training_pairs = []\n",
    "\n",
    "# Iterate through the lists of words in the vocab_categories dictionary.\n",
    "for category_list in vocab_categories.values():\n",
    "    # Generate all permutations of 2 words from the list and add them to the training_pairs.\n",
    "    training_pairs.extend(list(permutations(category_list, 2)))\n",
    "\n",
    "# Display the total number of pairs and a sample of the generated pairs.\n",
    "print(f\"Generated {len(training_pairs)} training pairs.\")\n",
    "print(\"Generated pairs:\\n\")\n",
    "for pair in training_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model\n",
    "\n",
    "Now that you have your training data, you need a model that can learn from it. You'll create a simple neural network with two key layers:\n",
    "\n",
    "* <code>[nn.Embedding](https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html)</code>: It acts as a lookup table where each word's index (like `3` for 'cat') maps to a specific vector. During training, the model uses this vector to make a prediction and then adjusts it to better capture the word's relationships, turning the initially random vector into a meaningful one.\n",
    "\n",
    "* **nn.Linear**: This is a standard fully connected layer. It will take the word vector from the embedding layer and produce a prediction for the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a `SimpleEmbeddingModel`.\n",
    "    * The model is initialized with `vocab_size` and `embedding_dim`.\n",
    "    * It contains an `embedding` **layer** that acts as a lookup table for word vectors.\n",
    "    * It also contains a `linear` **layer** that produces the final prediction scores.\n",
    "* Define the `forward` pass for the model.\n",
    "    * It takes a word's index (`x`) as input.\n",
    "    * It retrieves the word's vector from the `embedding` layer.\n",
    "    * It passes that vector to the `linear` layer to get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleEmbeddingModel(nn.Module):\n",
    "    \"\"\"A simple neural network model for learning word embeddings.\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: The total number of unique words in the vocabulary.\n",
    "            embedding_dim: The desired dimensionality of the word embeddings.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class (nn.Module).\n",
    "        super().__init__()\n",
    "        \n",
    "        # An embedding layer that maps word indices to dense vectors.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # A linear layer that projects the embedding vector to the vocabulary size.\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: A tensor of input word indices.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the output logits from the linear layer and\n",
    "            the intermediate embedding vectors.\n",
    "        \"\"\"\n",
    "        # The input 'x' is passed through the embedding layer.\n",
    "        embedded = self.embedding(x)\n",
    "        # The resulting embedding vector is passed through the linear layer.\n",
    "        output = self.linear(embedded)\n",
    "        \n",
    "        return output, embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = SimpleEmbeddingModel(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function\n",
    "\n",
    "* Define the `Adam` optimizer.\n",
    "* Define Cross Entropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Adam optimizer\n",
    "optimizer = torch.optim.Adam(embedding_model.parameters(), lr=0.01)\n",
    "\n",
    "# Initialize the CrossEntropyLoss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Embeddings with a Simple Model\n",
    "\n",
    "This training loop follows the same core logic you've seen before: it iterates for multiple **epochs**, performs a **forward pass**, calculates **loss**, and uses a **backward pass** to update the model.\n",
    "\n",
    "The key difference is how it handles the data. Instead of working with images and labels, this loop is designed for word pairs.\n",
    "* **Data Iteration**: Rather than using a `DataLoader` that yields batches of `(image, label)`, this loop iterates directly over your simple list of `(word1, word2)` pairs.\n",
    "\n",
    "* **Text-to-Index Conversion**: The most significant difference is the data preparation step inside the loop. For each pair, the string words (e.g., 'car', 'bike') must be converted into their numerical indices using the `word_to_idx` mapping. This is the essential step for processing text.\n",
    "\n",
    "* **Input and Target**: In this setup, the index of the first word in the pair (`word1_idx`) is the input to the model. The index of the second word (`word2_idx`) acts as the target label that the loss function uses to measure error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(model, training_pairs, epochs=2000):\n",
    "    \"\"\"\n",
    "    Trains a simple word embedding model.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to be trained.\n",
    "        training_pairs: A list of tuples, where each tuple is an\n",
    "                        (input_word, target_word) pair.\n",
    "        epochs: The total number of training iterations over the dataset.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the trained model and a list of the average\n",
    "        loss for each epoch.\n",
    "    \"\"\"\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "    # Initialize a list to store the loss value for each epoch.\n",
    "    losses = []\n",
    "\n",
    "    # Loop over the dataset for a specified number of epochs.\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize the total loss for the current epoch.\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Loop through each input-target pair in the training data.\n",
    "        for word1, word2 in training_pairs:\n",
    "            # Convert the string words into their corresponding numerical indices.\n",
    "            word1_idx = torch.tensor([word_to_idx[word1]])\n",
    "            word2_idx = torch.tensor([word_to_idx[word2]])\n",
    "\n",
    "            # Perform a forward pass to get the model's predictions.\n",
    "            output, _ = model(word1_idx)\n",
    "            # Calculate the loss between the predictions and the actual target.\n",
    "            loss = loss_function(output, word2_idx)\n",
    "\n",
    "            # Clear any previously calculated gradients before the backward pass.\n",
    "            optimizer.zero_grad()\n",
    "            # Compute the gradient of the loss with respect to model parameters.\n",
    "            loss.backward()\n",
    "            # Update the model's weights based on the computed gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for the current epoch.\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate the average loss for the epoch and store it.\n",
    "        losses.append(epoch_loss / len(training_pairs))\n",
    "\n",
    "        # Periodically print the training progress.\n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "    # Print the final loss after training is complete.\n",
    "    print(f\"Epoch {epochs}, Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the next cell to start training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_model, losses = training_loop(embedding_model, training_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Training Loss\n",
    "\n",
    "* Run the next cell to plot the loss values from each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.plot_loss(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC6shTiQSTKN"
   },
   "source": [
    "### Exploring Semantic Similarity\n",
    "\n",
    "Now that the model is trained, you can check if it has learned meaningful relationships. You'll do this using the same **cosine similarity** metric you explored earlier with the GloVe embeddings.\n",
    "\n",
    "The goal is to see high similarity scores for words within the same category (e.g., 'car' and 'bike') and low scores for words in different categories (e.g., 'car' and 'cat'). This will confirm that the training was successful.\n",
    "\n",
    "* Define the `cosine_similarity_words` function.\n",
    "    * The function will take two words as input and return their similarity score based on the learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_words(word1, word2, word_to_idx, embeddings_matrix):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two words.\n",
    "\n",
    "    Args:\n",
    "        word1 (str): The first word to compare.\n",
    "        word2 (str): The second word to compare.\n",
    "        word_to_idx (dict): A mapping of words to their indices.\n",
    "        embeddings_matrix (np.ndarray): The matrix containing all word vectors.\n",
    "    \"\"\"\n",
    "    idx1 = word_to_idx[word1]\n",
    "    idx2 = word_to_idx[word2]\n",
    "\n",
    "    emb1 = embeddings_matrix[idx1]\n",
    "    emb2 = embeddings_matrix[idx2]\n",
    "\n",
    "    similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pull all the final word vectors from the `trained_model`. The most efficient way is to grab the entire matrix of embeddings at once.\n",
    "\n",
    "    * `trained_model.embedding.weight`: Directly accesses the final, learned word vectors from the embedding layer's weight matrix.\n",
    "    * `.detach().numpy()`: Converts the vector matrix from a PyTorch tensor into a NumPy array, making it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode.\n",
    "trained_model.eval()\n",
    "\n",
    "# Extract the embedding matrix.\n",
    "all_embeddings = trained_model.embedding.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Define test pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "similarity_tests = [\n",
    "    (\"car\",\"car\"),\n",
    "    (\"car\",\"bike\"),\n",
    "    (\"car\",\"plane\"),\n",
    "\n",
    "    (\"car\",\"cat\"),\n",
    "    (\"car\",\"dog\"),\n",
    "    (\"car\",\"bird\"),\n",
    "\n",
    "    (\"car\",\"orange\"),\n",
    "    (\"car\",\"apple\"),\n",
    "    (\"car\",\"grape\"),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, loop through the test pairs, calculate the similarity score for each one, and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WDNDQ69STKN",
    "outputId": "4b5e116f-0cac-48b4-8409-3c2b3262218a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Semantic Similarity (Cosine Similarity):\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Loop through each pair of words in the test list.\n",
    "for word1, word2 in similarity_tests:\n",
    "    # Calculate the similarity score for the current pair\n",
    "    similarity = cosine_similarity_words(word1, word2, word_to_idx, all_embeddings)\n",
    "    \n",
    "    # Print the word pair and their calculated similarity\n",
    "    print(f\"{word1} <-> {word2}:\\t {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To get a more comprehensive view than just testing a few pairs, calculate the similarity score between every possible combination of words in your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty square matrix with zeros to hold the similarity scores.\n",
    "similarity_matrix = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "# Iterate through each row of the matrix (representing the first word).\n",
    "for i in range(vocab_size):\n",
    "    # Iterate through each column (representing the second word).\n",
    "    for j in range(vocab_size):\n",
    "        # For any word compared with itself, the similarity is a perfect 1.0.\n",
    "        if i == j:\n",
    "            similarity_matrix[i, j] = 1.0\n",
    "        # For pairs of different words:\n",
    "        else:\n",
    "            # Get the string representation of each word from their indices.\n",
    "            word1 = idx_to_word[i]\n",
    "            word2 = idx_to_word[j]\n",
    "            # Calculate the similarity and place it in the correct cell of the matrix.\n",
    "            similarity_matrix[i, j] = cosine_similarity_words(word1, word2, word_to_idx, all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the next cell to plot the `similarity_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.plot_similarity_matrix(similarity_matrix, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSFVPdaocskm"
   },
   "source": [
    "The resulting matrix clearly shows that the `Vehicle` words have high similarity between them, but low similarity with the `Pet` and `Fruit` words, and viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_G54-O9STKN"
   },
   "source": [
    "## Visualizing Your Word Embeddings\n",
    "\n",
    "Now that your basic model is trained, time to see how well it captured the semantic meaning of your small vocabulary. You can then compare your results to the clusters you saw earlier from the pre-trained GloVe model.\n",
    "\n",
    "* First, use PCA again to reduce your 3-dimensional vectors to 2-dimensional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dimensionality\n",
    "reducer = PCA(n_components=2)\n",
    "coords = reducer.fit_transform(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the next cell to plot your trained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.plot_embeddings(coords=coords, \n",
    "                             labels=vocabulary,\n",
    "                             label_dict=vocab_categories,\n",
    "                             title='Your Trained Word Embeddings'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRbXuMuDdOhH"
   },
   "source": [
    "The plot confirms that the training was a success. The result is conceptually similar to the GloVe plot you saw earlier. The words have once again separated into their three distinct semantic categories.\n",
    "\n",
    "This powerfully demonstrates that even the simple model you built from scratch effectively learned the relationships between words, proving the core principle behind word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyGmoxUdSTKO"
   },
   "source": [
    "## Beyond Static Embeddings: The Importance of Context\n",
    "\n",
    "While training your own embeddings is a fantastic way to understand how they work, for many real-world applications, you don't need to start from scratch. As you saw in the first half of this notebook, pre-trained models offer a powerful and efficient starting point.\n",
    "\n",
    "**Static models** like **GloVe** are excellent, computationally lightweight tools that are perfect for a variety of tasks where the broader meaning of words is sufficient. They are a great choice for applications like:\n",
    "\n",
    "* General sentiment analysis\n",
    "* Document classification or topic modeling\n",
    "* Scenarios where speed and a smaller memory footprint are important\n",
    "\n",
    "### The Limitation: The \"Bat\" Problem\n",
    "\n",
    "However, the \"one word, one vector\" approach of static models has a key limitation when dealing with words that have multiple meanings (a concept known as **polysemy**).\n",
    "\n",
    "For example, consider the word \"bat\" in these two sentences:\n",
    "\n",
    "> 1. \"A bat flew out of the cave.\" (an animal)\n",
    ">\n",
    "> 2. \"He swung the baseball bat.\" (sports equipment)\n",
    "\n",
    "A static model like GloVe will produce the exact same vector for \"bat\" in both cases. This vector is an average of all the contexts the model saw during its training, so it can't distinguish between the animal and the sports equipment.\n",
    "\n",
    "Let's see this in action.\n",
    "\n",
    "* First, define the two sentences you'll use for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The sentences for comparison\n",
    "sentence1 = \"A bat flew out of the cave.\"\n",
    "sentence2 = \"He swung the baseball bat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, retrieve the specific pre-trained GloVe vector for the word `\"bat\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the specific vectors for \"bat\" from each sentence\n",
    "bat_from_sentence1 = glove_embeddings[\"bat\"]\n",
    "bat_from_sentence2 = glove_embeddings[\"bat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print the vectors for both of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Print vectors for the first sentence ---\n",
    "print(\"--- Sentence 1 (first 5 values) ---\")\n",
    "for word in sentence1.split():\n",
    "    # Clean the word to remove common punctuation\n",
    "    clean_word = word.strip('.,?!').lower()\n",
    "    \n",
    "    # Check if the clean word exists in the GloVe vocabulary\n",
    "    if clean_word in glove_embeddings:\n",
    "        vector = glove_embeddings[clean_word]\n",
    "        print(f\"{clean_word:<12} {vector[:5]}\")\n",
    "    else:\n",
    "        print(f\"{clean_word:<12} {'(not in vocabulary)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Print vectors for the second sentence ---\n",
    "print(\"--- Sentence 2 (first 5 values) ---\")\n",
    "for word in sentence2.split():\n",
    "    # Clean the word to remove common punctuation\n",
    "    clean_word = word.strip('.,?!').lower()\n",
    "    \n",
    "    # Check if the clean word exists in the GloVe vocabulary\n",
    "    if clean_word in glove_embeddings:\n",
    "        vector = glove_embeddings[clean_word]\n",
    "        print(f\"{clean_word:<12} {vector[:5]}\")\n",
    "    else:\n",
    "        print(f\"{clean_word:<12} {'(not in vocabulary)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As a final confirmation, compare the two vectors retrieved for the word \"bat\" to prove they are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if the two vectors for \"bat\" are identical\n",
    "are_identical = np.array_equal(bat_from_sentence1, bat_from_sentence2)\n",
    "print(f\"Are the vectors for 'bat' from each sentence identical? {are_identical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhOIVuRdNwXu"
   },
   "source": [
    "### The Solution: Contextual Embeddings with BERT\n",
    "\n",
    "This is where advanced models like [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) (Bidirectional Encoder Representations from Transformers) come in. Unlike static models, BERT is **contextual**.\n",
    "\n",
    "This means it generates a unique, dynamic vector for a word every time it appears, based on the specific sentence it's in.\n",
    "\n",
    "Let's see this in action.\n",
    "\n",
    "* Ensure the model is downloaded locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.download_bert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the tokenizer and the model into their respective variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the local path where the BERT model is saved.\n",
    "bert_path = './bert_model'\n",
    "\n",
    "# Load the tokenizer and model from the specified path.\n",
    "tokenizer, model_bert = helper_utils.load_bert(bert_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the contextual vector for each token in the sentences.\n",
    "\n",
    "* First, process the first sentence. Uses the BERT tokenizer to prepare the input and then feed it into the model.\n",
    "* Next, extract the `last_hidden_state`, which contains the final, context-aware embedding for every token in the sentence.\n",
    "* Finally, loop through each token and its corresponding vector to print them side-by-side.\n",
    "\n",
    "**Note**: The period (`.`) appears as a separate **token** because BERT's advanced tokenizer recognizes that punctuation has grammatical meaning; unlike the simple `.split()` method used in the GloVe example which required you to manually remove the period, the BERT tokenizer intentionally preserves it as a meaningful part of the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Process and Print Vectors for Sentence 1 ---\n",
    "print(\"--- Sentence 1 (first 5 values) ---\")\n",
    "# Tokenize the sentence and get the model's output\n",
    "inputs1 = tokenizer(sentence1, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs1 = model_bert(**inputs1)\n",
    "last_hidden_state1 = outputs1.last_hidden_state[0] # Embeddings for all tokens\n",
    "\n",
    "# Get the actual tokens from their IDs\n",
    "tokens1 = tokenizer.convert_ids_to_tokens(inputs1['input_ids'][0])\n",
    "\n",
    "# Loop through each token and its corresponding vector\n",
    "for token, vector in zip(tokens1, last_hidden_state1):\n",
    "    # Print the token and the first 5 dimensions of its contextual vector\n",
    "    print(f\"{token:<12} {vector.numpy()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now do the same with the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Process and Print Vectors for Sentence 2 ---\n",
    "print(\"--- Sentence 2 (first 5 values) ---\")\n",
    "# Tokenize the sentence and get the model's output\n",
    "inputs2 = tokenizer(sentence2, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs2 = model_bert(**inputs2)\n",
    "last_hidden_state2 = outputs2.last_hidden_state[0] # Embeddings for all tokens\n",
    "\n",
    "# Get the actual tokens from their IDs\n",
    "tokens2 = tokenizer.convert_ids_to_tokens(inputs2['input_ids'][0])\n",
    "\n",
    "# Loop through each token and its corresponding vector\n",
    "for token, vector in zip(tokens2, last_hidden_state2):\n",
    "    # Print the token and the first 5 dimensions of its contextual vector\n",
    "    print(f\"{token:<12} {vector.numpy()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* You can also perform the same identity check on the BERT vectors.\n",
    "    * The result will be `False`, confirming that BERT produced two unique vectors for the word `\"bat\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the vector for \"bat\" from the first sentence (at token index 2)\n",
    "bat_animal_vector = last_hidden_state1[2].numpy()\n",
    "# Extract the vector for \"bat\" from the second sentence (at token index 5)\n",
    "bat_sport_vector = last_hidden_state2[5].numpy()\n",
    "# Check if the two contextual vectors for \"bat\" are identical\n",
    "are_identical = np.array_equal(bat_animal_vector, bat_sport_vector)\n",
    "print(f\"Are the contextual BERT vectors for 'bat' identical? {are_identical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Takeaway: Which Model Should You Use?\n",
    "\n",
    "After seeing the difference, the natural question is which type of embedding to use. The answer depends entirely on your specific task and resources.\n",
    "\n",
    "**Use Static Embeddings (like GloVe) when:**\n",
    "\n",
    "* You're performing a straightforward task like document classification where nuanced context is less critical.\n",
    "* You need a fast, computationally lightweight solution with a smaller memory footprint.\n",
    "\n",
    "**Use Contextual Embeddings (like BERT) when:**\n",
    "\n",
    "* Your task requires a deep understanding of language and ambiguity (e.g., question-answering, text summarization, or advanced chatbots).\n",
    "* Handling words with multiple meanings is important for your application's success.\n",
    "* You have the computational resources to run larger, more complex models.\n",
    "\n",
    "## Conclusion\n",
    "In this lab, you have journeyed through the complete process of working with word embeddings, moving from abstract numbers to rich, meaningful vector representations. You began by loading a powerful pre-trained static model, GloVe, and saw firsthand how its vectors capture complex semantic relationships, allowing you to solve analogies with simple arithmetic. Visualizing these embeddings with PCA made it clear how words with similar meanings group together in vector space.\n",
    "\n",
    "You then built your own embedding model from the ground up, gaining a fundamental understanding of how these representations are learned during training. This highlighted a pivotal limitation of static models: their inability to handle words with multiple meanings. To solve this, you explored BERT, a contextual model that generates unique embeddings based on a word's surrounding text, providing a far more nuanced understanding of language.\n",
    "\n",
    "You now have the practical skills to load, train, visualize, and differentiate between static and contextual embeddings. This knowledge is an essential foundation for building more advanced neural networks that can perform sophisticated NLP tasks like text classification and sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
