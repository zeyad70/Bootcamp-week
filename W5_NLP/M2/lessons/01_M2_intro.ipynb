{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From Statistical to Neural NLP\n",
        "\n",
        "## Module 2 Transition: From Words to Embeddings\n",
        "\n",
        "Welcome to **Module 2: LLM-based NLP**! Before we dive into transformers and large language models, we need to understand how we got here. This session bridges the gap between what you learned in Module 1 (Statistical NLP) and what you'll learn in Module 2 (Deep Learning NLP).\n",
        "\n",
        "**What you'll learn in this session:** By the end of this hour, you'll understand the fundamental paradigm shifts in tokenization and vectorization that enabled modern NLP. You'll see how we evolved from simple word counting to dense semantic representations, and why this evolution was necessary for building powerful language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "This transition session covers three critical paradigm shifts:\n",
        "\n",
        "1. **Tokenization Evolution**: From word-level to subword-level tokenization\n",
        "2. **Vectorization Evolution**: From sparse (BoW/TF-IDF) to dense (embeddings) representations\n",
        "3. **Static vs. Contextual Embeddings**: Understanding how modern models handle word meaning\n",
        "\n",
        "These shifts are the foundation that makes transformers and LLMs possible. Understanding them will help you make informed decisions about when to use different approaches in your applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By the end of this session, you will be able to:\n",
        "\n",
        "- **Compare** word-level and subword-level tokenization approaches\n",
        "- **Explain** why subword tokenization solves the out-of-vocabulary (OOV) problem\n",
        "- **Understand** the conceptual shift from sparse to dense vector representations\n",
        "- **Distinguish** between static and contextual embeddings\n",
        "- **Recognize** when to use different tokenization and vectorization approaches\n",
        "- **Connect** Module 1 concepts to Module 2 concepts\n",
        "\n",
        "**Practical Goal**: You'll understand the \"why\" behind modern NLP tools, not just the \"how\", enabling you to make better technical decisions for your applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Glossary of Terms\n",
        "\n",
        "- **Tokenization**: The process of splitting text into smaller units (tokens)\n",
        "- **Word-level tokenization**: Splitting text into complete words\n",
        "- **Subword tokenization**: Splitting words into smaller units (subwords)\n",
        "- **OOV (Out-of-Vocabulary)**: Words not seen during training\n",
        "- **BPE (Byte Pair Encoding)**: A subword tokenization algorithm\n",
        "- **WordPiece**: Another subword tokenization algorithm used by BERT\n",
        "- **Sparse vector**: A high-dimensional vector with mostly zeros (e.g., BoW, TF-IDF)\n",
        "- **Dense vector**: A lower-dimensional vector with mostly non-zero values (embeddings)\n",
        "- **Static embedding**: A fixed vector representation for a word (same vector regardless of context)\n",
        "- **Contextual embedding**: A vector representation that changes based on the word's context in a sentence\n",
        "- **Distributional hypothesis**: The idea that words appearing in similar contexts have similar meanings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outline\n",
        "\n",
        "1. **The Evolution of NLP Approaches** - Statistical NLP vs. Deep Learning NLP\n",
        "2. **Tokenization Evolution** - From words to subwords\n",
        "3. **The OOV Problem** - Why word-level tokenization fails\n",
        "4. **Subword Tokenization** - How BPE and WordPiece solve OOV\n",
        "5. **Vectorization Evolution** - From sparse to dense\n",
        "6. **Why Dense Embeddings Matter** - Capturing semantic meaning\n",
        "7. **Static vs. Contextual Embeddings** - The polysemy problem\n",
        "8. **When to Use What** - Practical guidance for your applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. The Evolution of NLP Approaches\n",
        "\n",
        "Let's start by understanding the big picture: how NLP evolved from statistical methods to deep learning.\n",
        "\n",
        "![flat vector art diagram showing evolution from statistical NLP (left side with word clouds and TF-IDF formulas) to deep learning NLP (right side with neural network layers and transformer architecture), minimalist style, clean lines, white background, soft blue and orange accent colors](../assets/nlp_evolution.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Module 1: Statistical NLP (What You Learned)\n",
        "\n",
        "In Module 1, you learned **statistical NLP** approaches:\n",
        "\n",
        "- **Text preprocessing**: Heavy cleaning and normalization\n",
        "- **Tokenization**: Simple word-level splitting (whitespace-based)\n",
        "- **Vectorization**: Sparse vectors (Bag of Words, TF-IDF)\n",
        "- **Models**: Scikit-learn classifiers (logistic regression, naive bayes, etc.)\n",
        "- **Philosophy**: \"Super-fast librarian\" - organize and retrieve based on word statistics\n",
        "\n",
        "**Strengths**:\n",
        "- Fast and efficient\n",
        "- Interpretable (you can see which words matter)\n",
        "- Works well for keyword-based tasks\n",
        "- Low computational requirements\n",
        "\n",
        "**Limitations**:\n",
        "- Doesn't capture semantic relationships\n",
        "- Struggles with context and ambiguity\n",
        "- Can't handle words not in vocabulary\n",
        "- Limited understanding of word order and syntax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Module 2: Deep Learning NLP (What You'll Learn)\n",
        "\n",
        "In Module 2, you'll learn **deep learning NLP** approaches:\n",
        "\n",
        "- **Text preprocessing**: Minimal (mostly formatting)\n",
        "- **Tokenization**: Subword-level (BPE, WordPiece)\n",
        "- **Vectorization**: Dense embeddings (learned representations)\n",
        "- **Models**: Transformers (BERT, GPT, etc.)\n",
        "- **Philosophy**: learn meaning from context\n",
        "\n",
        "**Strengths**:\n",
        "- Captures semantic relationships\n",
        "- Handles context and ambiguity\n",
        "- Can process any text (subword tokenization)\n",
        "- Understands word order and syntax\n",
        "- Pre-trained on massive datasets\n",
        "\n",
        "**Trade-offs**:\n",
        "- Higher computational requirements\n",
        "- Less interpretable\n",
        "- Requires more data (or use pre-trained models)\n",
        "- More complex architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### When to Use Each Approach?\n",
        "\n",
        "**Use Statistical NLP (Module 1)** when:\n",
        "- You need fast, interpretable results\n",
        "- Your task is keyword-based (search, simple classification)\n",
        "- You have limited computational resources\n",
        "- You need to understand which words drive decisions\n",
        "\n",
        "**Use Deep Learning NLP (Module 2)** when:\n",
        "- You need semantic understanding\n",
        "- Context matters (sentiment, translation, QA)\n",
        "- You want to leverage pre-trained models\n",
        "- You have access to GPUs/cloud resources\n",
        "- You need state-of-the-art performance\n",
        "\n",
        "**In practice**: Many systems use both! Statistical methods for initial filtering, deep learning for complex understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Tokenization Evolution: From Words to Subwords\n",
        "\n",
        "Let's dive into the first major paradigm shift: how we split text into tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Module 1: Word-Level Tokenization\n",
        "\n",
        "In Module 1, you learned simple word-level tokenization:\n",
        "\n",
        "```python\n",
        "# Simple word-level tokenization (Module 1 approach)\n",
        "text = \"I love machine learning\"\n",
        "tokens = text.lower().split()  # ['i', 'love', 'machine', 'learning']\n",
        "```\n",
        "\n",
        "**How it works**:\n",
        "- Split text by whitespace\n",
        "- Optionally lowercase\n",
        "- Each word becomes one token\n",
        "- Simple and intuitive\n",
        "\n",
        "**Example**:\n",
        "- English: \"Natural language processing\" → `['natural', 'language', 'processing']`\n",
        "- Arabic: \"الذكاء الاصطناعي\" → `['الذكاء', 'الاصطناعي']` (after proper Arabic tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Play around with Tokenizers\n",
        "\n",
        "- https://tiktokenizer.vercel.app/\n",
        "- https://platform.openai.com/tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: I love machine learning and natural language processing\n",
            "Tokens: ['i', 'love', 'machine', 'learning', 'and', 'natural', 'language', 'processing']\n",
            "Number of tokens: 8\n",
            "\n",
            "Vocabulary (unique tokens): {'processing', 'and', 'i', 'learning', 'machine', 'language', 'love', 'natural'}\n"
          ]
        }
      ],
      "source": [
        "# Let's see word-level tokenization in action (Module 1 style)\n",
        "text = \"I love machine learning and natural language processing\"\n",
        "\n",
        "# Simple word-level tokenization\n",
        "tokens = text.lower().split()\n",
        "\n",
        "print(\"Text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Number of tokens:\", len(tokens))\n",
        "print(\"\\nVocabulary (unique tokens):\", set(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Problem: Out-of-Vocabulary (OOV) Words\n",
        "\n",
        "Word-level tokenization has a critical limitation: **what happens when you encounter a word you've never seen?**\n",
        "\n",
        "**Scenario**: You trained a model on a vocabulary of 10,000 words. Now you see:\n",
        "- \"unhappiness\" (not in vocabulary)\n",
        "- \"pre-trained\" (not in vocabulary)\n",
        "- \"BERT\" (not in vocabulary - it's a proper noun)\n",
        "\n",
        "**Word-level approach**: These words become `<UNK>` (unknown token) - you lose all information!\n",
        "\n",
        "**The vocabulary explosion problem**:\n",
        "- English has ~170,000 words\n",
        "- But with inflections, compounds, and new words, the vocabulary grows infinitely\n",
        "- You can't include every possible word in your vocabulary\n",
        "- New words appear constantly (slang, technical terms, proper nouns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New text: I feel unhappiness about the pre-trained model\n",
            "Tokens: ['i', 'feel', 'unhappiness', 'about', 'the', 'pre-trained', 'model']\n",
            "\n",
            "Checking vocabulary:\n",
            "  'i' ✓ (in vocabulary)\n",
            "  'feel' ✗ (OOV - becomes <UNK>)\n",
            "  'unhappiness' ✗ (OOV - becomes <UNK>)\n",
            "  'about' ✗ (OOV - becomes <UNK>)\n",
            "  'the' ✗ (OOV - becomes <UNK>)\n",
            "  'pre-trained' ✗ (OOV - becomes <UNK>)\n",
            "  'model' ✗ (OOV - becomes <UNK>)\n",
            "\n",
            "Problem: We lose information about 'unhappiness' and 'pre-trained'!\n"
          ]
        }
      ],
      "source": [
        "# Demonstrating the OOV problem with word-level tokenization\n",
        "\n",
        "# Let's say we have a vocabulary from training data\n",
        "training_vocab = {\n",
        "    'i', 'love', 'machine', 'learning', 'natural', 'language', \n",
        "    'processing', 'happy', 'sad', 'good', 'bad'\n",
        "}\n",
        "\n",
        "# New text with unseen words\n",
        "new_text = \"I feel unhappiness about the pre-trained model\"\n",
        "\n",
        "# Word-level tokenization\n",
        "tokens = new_text.lower().split()\n",
        "\n",
        "print(\"New text:\", new_text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"\\nChecking vocabulary:\")\n",
        "\n",
        "for token in tokens:\n",
        "    if token in training_vocab:\n",
        "        print(f\"  '{token}' ✓ (in vocabulary)\")\n",
        "    else:\n",
        "        print(f\"  '{token}' ✗ (OOV - becomes <UNK>)\")\n",
        "        \n",
        "print(\"\\nProblem: We lose information about 'unhappiness' and 'pre-trained'!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Module 2: Subword Tokenization\n",
        "\n",
        "**Solution**: Instead of treating words as atomic units, break them into smaller pieces (subwords)!\n",
        "\n",
        "**Key insight**: Most words are made of smaller, reusable pieces:\n",
        "- \"unhappiness\" = \"un\" + \"happi\" + \"ness\"\n",
        "- \"pre-trained\" = \"pre\" + \"-\" + \"train\" + \"ed\"\n",
        "- \"BERT\" = \"BERT\" (or could be \"B\" + \"ER\" + \"T\" if needed)\n",
        "\n",
        "**Benefits**:\n",
        "- Handle any word by combining subwords\n",
        "- Smaller vocabulary (reuse subwords across words)\n",
        "- Better for morphologically rich languages (like Arabic)\n",
        "- Can represent new words without `<UNK>`\n",
        "\n",
        "![flat vector art diagram showing subword tokenization breaking 'unhappiness' into 'un', 'happi', 'ness' subwords, with checkmarks, minimalist style, clean lines, white background, soft blue and orange accent colors](../assets/subword_solution.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Subword Tokenization Works\n",
        "\n",
        "**Two main algorithms**:\n",
        "\n",
        "1. **BPE (Byte Pair Encoding)**: \n",
        "   - Start with characters\n",
        "   - Iteratively merge most frequent pairs\n",
        "\n",
        "2. **WordPiece**: Similar to BPE but optimized for language modeling\n",
        "\n",
        "**Example**: \"unhappiness\"\n",
        "- Word-level: `['unhappiness']` → OOV if not in vocab\n",
        "- Subword-level: `['un', '##happi', '##ness']` → All subwords likely in vocab!\n",
        "\n",
        "**The `##` prefix**: In WordPiece, `##` indicates this subword continues from the previous one.\n",
        "\n",
        "**Arabic example**: \"الذكاء\" (intelligence)\n",
        "- Could be split into: `['ال', '##ذكاء']` or `['الذكاء']` depending on the tokenizer\n",
        "- Subwords help handle Arabic's rich morphology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-level tokenization:\n",
            "  Tokens: ['i', 'feel', 'unhappiness', 'about', 'the', 'pre-trained', 'model']\n",
            "  Count: 7\n",
            "  OOV risk: High (if 'unhappiness' or 'pre-trained' not in vocab)\n",
            "\n",
            "Subword-level tokenization:\n",
            "  Tokens: ['i', 'feel', 'un', '##happi', '##ness', 'about', 'the', 'pre', '-', 'train', '##ed', 'model']\n",
            "  Count: 12\n",
            "  OOV risk: Low (subwords like 'un', 'happi', 'ness' are common)\n",
            "\n",
            "Key insight: Even if 'unhappiness' is OOV, its subwords likely exist!\n"
          ]
        }
      ],
      "source": [
        "# Comparing word-level vs subword-level tokenization\n",
        "\n",
        "# Simulating subword tokenization (simplified example)\n",
        "# In reality, this is done by trained tokenizers\n",
        "\n",
        "text = \"I feel unhappiness about the pre-trained model\"\n",
        "\n",
        "# Word-level (Module 1 style)\n",
        "word_tokens = text.lower().split()\n",
        "print(\"Word-level tokenization:\")\n",
        "print(f\"  Tokens: {word_tokens}\")\n",
        "print(f\"  Count: {len(word_tokens)}\")\n",
        "print(f\"  OOV risk: High (if 'unhappiness' or 'pre-trained' not in vocab)\\n\")\n",
        "\n",
        "# Subword-level (Module 2 style - simplified simulation)\n",
        "# In practice, a tokenizer would do this automatically\n",
        "subword_tokens = ['i', 'feel', 'un', '##happi', '##ness', 'about', 'the', \n",
        "                  'pre', '-', 'train', '##ed', 'model']\n",
        "print(\"Subword-level tokenization:\")\n",
        "print(f\"  Tokens: {subword_tokens}\")\n",
        "print(f\"  Count: {len(subword_tokens)}\")\n",
        "print(f\"  OOV risk: Low (subwords like 'un', 'happi', 'ness' are common)\")\n",
        "\n",
        "print(\"\\nKey insight: Even if 'unhappiness' is OOV, its subwords likely exist!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison: Word-Level vs. Subword-Level\n",
        "\n",
        "| Aspect | Word-Level (M1) | Subword-Level (M2) |\n",
        "|--------|----------------|-------------------|\n",
        "| **Vocabulary size** | Large (10K-100K words) | Smaller (30K-50K subwords) |\n",
        "| **OOV handling** | `<UNK>` token (loses info) | Breaks into subwords (preserves info) |\n",
        "| **New words** | Cannot handle | Can handle by combining subwords |\n",
        "| **Morphology** | Treats each form separately | Shares subwords across forms |\n",
        "| **Complexity** | Simple (split by space) | More complex (learned algorithm) |\n",
        "| **Use case** | Statistical NLP, keyword search | Deep learning, transformers |\n",
        "\n",
        "**Takeaway**: Subword tokenization is essential for deep learning models because they need to handle any text, not just pre-seen words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Vectorization Evolution: From Sparse to Dense\n",
        "\n",
        "Now let's explore the second major paradigm shift: how we represent text as numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Module 1: Sparse Vectorization (BoW, TF-IDF)\n",
        "\n",
        "In Module 1, you learned **sparse vectorization**:\n",
        "\n",
        "**Bag of Words (BoW)**:\n",
        "- Count how many times each word appears\n",
        "- Create a vector with one dimension per word in vocabulary\n",
        "- Most values are 0 (sparse!)\n",
        "\n",
        "**Example**:\n",
        "- Vocabulary: `['i', 'love', 'machine', 'learning', 'python']` (5 words)\n",
        "- Text: \"I love machine learning\"\n",
        "- Vector: `[1, 1, 1, 1, 0]` (one 1 for each word present, 0 for 'python')\n",
        "\n",
        "**TF-IDF**: Similar, but weights words by importance (rare words get higher weights)\n",
        "\n",
        "**Characteristics**:\n",
        "- **High-dimensional**: One dimension per word (10K-100K dimensions)\n",
        "- **Sparse**: Most values are 0 (each document uses only ~100-200 words)\n",
        "- **Interpretable**: You can see which words are present\n",
        "- **No semantics**: \"cat\" and \"dog\" are as different as \"cat\" and \"banana\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrating sparse vectorization (Module 1 style)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Small corpus\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"I love python programming\",\n",
        "    \"Machine learning is fun\"\n",
        "]\n",
        "\n",
        "# Create sparse BoW vectors\n",
        "vectorizer = CountVectorizer()\n",
        "sparse_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\", list(vocab))\n",
        "print(f\"Vocabulary size: {len(vocab)} words\")\n",
        "print(f\"\\nSparse matrix shape: {sparse_matrix.shape}\")\n",
        "print(f\"Matrix density: {sparse_matrix.nnz / (sparse_matrix.shape[0] * sparse_matrix.shape[1]):.2%}\")\n",
        "\n",
        "# Show the dense representation (for visualization)\n",
        "dense_matrix = sparse_matrix.toarray()\n",
        "print(\"\\nDense representation (for visualization):\")\n",
        "print(dense_matrix)\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- High dimensionality: 7 dimensions for just 3 short documents\")\n",
        "print(\"- Very sparse: Most values are 0\")\n",
        "print(\"- No semantic relationships: 'machine' and 'learning' are independent dimensions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Problem: Sparse Vectors Don't Capture Meaning\n",
        "\n",
        "**Key limitation**: Sparse vectors treat words as independent. They don't capture relationships!\n",
        "\n",
        "**Example**:\n",
        "- \"cat\" → `[0, 0, 1, 0, 0, ...]` (1 in 'cat' dimension, 0 elsewhere)\n",
        "- \"dog\" → `[0, 1, 0, 0, 0, ...]` (1 in 'dog' dimension, 0 elsewhere)\n",
        "- \"banana\" → `[1, 0, 0, 0, 0, ...]` (1 in 'banana' dimension, 0 elsewhere)\n",
        "\n",
        "**Distance between vectors**:\n",
        "- Distance(\"cat\", \"dog\") = 2 (they differ in 2 dimensions)\n",
        "- Distance(\"cat\", \"banana\") = 2 (same distance!)\n",
        "\n",
        "**But semantically**: \"cat\" and \"dog\" are much more similar than \"cat\" and \"banana\"!\n",
        "\n",
        "**The problem**: Sparse vectors can't represent that \"cat\" and \"dog\" are both:\n",
        "- Animals\n",
        "- Pets\n",
        "- Mammals\n",
        "- Four-legged\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distributional Hypothesis\n",
        "\n",
        "> \"You shall know a word by the company it keeps.\" — J.R. Firth, 1957\n",
        "\n",
        "Here's the key insight: **words that appear near similar words have similar meanings.**\n",
        "\n",
        "**English examples**:\n",
        "- \"Dog\" and \"cat\" both appear near: \"pet\", \"animal\", \"furry\", \"feed\"\n",
        "- \"Bank\" (financial) appears near: \"money\", \"deposit\", \"account\", \"loan\"\n",
        "- \"Bank\" (river) appears near: \"river\", \"water\", \"shore\", \"fishing\"\n",
        "\n",
        "**Arabic examples**:\n",
        "- \"كلب\" (dog) and \"قطة\" (cat) both appear near: \"حيوان\" (animal), \"أليف\" (pet), \"طعام\" (food)\n",
        "- \"عين\" (eye) appears near: \"نظر\" (look), \"رؤية\" (vision), \"وجه\" (face)\n",
        "- \"عين\" (spring) appears near: \"ماء\" (water), \"نبع\" (source), \"شرب\" (drink)\n",
        "\n",
        "**The Distributional Hypothesis**: Words with similar distributions (contexts) have similar meanings.\n",
        "\n",
        "Let's see how we can learn word meaning from context alone. This demonstrates the core idea behind embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Discovering Meaning from Context\n",
        "\n",
        "Imagine you encounter a word you've never seen. How would you figure out what it means? You'd look at the words around it—the context.\n",
        "\n",
        "**Arabic example** (\"كتاب\" - kitab - book):\n",
        "If we see contexts like:\n",
        "- \"قرأت الكتاب\" (I read the **book**)\n",
        "- \"كتب في الكتاب\" (He wrote in the **book**)\n",
        "- \"اشترى كتاباً جديداً\" (He bought a new **book**)\n",
        "- \"الكتاب مفيد\" (The **book** is useful)\n",
        "\n",
        "Which gives us these many meanings to the wrod \"كتاب\":\n",
        "\n",
        "- It's something that can be **read**\n",
        "- It can be **written in**\n",
        "- It can be **bought/sold**\n",
        "- It can be **useful or not**\n",
        "- It's a **noun** (used as an object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try this with a real example from `text1` in `nltk`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's see how we can learn word meaning from context\n",
        "import nltk\n",
        "nltk.download(\"book\", quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
            "Contexts where 'apple' appears:\n",
            "======================================================================\n",
            "Displaying 4 of 4 matches:\n",
            " an idea first born on an undigested apple - dumpling ; and since then perpetua\n",
            " the whale , is much like halving an apple ; there is no intermediate remainder\n",
            "e of an anchor , or the crotch of an apple tree ), and then giving the word , h\n",
            "shook , and cast his last , cindered apple to the soil . \" What is it , what na\n"
          ]
        }
      ],
      "source": [
        "from nltk.book import text1\n",
        "\n",
        "# Show all instances of \"apple\" with surrounding context\n",
        "print(\"Contexts where 'apple' appears:\")\n",
        "print(\"=\" * 70)\n",
        "text1.concordance(\"apple\", width=80, lines=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The **English example** above (\"apple\"):\n",
        "From just these few examples, we can infer:\n",
        "- It can be **eaten** (\"undigested apple\")\n",
        "- It can be **cut** (\"halving an apple\")\n",
        "- It comes from a **tree** (\"apple tree\")\n",
        "- It's a **noun** (used as an object)\n",
        "\n",
        "> **Key insight**: The more contexts we see, the clearer the meaning becomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.book import text2\n",
        "\n",
        "# Show all instances of \"man\" with surrounding context\n",
        "print(\"Contexts where 'man' appears:\")\n",
        "print(\"=\" * 70)\n",
        "text2.concordance(\"man\", width=80, lines=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**This is what Dense Embeddings do** (explained next): They aggregate information from millions of contexts to build rich vector representations. More training data → better embeddings → better performance in your applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Module 2: Dense Embeddings\n",
        "\n",
        "**Solution**: Learn dense, low-dimensional vectors that capture semantic meaning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**How it works**:\n",
        "1. Train a neural network on millions of texts\n",
        "2. The network learns to predict words from context (or context from words)\n",
        "3. The learned internal representations become word embeddings\n",
        "4. Similar words end up with similar vectors\n",
        "\n",
        "**Characteristics**:\n",
        "- **Low-dimensional**: 100-768 dimensions (vs. 10K-100K for sparse)\n",
        "- **Dense**: Most values are non-zero\n",
        "- **Semantic**: Similar words have similar vectors\n",
        "- **Learned**: Representations come from training, not manual design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Dense Embeddings Capture Semantic Relationships\n",
        "\n",
        "**Sparse vectors (Module 1)**:\n",
        "\n",
        "* \"king\": `[0, 0, 1, 0, ...]` (Treats \"king\" as a unique ID)\n",
        "* \"queen\": `[0, 0, 0, 1, ...]` (Treats \"queen\" as a unique ID)\n",
        "* **Result:** Mathematically, they are completely different (orthogonal). The model doesn't know they are related.\n",
        "\n",
        "**Dense embeddings (Module 2)**:\n",
        "\n",
        "* \"king\": `[0.95, -0.8, 0.1, ...]` (High \"royalty\", Male)\n",
        "* \"man\": `[0.05, -0.9, 0.2, ...]` (Low \"royalty\", Male)\n",
        "* \"woman\": `[0.06, 0.85, 0.3, ...]` (Low \"royalty\", Female)\n",
        "* **Result:** The vectors capture relationships. If you perform the math operation: `king - man + woman`, the resulting vector is incredibly close to `queen`.\n",
        "\n",
        "**What the dimensions mean**:\n",
        "Instead of just matching keywords, the model has learned concepts:\n",
        "\n",
        "* One dimension might represent **\"Gender\"** (Positive for female, negative for male).\n",
        "* Another might represent **\"Royalty\"** (High for King/Queen, low for Man/Woman).\n",
        "* The relationship is encoded geometrically in the vector space.\n",
        "\n",
        "\n",
        "![Words to Embedding Vectors to 3D Visual (King-Queen : Man-Woman)](../assets/words_embeddings_visualized.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Embeddings Capture Syntactic, Semantic, and other Relationships\n",
        "\n",
        "When visualized in 2D, embeddings show clear structure:\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:678/1*5F4TXdFYwqi-BWTToQPIfg.jpeg\">\n",
        "\n",
        "**The model learns these dimensions automatically** from data—we don't specify what each dimension means. The relationships between vectors are what matter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "**Cosine Similarity** measures the angle between two vectors (word embeddings):\n",
        "\n",
        "- It ranges from -1 to 1\n",
        "- **1.0** = vectors point in the same direction (**~same**)\n",
        "- **0.0** = vectors are perpendicular (**no relationship / indifference**)\n",
        "- **-1.0** = vectors point in opposite directions (**~opposite**)\n",
        "\n",
        "![](../assets/cosine_similarity_3_cases.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison: Sparse vs. Dense Vectors\n",
        "\n",
        "| Aspect | Sparse (M1) | Dense (M2) |\n",
        "|--------|-------------|-----------|\n",
        "| **Dimensions** | 10K-100K (vocab size) | 100-768 (fixed) |\n",
        "| **Density** | ~1-2% non-zero | ~100% non-zero |\n",
        "| **Semantics** | No (words independent) | Yes (similar words close) |\n",
        "| **Memory** | Efficient (sparse storage) | More memory per vector |\n",
        "| **Interpretability** | High (see which words) | Low (dimensions abstract) |\n",
        "| **Learning** | Statistical (count/weight) | Neural (learned from data) |\n",
        "| **Use case** | Keyword search, simple classification | Semantic understanding, similarity |\n",
        "\n",
        "**Takeaway**: Dense embeddings enable semantic understanding that sparse vectors cannot provide. This is why modern NLP uses embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Static vs. Contextual Embeddings\n",
        "\n",
        "The final paradigm shift: understanding how word meaning changes with context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Polysemy Problem\n",
        "\n",
        "Many words have multiple meanings depending on context:\n",
        "\n",
        "**English examples**:\n",
        "- \"bank\": financial institution vs. river edge\n",
        "- \"bark\": tree covering vs. dog sound\n",
        "- \"bat\": flying mammal vs. sports equipment\n",
        "\n",
        "**Arabic examples**:\n",
        "- \"عين\" (ayn): eye vs. spring/water source\n",
        "- \"ساق\" (saq): leg vs. stem (of plant)\n",
        "- \"رأس\" (ra's): head vs. beginning/start\n",
        "\n",
        "**The challenge**: How do we represent words that mean different things in different contexts?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Static Embeddings (Early Deep Learning)\n",
        "\n",
        "**Static embeddings** (Word2Vec, GloVe):\n",
        "- One vector per word, regardless of context\n",
        "- \"bank\" always has the same vector\n",
        "- Learned from word co-occurrence patterns\n",
        "\n",
        "**Limitation**: \n",
        "- \"I deposited money at the **bank**\" → same vector as \"We sat by the river **bank**\"\n",
        "- The model can't distinguish between meanings!\n",
        "\n",
        "**When it works**: \n",
        "- When context doesn't matter much\n",
        "- For general semantic similarity\n",
        "- When words have consistent meanings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Static Embeddings Problem:\n",
            "  'bank' in 'I deposited money at the bank'\n",
            "  'bank' in 'We sat by the river bank'\n",
            " True → Same vector representation!\n"
          ]
        }
      ],
      "source": [
        "# Demonstrating static embeddings limitation\n",
        "\n",
        "# Simulated static embeddings (same vector for 'bank' regardless of context)\n",
        "static_embeddings = {\n",
        "    'bank_financial': [0.3, 0.1, 0.5, 0.2, ...],  # Simplified\n",
        "    'bank_river': [0.3, 0.1, 0.5, 0.2, ...],      # Same vector!\n",
        "}\n",
        "\n",
        "same = static_embeddings['bank_financial'] == static_embeddings['bank_river']\n",
        "\n",
        "print(\"Static Embeddings Problem:\")\n",
        "print(\"  'bank' in 'I deposited money at the bank'\")\n",
        "print(\"  'bank' in 'We sat by the river bank'\")\n",
        "print(f\" {same} → Same vector representation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Problem: Model can't distinguish between meanings\n",
        "- Solution: Contextual embeddings (next section)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contextual Embeddings (Modern Transformers)\n",
        "\n",
        "**Contextual embeddings** (BERT, GPT, modern transformers):\n",
        "- Vector changes based on the word's context in the sentence\n",
        "- \"bank\" in financial context → different vector than \"bank\" in river context\n",
        "- Learned by processing entire sentences through transformer layers\n",
        "\n",
        "**How it works**:\n",
        "1. Tokenize the sentence\n",
        "2. Pass through transformer layers (self-attention)\n",
        "3. Each token gets a vector that depends on all other tokens\n",
        "4. Same word, different contexts → different vectors!\n",
        "\n",
        "**Advantage**: \n",
        "- \"I deposited money at the **bank**\" → vector close to \"money\", \"deposit\", \"account\"\n",
        "- \"We sat by the river **bank**\" → vector close to \"river\", \"water\", \"shore\"\n",
        "- The model can distinguish meanings!\n",
        "\n",
        "![flat vector art diagram showing contextual embeddings where 'bank' has different vector representations in 'financial bank' context (near money/deposit vectors) vs 'river bank' context (near river/water vectors), minimalist style, clean lines, white background, soft blue and orange accent colors](../assets/contextual_embeddings_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../assets/contextual_embeddings_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison: Static vs. Contextual Embeddings\n",
        "\n",
        "| Aspect | Static (Word2Vec, GloVe) | Contextual (BERT, GPT) |\n",
        "|--------|-------------------------|----------------------|\n",
        "| **Vector per word** | One (fixed) | Many (depends on context) |\n",
        "| **Polysemy handling** | No (same vector) | Yes (different vectors) |\n",
        "| **Context awareness** | No | Yes (considers sentence) |\n",
        "| **Computational cost** | Low (lookup) | Higher (neural processing) |\n",
        "| **Use case** | General similarity, simple tasks | Understanding, complex tasks |\n",
        "| **When to use** | Fast semantic search, word similarity | Sentiment, translation, QA |\n",
        "\n",
        "**Takeaway**: Contextual embeddings are essential for understanding words with multiple meanings. This is why transformers (BERT, GPT) are so powerful!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Challenge of Semantic Shift\n",
        "\n",
        "Here's something important for real-world applications: **word meanings evolve**. This is called **semantic shift** or **semantic change**.\n",
        "\n",
        "**Why this matters**: If you train a model on data in many contexts, but leave out others that relate to your problem domain (like medicine, law, religion), it will misunderstand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Figure: How three words changed their meanings over time. Notice how their \"neighbors\" in semantic space shifted!](../../assets/semantic_shift.png)\n",
        "\n",
        "*Figure: How three words changed their meanings over time. Notice how their \"neighbors\" in semantic space shifted!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Case Study 1: The Evolution of \"Broadcast\" (English)\n",
        "\n",
        "Let's trace how one word's meaning changed over time. This shows how distributional semantics captures meaning change:\n",
        "\n",
        "**1850s: The Agricultural Era**\n",
        "- **Context**: \"The farmer broadcast seeds across the field\"\n",
        "- **Neighbors**: seed, sow, scatter, spread, field, harvest\n",
        "- **Meaning**: To cast seeds broadly by hand\n",
        "\n",
        "**1900s: The Metaphorical Shift**\n",
        "- **Context**: \"Newspapers broadcast news to the masses\"\n",
        "- **Neighbors**: news, information, circulate, distribute, media\n",
        "- **Meaning**: Still connected to \"scattering widely\", but now about information\n",
        "\n",
        "**1990s: The Modern Era**\n",
        "- **Context**: \"The BBC will broadcast the news at 6 PM\"\n",
        "- **Neighbors**: television, radio, network, program, channel\n",
        "- **Meaning**: Firmly attached to mass media\n",
        "\n",
        "**What happened?** The word's **distributional neighbors** changed. In 1850, \"broadcast\" appeared near farming words. By 1990, it appeared near media words. The meaning shifted because the context shifted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Case Study 2: The Evolution of \"هاتف\" (Hatif - Telephone) in Arabic\n",
        "\n",
        "Arabic also shows fascinating semantic shifts. Let's look at **\"هاتف\"** (hatif):\n",
        "\n",
        "**Classical Arabic (pre-20th century)**:\n",
        "- **Context**: \"سمعت هاتفاً ينادي\" (I heard a **voice from the unseen** calling)\n",
        "- **Neighbors**: صوت (voice), غيب (unseen), نداء (call), خفي (hidden)\n",
        "- **Meaning**: An invisible voice or caller (often in poetry/mystical contexts)\n",
        "\n",
        "**Early 20th Century (Technology Introduction)**:\n",
        "- **Context**: \"استخدمت الهاتف للاتصال\" (I used the **telephone** to call)\n",
        "- **Neighbors**: اتصال (connection), مكالمة (call), سلك (wire), جهاز (device)\n",
        "- **Meaning**: The new technology - telephone\n",
        "\n",
        "**Modern Arabic (21st century)**:\n",
        "- **Context**: \"هاتفي الذكي\" (my **smartphone**)\n",
        "- **Neighbors**: تطبيق (app), إنترنت (internet), شاشة (screen), ذكي (smart)\n",
        "- **Meaning**: Expanded to include all phone types, especially smartphones\n",
        "\n",
        "**What happened?** The word's **distributional neighbors** shifted from mystical/poetic contexts to technological contexts. The meaning evolved from \"invisible voice\" to \"telephone device\" to \"smartphone\"—a complete semantic transformation!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Case Study 3: The Evolution of \"شبكة\" (Shabaka - Network) in Arabic\n",
        "\n",
        "Another example: **\"شبكة\"** (shabaka):\n",
        "\n",
        "**Traditional meaning**:\n",
        "- **Context**: \"ألقى الصياد الشبكة في البحر\" (The fisherman cast the **net** into the sea)\n",
        "- **Neighbors**: صيد (fishing), بحر (sea), سمك (fish), خيط (thread)\n",
        "- **Meaning**: A fishing net\n",
        "\n",
        "**Modern meaning**:\n",
        "- **Context**: \"اتصلت بالشبكة\" (I connected to the **network**)\n",
        "- **Neighbors**: إنترنت (internet), اتصال (connection), واي فاي (WiFi), بيانات (data)\n",
        "- **Meaning**: Computer/internet network\n",
        "\n",
        "**The shift**: From physical net (fishing) to abstract network (technology). The word kept its core concept of \"interconnected structure\" but applied it to a completely different domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Tokenization Evolution**:\n",
        "   - Module 1: Word-level tokenization (simple, but fails on OOV words)\n",
        "   - Module 2: Subword tokenization (handles any text by breaking words into pieces)\n",
        "   - **Why**: Enables models to process any text, not just pre-seen words\n",
        "\n",
        "2. **Vectorization Evolution**:\n",
        "   - Module 1: Sparse vectors (BoW, TF-IDF) - high-dimensional, no semantics\n",
        "   - Module 2: Dense embeddings - low-dimensional, captures semantic meaning\n",
        "   - **Why**: Enables semantic understanding and similarity calculations\n",
        "\n",
        "3. **Static vs. Contextual Embeddings**:\n",
        "   - Static: One vector per word (fails on polysemy)\n",
        "   - Contextual: Vector depends on context (handles multiple meanings)\n",
        "   - **Why**: Enables understanding words with multiple meanings\n",
        "\n",
        "4. **The Big Picture**:\n",
        "   - These paradigm shifts enabled transformers and LLMs\n",
        "   - Each approach has its place (use the right tool for the job)\n",
        "   - Modern NLP combines multiple approaches for best results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "1. **Subword Tokenization**:\n",
        "   - Sennrich, R., et al. (2016). \"Neural Machine Translation of Rare Words with Subword Units.\" ACL.\n",
        "   - Devlin, J., et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" NAACL.\n",
        "\n",
        "2. **Word Embeddings**:\n",
        "   - Mikolov, T., et al. (2013). \"Efficient Estimation of Word Representations in Vector Space.\" ICLR.\n",
        "   - Pennington, J., et al. (2014). \"GloVe: Global Vectors for Word Representation.\" EMNLP.\n",
        "\n",
        "3. **Contextual Embeddings**:\n",
        "   - Devlin, J., et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" NAACL.\n",
        "   - Vaswani, A., et al. (2017). \"Attention Is All You Need.\" NIPS.\n",
        "\n",
        "4. **Distributional Hypothesis**:\n",
        "   - Firth, J.R. (1957). \"A Synopsis of Linguistic Theory.\" Studies in Linguistic Analysis."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
