{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: EMNIST Letter Detective\n",
    "\n",
    "You’ve already trained a neural network to recognize handwritten digits using the MNIST dataset. That gave you a clean, structured environment to get familiar with image classification.\n",
    "\n",
    "Now it’s time for something more challenging. \n",
    "You’re going to apply those same skills to a new dataset: EMNIST. It contains handwritten **letters** instead of digits. The data is messier and more varied, and there are 26 classes instead of 10, so your model will need to work a little harder to get things right.\n",
    "\n",
    "At the end of the lab, you’ll put your trained model to the test by decoding a handwritten message from Andrew Ng.\n",
    "\n",
    "\n",
    "**What You'll Do in This Assignment**\n",
    "\n",
    "* Load and explore the EMNIST Letters dataset to understand its structure and contents\n",
    "\n",
    "* Preprocess the images by fixing their orientation, normalizing pixel values, and converting them to tensors\n",
    "\n",
    "* Build a multi-layer neural network that can classify handwritten letters\n",
    "\n",
    "* Train and evaluate your model on unseen examples\n",
    "\n",
    "* Use your trained model to decode a secret handwritten message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='submission'></a>\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "* All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "* In each exercise cell, look for comments `### START CODE HERE ###` and `### END CODE HERE ###`. These show you where to write the solution code. **Do not add or change any code that is outside these comments**.\n",
    "\n",
    "* You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "* Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Imports](#imports)\n",
    "- [1. Letter Images](#1)\n",
    "  - [1.1 Load the Dataset](#1.1)\n",
    "  - [1.2 Explore the Raw Data](#1.2)\n",
    "  - [1.3 Preprocessing the Images](#1.3)\n",
    "  - [1.4 Loading Data in Batches](#1.4)\n",
    "    - **[Exercise 1 - create_emnist_dataloaders](#ex1)**   \n",
    "- [2. Building the Neural Network](#2)\n",
    "  - **[Exercise 2 - initialize_emnist_model](#ex2)**\n",
    "- [3. Training and Evaluation of the Model](#3)\n",
    "    - [3.1 Training the Model](#3.1)\n",
    "        - **[Exercise 3 - train_epoch](#ex3)**\n",
    "    - [3.2 Evaluation of the Model](#3.2)\n",
    "        - **[Exercise 4 - evaluate](#ex4)**\n",
    "    - [3.3 Putting it all together](#3.3)\n",
    "        - **[Exercise 5 - train_and_evaluate](#ex5)**\n",
    "- [4. Decoding the Secret Message](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='imports'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content', ['.config', 'Bootcamp-week4', 'sample_data'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd(), os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\hp\\Desktop\\Bootcamp-week4\\Bootcamp-week4\\W4\\C1_M2_PyTorch_Workflow\\C1M2_Assignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helper_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-237458029.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhelper_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munittests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helper_utils'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import helper_utils\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the `DEVICE` available is \"cuda\" or \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Letter Images\n",
    "\n",
    "You’ll start by loading the EMNIST dataset, focusing on the **Letters** subset.\n",
    "It extends MNIST by providing handwritten **letters** instead of digits.\n",
    "This subset includes **124,800 grayscale images** for training and **20,800 images** for testing, each labeled with one of **26 lowercase letter classes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "### 1.1 - Load the Dataset\n",
    "\n",
    "Before you can work with the EMNIST Letters data, you'll need to make sure it’s available in your workspace.\n",
    "If it’s not already there, you can download it using `torchvision.datasets.EMNIST`.\n",
    "You’ll be using the `letters` split of the EMNIST dataset, which includes handwritten characters from 'a' through 'z'.\n",
    "To select this subset, make sure to pass `split='letters'` to the `EMNIST` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Define the path where the EMNIST data will be stored\n",
    "data_path = './EMNIST_data'\n",
    "\n",
    "# Check if the data folder exists to avoid re-downloading\n",
    "if os.path.exists(data_path) and os.path.isdir(data_path):\n",
    "    download = False\n",
    "    print(\"EMNIST Data folder found locally. Loading from local.\\n\")\n",
    "else:\n",
    "    download = True\n",
    "    print(\"EMNIST Data folder not found locally. Downloading data.\\n\")\n",
    "\n",
    "\n",
    "# Load the EMNIST Letters training set\n",
    "train_dataset = datasets.EMNIST(\n",
    "    root=data_path,  # Specify the root directory for the dataset\n",
    "    split='letters',  # Use the 'letters' subset (26 lowercase classes)\n",
    "    train=True,  # Indicate that this is the training set\n",
    "    download=download  # Download the dataset if needed (based on the previous check)\n",
    ")\n",
    "\n",
    "test_dataset = datasets.EMNIST(\n",
    "    root=data_path,  # Specify the root directory for the dataset\n",
    "    split='letters',  # Use the 'letters' subset (26 lowercase classes)\n",
    "    train=False,  # Indicate that this is the test set\n",
    "    download=download  # Download the dataset if needed (based on the previous check)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 - Explore the Raw Data\n",
    "\n",
    "Now that your dataset is loaded, take a moment to see what you’re working with. Visualizing a few samples will help you understand how the letters are written and what challenges your model might face.\n",
    "\n",
    "Start by displaying a sample image using `helper_utils.visualize_image`. \n",
    "Then, try changing the index value to view different examples.\n",
    "\n",
    "As you explore, consider:\n",
    "* Do these letters look the way you’d expect?\n",
    "* Is there anything about their appearance that might make training or interpretation harder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "index = 90000  # Pick an index from the training set\n",
    "\n",
    "img, label = train_dataset[index]  # Extract the image and its label\n",
    "\n",
    "helper_utils.visualize_image(img, label)  # Display the image with its label\n",
    "\n",
    "# The image is stored as a PIL Image object. You'll convert it to a tensor before training.\n",
    "print(f\"\\n Image type: {type(img)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll probably notice that the handwritten letters appear rotated or flipped. Technically, a model can still learn from them as long as **every image is oriented the same way**. The model doesn’t “know” what properly oriented letter looks like; it just learns patterns that are consistent across the dataset.\n",
    "\n",
    "However, this can make it harder for *you* to visually inspect or interpret the samples.\n",
    "In the next step, you’ll use a helper function to display the letters in a more familiar orientation so they’re easier to look at. This correction is only for visualization. The underlying data will remain unchanged, and the model will still train on it exactly as provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "### 1.3 - Preprocessing the Images\n",
    "\n",
    "Before you can train your model, you need to convert the images into a format that PyTorch can work with, and apply normalization to help the model learn more effectively.\n",
    "\n",
    "To do this, you’ll use `transforms.Compose` to apply a sequence of preprocessing steps: \n",
    "* `transforms.ToTensor()` converts the image from a PIL format to a PyTorch tensor.\n",
    "* `transforms.Normalize(mean, std)` scales pixel values using precomputed mean and standard deviation values.\n",
    "\n",
    "Normalization keeps pixel values within a consistent range, which improves numerical stability and helps your model train faster and more reliably.\n",
    "\n",
    "These transformations are applied automatically **each time a sample is loaded from the dataset**, ensuring that all data passed to the model is standardized and ready for training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Precomputed mean and std for EMNIST Letters dataset\n",
    "mean = (0.1736,)\n",
    "std = (0.3317,)\n",
    "\n",
    "# Create a transform that converts images to tensors and normalizes them\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts images to PyTorch tensors and scales pixel values to [0, 1]\n",
    "    transforms.Normalize(mean=mean, std=std)   # Applies normalization using the computed mean and std\n",
    "])\n",
    "\n",
    "# Assign the transform to both the training and test datasets\n",
    "train_dataset.transform = transform\n",
    "test_dataset.transform = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take another look at the same sample. \n",
    "This time, you’re seeing the raw tensor after the transformations were applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Set the index of the sample image to view\n",
    "index = 90000\n",
    "\n",
    "# Retrieve the transformed image tensor and its label from the training set\n",
    "img_tensor, label = train_dataset[index]\n",
    "\n",
    "# Print the image tensor to see the raw values (after transformation)\n",
    "print(img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, each pixel value (originally between 0 and 255) was scaled to a range between 0 and 1. Then, those values were normalized by subtracting the dataset’s average pixel value and dividing by its standard deviation.\n",
    "\n",
    "That’s why the numbers you see now are centered around 0—some are negative, some positive. These values show how much brighter or darker each pixel is compared to the dataset average. This normalization step helps your model train faster and more reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can visualize one of the transformed images using the `helper_utils.visualize_image` function.\n",
    "\n",
    "Because the images in this dataset aren’t oriented correctly, you’ll first apply the `correct_image_orientation` function. This function uses transformations like `F.rotate` and `F.vflip` to display the letters in their proper orientation.\n",
    "\n",
    "Keep in mind that this step is **only for visualization**. It helps you interpret the images more easily. The dataset itself remains unchanged, and the model will still train on the original, uncorrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def correct_image_orientation(image):\n",
    "    rotated = F.rotate(image, 90) # Rotate the image 90 degrees clockwise\n",
    "    flipped = F.vflip(rotated) # Flip the image vertically\n",
    "    return flipped\n",
    "\n",
    "# Rotate the image and Reflect it\n",
    "img_transformed = correct_image_orientation(img_tensor)\n",
    "\n",
    "# Visualize the transformed image\n",
    "helper_utils.visualize_image(img_transformed, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.4'></a>\n",
    "### 1.4 - Loading Data in Batches\n",
    "\n",
    "Your neural network needs input in a structured, consistent format. To achieve that, you’ll use PyTorch’s `DataLoader` to organize your preprocessed letter images into batches—making training more efficient and easier to manage.\n",
    "\n",
    "In this step, you’ll complete the `create_emnist_dataloaders` to set up `DataLoader` objects for both training and test data. These loaders will automatically handle batching and shuffling, ensuring that your model trains effectively and generalizes well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex1'></a>\n",
    "### Exercise 1 - create_emnist_dataloaders\n",
    "\n",
    "Implement the `create_emnist_dataloaders` function to create PyTorch `DataLoader` objects for the EMNIST training and testing datasets.\n",
    "\n",
    "**Your task:**\n",
    "* Create `train_dataloader` and `test_dataloader` using the `DataLoader` class from `torch.utils.data`. \n",
    "    * Define `suffle` accordingly to each dataset.\n",
    "    * Use the `batch_size` parameter to set the number of samples per batch.\n",
    "\n",
    "<details>\n",
    "  <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "**For `train_dataloader`:**\n",
    "  * Use `shuffle=True` to ensure the training data is shuffled at every epoch.\n",
    "  * Set `batch_size` to the given `batch_size` parameter.\n",
    "  \n",
    "**For `test_dataloader`:**\n",
    "  * Use `shuffle=False` since you typically don't shuffle test data.\n",
    "  * Set `batch_size` to the given `batch_size` parameter.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_emnist_dataloaders\n",
    "\n",
    "def create_emnist_dataloaders(train_dataset, test_dataset, batch_size=64):\n",
    "    \"\"\"\n",
    "    Creates DataLoader objects for the EMNIST training and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        train_dataset (torch.utils.data.Dataset): The training dataset.\n",
    "        test_dataset (torch.utils.data.Dataset): The testing dataset.\n",
    "        batch_size (int, optional): The batch size for the DataLoaders. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        tuple (torch.utils.data.DataLoader, torch.utils.data.DataLoader):\n",
    "            A tuple containing the training and testing DataLoaders (train_loader, test_loader).\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Create a DataLoader for the training dataset\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        # Pass in the train_dataset\n",
    "        __BLANK__,\n",
    "        # Set batch_size\n",
    "        __BLANK__,\n",
    "        # Set shuffle\n",
    "        __BLANK__\n",
    "    )  \n",
    "\n",
    "    # Create a DataLoader for the testing dataset\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        # Pass in the test_dataset\n",
    "        __BLANK__,\n",
    "        # Set batch_size\n",
    "        __BLANK__,\n",
    "        # Set shuffle\n",
    "        __BLANK__\n",
    "    )  \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Return the created DataLoaders\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the contents of the resulting `DataLoader` objects. \n",
    "This will help you verify that the artifacts have been correctly organized into batches and that the shuffling has been applied to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_emnist_dataloaders(train_dataset, test_dataset, batch_size=64)\n",
    "\n",
    "print(\"--- Train Loader --- \\n\")\n",
    "helper_utils.display_data_loader_contents(train_loader)\n",
    "print(\"\\n--- Test Loader --- \\n\")\n",
    "helper_utils.display_data_loader_contents(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "```\n",
    "--- Train Loader --- \n",
    "\n",
    "Total number of images in dataset: 124800\n",
    "Total number of batches: 1950\n",
    "--- Batch 1 ---\n",
    "Data shape: torch.Size([64, 1, 28, 28])\n",
    "Labels shape: torch.Size([64])\n",
    "\n",
    "--- Test Loader --- \n",
    "\n",
    "Total number of images in dataset: 20800\n",
    "Total number of batches: 325\n",
    "--- Batch 1 ---\n",
    "Data shape: torch.Size([64, 1, 28, 28])\n",
    "Labels shape: torch.Size([64])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_1(create_emnist_dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Building the Neural Network\n",
    "\n",
    "Now that your data pipeline is ready, it’s time to build your neural network model.\n",
    "\n",
    "In this step, you’ll define a model that can learn to recognize and classify handwritten letters from the EMNIST dataset. You’ll also set up the loss function and optimizer that the model will use during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex2'></a>\n",
    "### Exercise 2 - initialize_emnist_model\n",
    "\n",
    "Implement the `initialize_emnist_model` function to create a PyTorch `Sequential` model for EMNIST classification.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "* **Define the Model Architecture**: \n",
    "  * Define a `model` using `nn.Sequential`.\n",
    "  * The **first layer must** be a flatten layer (`nn.Flatten()`).\n",
    "  * The middle layers, which you can design, **must only** consist of linear (`nn.Linear`) and ReLU (`nn.ReLU`) layers.\n",
    "  * The middle layers **must not exceed** 5 layers in total.\n",
    "  * The hidden unit size **must be less than or equal to** 256.\n",
    "  * The **final layer must** be a linear layer (`nn.Linear`) mapping features to `num_classes` outputs.\n",
    "  * The **total number of layers (including the first and final) must not exceed** 7.\n",
    "\n",
    "* **Define the Loss Function**:\n",
    "  * Define `loss_function` as **Cross Entropy Loss**.\n",
    "\n",
    "* **Define the Optimizer**:\n",
    "  * Define `optimizer` as **Adam** with a learning rate of 0.001. You should pass the model parameters (`model.parameters()`) to the optimizer.\n",
    "\n",
    "<details>\n",
    "  <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "  \n",
    "**For the Model:**\n",
    "* Remember to list your layers inside the `nn.Sequential()` constructor, separated by commas.\n",
    "* The `nn.Linear()` layer takes two main arguments: `in_features` and `out_features`. \n",
    "Ensure the `in_features` of one layer matches the `out_features` of the one before it.\n",
    "* The correct order of layers is: **Flatten -> Linear -> ReLU -> Linear -> ReLU -> ... -> Linear**.\n",
    "* The final layer should have `out_features` equal to `num_classes`, which is 26 for the EMNIST Letters dataset.\n",
    "\n",
    "**For the Loss Function:**\n",
    "* Use `nn.CrossEntropyLoss()` to define the loss function.\n",
    "\n",
    "**For the Optimizer:**\n",
    "* You will use `optim.Adam` as the optimizer. Its first argument should be the model parameters, which you can access using `model.parameters()`.\n",
    "* The learning rate can be set by passing `lr=0.001` as a keyword argument to the optimizer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_emnist_model\n",
    "\n",
    "def initialize_emnist_model(num_classes=26):\n",
    "    \"\"\"\n",
    "    Initializes a sequential neural network model for EMNIST classification.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): The number of output classes. Defaults to 26.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the model, loss function, and optimizer.\n",
    "               (model, loss_function, optimizer)\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(42)  # Set seed for reproducibility\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Define the neural network model using nn.Sequential\n",
    "    model = __BLANK__(\n",
    "        # Flatten the input images\n",
    "        __BLANK__,\n",
    "        # Middle part should not have more than 5 layers. Only Linear and ReLU layers.\n",
    "        # Note: Hidden layer units should not exceed 256\n",
    "        __BLANK__,\n",
    "        # Final Layer: SHOULD be Linear Layer\n",
    "        # Output layer with shape nn.Linear(valid inputs to the layer, num_classes outputs)\n",
    "        __BLANK__,\n",
    "    )\n",
    "\n",
    "    # Define the loss function (Cross-Entropy Loss for multi-class classification)\n",
    "    loss_function = __BLANK__\n",
    "\n",
    "    # Define the optimizer (Adaptive Moment Estimation)\n",
    "    optimizer = __BLANK__\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return model, loss_function, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the model definition.\n",
    "This will help you ensure that the model architecture, loss function, and optimizer are correctly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "your_model, loss_func, optimizer = initialize_emnist_model(num_classes=26)\n",
    "\n",
    "print(f\"Your model's architecture:\\n\\n{your_model}\\n\")\n",
    "print(f\"Your model's loss function: {loss_func}\\n\")\n",
    "print(f\"Your model's optimizer:\\n\\n{optimizer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output (Approximately):\n",
    "```\n",
    "Your model's architecture:\n",
    "\n",
    "Sequential(\n",
    "  (0): Flatten(start_dim=1, end_dim=-1)\n",
    "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
    "  (2): ReLU()\n",
    "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
    "  (4): ReLU()\n",
    "  (5): Linear(in_features=128, out_features=26, bias=True)\n",
    ")\n",
    "\n",
    "Your model's loss function: CrossEntropyLoss()\n",
    "\n",
    "Your model's optimizer:\n",
    "\n",
    "Adam (\n",
    "Parameter Group 0\n",
    "    amsgrad: False\n",
    "    betas: (0.9, 0.999)\n",
    "    capturable: False\n",
    "    decoupled_weight_decay: False\n",
    "    differentiable: False\n",
    "    eps: 1e-08\n",
    "    foreach: None\n",
    "    fused: None\n",
    "    lr: 0.001\n",
    "    maximize: False\n",
    "    weight_decay: 0\n",
    ")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_2(initialize_emnist_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Training and evaluation of the Model\n",
    "\n",
    "<a name='3.1'></a>\n",
    "### 3.1 - Training the Model\n",
    "\n",
    "The data pipeline is ready, and your model is built. Now it's time for **training**.\n",
    "\n",
    "This is where all the previous steps come together—allowing your model to learn from the training data.\n",
    "<a name='ex3'></a>\n",
    "### Exercise 3 - train_epoch\n",
    "\n",
    "Implement the `train_epoch` function to train a PyTorch model for one epoch and calculate the average loss and accuracy on the training set.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "* **Within the epoch training loop**:\n",
    "    * Zero the gradients of the optimizer.\n",
    "    * Fill `outputs` with the model's predictions for the current `inputs`.\n",
    "    * Calculate the `loss` using the `loss_function` with `outputs` and `targets`.\n",
    "    * Perform backpropagation with `loss.backward()` and update the model parameters using the `optimizer`.\n",
    "\n",
    "  * **For the calculation of loss and accuracy**:\n",
    "    * Obtain the current `loss_value` using `loss.item()` and accumulate it in `running_loss`.\n",
    "    * Obtain the `predicted_indices` by taking the `argmax` along dimension 1 of the `outputs`.\n",
    "    * Obtain the `correct_predictions` by comparing `predicted_indices` to `targets`.\n",
    "    * Get `num_correct_in_batch` by summing the correct predictions in the batch from `correct_predictions`.\n",
    "    * Update `num_correct_predictions` by adding `num_correct_in_batch`.\n",
    "    * Get the batch_size from the targets by using `targets.size(0)`.\n",
    "    * Add the current batch size to `total_predictions`.\n",
    "\n",
    "* **After all batches:**\n",
    "  * Calculate the average loss for the epoch by dividing `running_loss` by the number of batches.\n",
    "  * Calculate the accuracy percentage for the epoch.\n",
    "\n",
    "<details>\n",
    "  <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "* **Within the epoch training loop**:\n",
    "  * Use `optimizer.zero_grad()` to zero the gradients.\n",
    "  * Use `outputs = model(inputs)` to get the model's predictions.\n",
    "  * Use `optimizer.step()` to update the model parameters after backpropagation.\n",
    "\n",
    "* **For the calculation of loss and accuracy**:\n",
    "  * Use `outputs.argmax(dim=1)` to get the predicted indices.\n",
    "  * Use `predicted_indices.eq(targets)` to compare predicted indices to actual targets.\n",
    "  * Use `correct_predictions.sum().item()` to get the number of correct predictions in the batch.\n",
    "\n",
    "* **After all batches**:\n",
    "  * Use `running_loss / len(train_loader)` to calculate the average loss for the epoch\n",
    "  * Use `num_correct_predictions / total_predictions * 100` to calculate the accuracy percentage for the epoch.\n",
    "\n",
    "\n",
    "</details>\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loss_function, optimizer, train_loader, device, verbose=True):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch and calculates the average loss and accuracy.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to be trained.\n",
    "        loss_function (nn.Module): The loss function used for training.\n",
    "        optimizer (optim.Optimizer): The optimizer used for updating model parameters.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset, providing batches of data.\n",
    "        device (torch.device): The device (CPU or CUDA) where the model and data will be moved.\n",
    "        verbose (bool, optional): If True, prints the average loss and accuracy for the epoch. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - nn.Module: The trained model after one epoch.\n",
    "            - float: The average loss for the epoch.\n",
    "            - float: The accuracy percentage for the epoch.\n",
    "\n",
    "    Note:\n",
    "        - The target labels in the `train_loader` (EMNIST letters) are 1-indexed and so the function adjusts them\n",
    "          to 0-indexed before calculating the loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move the model to the specified device (CPU or GPU)\n",
    "    model.to(device)\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Initialize running loss to 0\n",
    "    running_loss = 0.0\n",
    "    # Initialize the number of correct predictions to 0\n",
    "    num_correct_predictions = 0\n",
    "    # Initialize the total number of predictions to 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Iterate over the batches of data from the training DataLoader\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move the inputs and targets to the specified device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Shift target labels down by 1 (adjusting for EMNIST letters dataset)\n",
    "        targets = targets - 1\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "        # Zero the gradients of the optimizer.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass the inputs through the model to get the outputs.\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss between the outputs and the targets.\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backpropagation to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters using the optimizer.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Loss \n",
    "        # Accumulate the loss value to running_loss\n",
    "        loss_value = loss.item()\n",
    "        # Add current loss value to the total running loss.\n",
    "        running_loss += loss_value\n",
    "\n",
    "        # Accuracy\n",
    "        # Get the predicted indices (by taking the argmax along dimension 1 of the outputs).\n",
    "        _, predicted_indices = torch.max(outputs, 1)\n",
    "\n",
    "        # Compare predicted indices to actual targets\n",
    "        correct_predictions = predicted_indices.eq(targets)\n",
    "\n",
    "        # Sum of correct predictions in the current batch.\n",
    "        num_correct_in_batch = correct_predictions.sum().item()\n",
    "        \n",
    "        # Add correct predictions to the total correct predictions.\n",
    "        num_correct_predictions += num_correct_in_batch\n",
    "\n",
    "        # Get the batch size from the targets and add it to total predictions.\n",
    "        batch_size = targets.size(0)\n",
    "        total_predictions += batch_size\n",
    "\n",
    "\n",
    "    # Calculate the average loss for the epoch. \n",
    "    # Divide the running loss by the number (len of train loader) of batches.\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Calculate the accuracy percentage for the epoch. Multiply correct predictions by 100.\n",
    "    accuracy_percentage = 100. * num_correct_predictions / total_predictions\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Conditionally print based on verbose flag\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Epoch Loss (Avg): {average_loss:.3f} | Epoch Acc: {accuracy_percentage:.2f}%\"\n",
    "        )\n",
    "\n",
    "    # Return the trained model and average loss\n",
    "    return model, average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the model's initial learning process.\n",
    "You'll execute the function for a single epoch and verify that the model is indeed training as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### This will take a few seconds to execute\n",
    "model, loss_function, optimizer = initialize_emnist_model(num_classes=26)\n",
    "\n",
    "model_one_train_epoch, _ = train_epoch(model=model, # model\n",
    "                                       loss_function=loss_function, # loss_function\n",
    "                                       optimizer=optimizer, # optimizer\n",
    "                                       train_loader=train_loader, # train_loader\n",
    "                                       device=DEVICE) # DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output (Approximately):\n",
    "```\n",
    "Epoch Loss (Avg): 0.602 | Epoch Acc: 81.40%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "\n",
    "### This will take a few seconds to execute\n",
    "unittests.exercise_3(train_epoch, model, loss_function, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Evaluation of the Model\n",
    "\n",
    "After training your model, it’s time to see how well it performs on data it hasn’t seen before. Evaluating on unseen data helps you measure how well the model generalizes to new handwritten samples.\n",
    "\n",
    "In this step, you’ll implement the `evaluate_model` function to assess your model’s performance on the test dataset. You’ll use **accuracy** as the evaluation metric to quantify how often the model’s predictions match the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex4'></a>\n",
    "### Exercise 4 - evaluate\n",
    "\n",
    "Implement the `evaluate` function to evaluate a PyTorch model on a test dataset and calculate the average accuracy.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "* Disable gradient calculation using `torch.no_grad()` during evaluation.\n",
    "\n",
    "* **For the loop over the batches in `test_loader`**:\n",
    "    * Fill `outputs` with the model's predictions for the current `inputs`.\n",
    "    * Obtain the `predicted_indices` by taking the `argmax` along dimension 1 of the `outputs`.\n",
    "    * Obtain the `correct_predictions` by comparing `predicted_indices` to `targets`.\n",
    "    * Get `num_correct_in_batch` by summing the correct predictions in the batch from `correct_predictions`.\n",
    "    * Update `num_correct_predictions` by adding `num_correct_in_batch`.\n",
    "    * Get the batch_size from the targets by using `targets.size(0)`.\n",
    "    * Add the current batch size to `total_predictions`.\n",
    "\n",
    "<details>\n",
    "  <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "* **For the loop over the batches in `test_loader`:**\n",
    "  * Use `outputs = model(inputs)` to get the model's predictions.\n",
    "  * Use `predicted_indices = outputs.argmax(dim=1)` to get the predicted indices.\n",
    "  * Use `correct_predictions = predicted_indices.eq(targets)` to compare predicted indices to actual targets.\n",
    "  * Use `num_correct_in_batch = correct_predictions.sum().item()` to get the number of correct predictions in the batch.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: evaluate\n",
    "\n",
    "def evaluate(model, test_loader, device, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and returns the accuracy percentage.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to be evaluated.\n",
    "        test_loader (DataLoader): DataLoader for the testing dataset.\n",
    "        device (torch.device): The device to use (CPU or CUDA).\n",
    "        verbose (bool, optional): If True, prints the test accuracy. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy percentage of the model on the test dataset.\n",
    "\n",
    "    Note:\n",
    "        - The target labels in the `test_loader` (EMNIST letters) are 1-indexed and so the function adjusts them \n",
    "          to 0-indexed before calculating the accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to evaluate mode\n",
    "    model.eval()\n",
    "    # Initialize the number of correct predictions to 0\n",
    "    num_correct_predictions = 0\n",
    "    # Initialize the total number of predictions to 0\n",
    "    total_predictions = 0 \n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Use torch.no_grad() context to disable gradient calculations during evaluation.\n",
    "    with __BLANK__:\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "        # Iterate over the batches of data from the test DataLoader\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move the inputs and targets to the specified device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Shift target labels down by 1 (adjusting for EMNIST letters dataset)\n",
    "            targets = targets - 1\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "            # Pass the inputs through the model to get the outputs.\n",
    "            outputs = __BLANK__\n",
    "\n",
    "            # Get the predicted indices (by taking the argmax along dimension 1 of the outputs).\n",
    "            predicted_indices = torch.argmax(outputs, 1)\n",
    "\n",
    "            # Compare predicted indices to actual targets\n",
    "            correct_predictions = predicted_indices.eq(targets)\n",
    "            \n",
    "            # Sum of correct predictions in the current batch.\n",
    "            num_correct_in_batch = correct_predictions.sum().item()\n",
    "            \n",
    "            # Add correct predictions to the total correct predictions.\n",
    "            num_correct_predictions += num_correct_in_batch\n",
    "\n",
    "            # Get the batch size from the targets and add it to total predictions.\n",
    "            batch_size = targets.size(0)\n",
    "            total_predictions += batch_size\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Calculate the accuracy percentage. Multiply correct predictions by 100.\n",
    "        accuracy_percentage = (num_correct_predictions / total_predictions) * 100\n",
    "\n",
    "    if verbose:  # Conditionally print based on verbose flag\n",
    "        print((f'Test Accuracy: {accuracy_percentage:.2f}%'))\n",
    "\n",
    "    return accuracy_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that your model is working as intended, start by evaluating it on the test dataset. This step confirms that your model has learned meaningful patterns from the training data and that your evaluation function is performing correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "model_evaluate = evaluate(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Expected Output (Approximately):\n",
    "\n",
    " ```\n",
    "\n",
    " Test Accuracy: 87.59%\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_4(evaluate, model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 - Putting it all together\n",
    "\n",
    "Now that you’ve implemented both the training and evaluation functions, it’s time to put everything together.\n",
    "In this step, you’ll use the previously defined `train_epoch` and `evaluate` functions to create a complete training and evaluation loop.\n",
    "\n",
    "<a name='ex5'></a>\n",
    "### Exercise 5 - train_and_evaluate\n",
    "\n",
    "Implement the `train_and_evaluate` function to train and evaluate a PyTorch model for a specified number of epochs.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "For each epoch in the range of `num_epochs`:\n",
    "\n",
    "* Train the model for that epoch using `train_epoch`. \n",
    "Pass the `model`, `train_loader`, `loss_function`, `optimizer` and `device` as arguments.\n",
    "\n",
    "* Evaluate the model using the `evaluate` function.\n",
    "Pass the `model`, `test_loader` and `device` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: training_loop\n",
    "\n",
    "def train_and_evaluate(\n",
    "    model, train_loader, test_loader, num_epochs, loss_function, optimizer, device\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the model for the specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to be trained.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        test_loader (DataLoader): DataLoader for the testing dataset.\n",
    "        num_epochs (int): The number of epochs to train for.\n",
    "        loss_function (nn.Module): The loss function used for training.\n",
    "        optimizer (optim.Optimizer): The optimizer used for updating model parameters.\n",
    "        device (torch.device): The device to use (CPU or CUDA).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch: {epoch+1}\")\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Train the model for one epoch using the train_epoch function.\n",
    "        trained_model, _ = __BLANK__(...)\n",
    "\n",
    "        # Evaluate the trained model on the test dataset using the evaluate function.\n",
    "        accuracy = __BLANK__(...)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Your Model\n",
    "\n",
    "That is it! You have successfully implemented the training and evaluation functions for your model. It is now time to train your model on the EMNIST dataset and evaluate its performance.\n",
    " \n",
    " Feel free to set the `num_epochs` to a value of your choice—just make sure it’s 15 or less to keep training manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL:\n",
    "\n",
    "# Set the number of training epochs. Don't set more than 15\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### This might take a minute or so to execute\n",
    "\n",
    "if num_epochs > 15 or num_epochs < 1:\n",
    "    print(\"\\033[91mSet num_epochs between 1 and 15 (inclusive).\")\n",
    "\n",
    "else:\n",
    "    # Initialize the EMNIST model, loss function, and optimizer\n",
    "    emnist_model, loss_function, optimizer = initialize_emnist_model(num_classes=26)\n",
    "    \n",
    "    # Train and evaluate the model for the specified number of epochs\n",
    "    trained_model = train_and_evaluate(\n",
    "        model=emnist_model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        device=DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output (Approximately for 10 Epochs):\n",
    "```\n",
    "Epoch: 1\n",
    "Epoch Loss (Avg): 0.600 | Epoch Acc: 81.48%\n",
    "Test Accuracy: 87.25%\n",
    "\n",
    "Epoch: 2\n",
    "Epoch Loss (Avg): 0.328 | Epoch Acc: 89.38%\n",
    "Test Accuracy: 89.60%\n",
    "\n",
    "Epoch: 3\n",
    "Epoch Loss (Avg): 0.272 | Epoch Acc: 90.88%\n",
    "Test Accuracy: 89.92%\n",
    "\n",
    "Epoch: 4\n",
    "Epoch Loss (Avg): 0.241 | Epoch Acc: 91.76%\n",
    "Test Accuracy: 89.86%\n",
    "\n",
    "Epoch: 5\n",
    "Epoch Loss (Avg): 0.215 | Epoch Acc: 92.59%\n",
    "Test Accuracy: 90.70%\n",
    "\n",
    "Epoch: 6\n",
    "Epoch Loss (Avg): 0.197 | Epoch Acc: 92.99%\n",
    "Test Accuracy: 90.46%\n",
    "\n",
    "Epoch: 7\n",
    "Epoch Loss (Avg): 0.181 | Epoch Acc: 93.47%\n",
    "Test Accuracy: 90.60%\n",
    "\n",
    "Epoch: 8\n",
    "Epoch Loss (Avg): 0.169 | Epoch Acc: 93.75%\n",
    "Test Accuracy: 90.39%\n",
    "\n",
    "Epoch: 9\n",
    "Epoch Loss (Avg): 0.160 | Epoch Acc: 94.13%\n",
    "Test Accuracy: 90.47%\n",
    "\n",
    "Epoch: 10\n",
    "Epoch Loss (Avg): 0.151 | Epoch Acc: 94.40%\n",
    "Test Accuracy: 90.57%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, it's time to evaluate its performance on the test dataset.\n",
    "You'll evaluate its accuracy *on each individual letter class* to identify any potential strengths or weaknesses in its analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained model's performance on each letter class\n",
    "class_accuracies = helper_utils.evaluate_per_class(trained_model, test_loader, DEVICE)\n",
    "\n",
    "# Print the accuracy for each letter class\n",
    "for letter, accuracy in class_accuracies.items():\n",
    "    print(f\"Accuracy for {letter}: {(accuracy*100):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_5(class_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Save your Trained Model\n",
    "\n",
    " <p style=\"background-color:#ffe6f0; color:#282828; padding:15px; border-width:3px; border-color:#d8b2c2; border-style:solid; border-radius:6px\">\n",
    "   🚨&nbsp;<b>IMPORTANT:</b> Please ensure the test in the cell above has passed successfully before proceeding. The following cell will save your trained model, which is essential for grading. This saved model, along with your assignment notebook, will be used in the overall grading of your assignment when you submit it. Failure to run the next cell will prevent your model from being saved, resulting in an error during assignment submission.<br><br>\n",
    "   🔄&nbsp;<b>Note:</b> You can run this cell multiple times to save your model. Each run will overwrite the previously saved version.\n",
    "\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### Run this cell to save your trained model\n",
    "helper_utils.save_student_model(model=trained_model, filename='trained_student_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Congratulations!\n",
    "\n",
    "Congratulations! You've completed the final graded exercise of this assignment.\n",
    "\n",
    "If you've successfully passed all the unit tests above, you've completed the core requirements of this assignment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4- Decoding the Secret Message\n",
    "\n",
    "You now have all the tools you need to decode the secret message.\n",
    "\n",
    "First, load the handwritten message images using `helper_utils.load_hidden_message_images`.\n",
    "Then, use your trained model to predict each letter and reconstruct the words in the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# get the images of the secret message\n",
    "message_imgs = helper_utils.load_hidden_message_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement the `decode_word_imgs` function to decode a list of word images into a string using the trained model.\n",
    "This function works similarly to `evaluate_model`, but instead of computing accuracy, it collects the model’s predicted characters and combines them to form words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def decode_word_imgs(word_imgs, model, device):\n",
    "    # put the model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    decoded_chars = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # iterate through each character image in the word\n",
    "        for char_img in word_imgs:\n",
    "            # add batch dimension and move to device\n",
    "            char_img = char_img.unsqueeze(0).to(device)\n",
    "\n",
    "            # predict the character with the model\n",
    "            output = model(char_img)\n",
    "            _, predicted = output.max(1)\n",
    "            predicted_label = predicted.item()\n",
    "\n",
    "            # match the predicted label to the corresponding character\n",
    "            lowercase_char = chr(ord(\"a\") + predicted_label)\n",
    "\n",
    "            # append the character to the decoded_chars list\n",
    "            decoded_chars.append(f\"{lowercase_char}\")\n",
    "\n",
    "    # join the characters to form the decoded word\n",
    "    decoded_word = \"\".join(decoded_chars)\n",
    "    return decoded_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are finally ready to decode the secret message! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "for sentence_imgs in message_imgs:\n",
    "    decoded_sentence = []\n",
    "\n",
    "    for word_imgs in sentence_imgs:\n",
    "        decoded_word = decode_word_imgs(word_imgs, trained_model, DEVICE)\n",
    "        decoded_sentence.append(decoded_word)\n",
    "\n",
    "    print(\" \".join(decoded_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the decoded message and see if you can read Andrew Ng’s handwritten note.\n",
    "\n",
    "Most letters should be correctly identified, though a few minor mistakes may appear—for example, the letter `d` being read as `a`, `l` as `i`, or `n` as `m` or `h`. These errors likely occur because of how similar the letters look when written quickly or unevenly.\n",
    "Indeed, given the `class_accuracies` observed during the evaluation phase (subsection 3.3), it is understandable that some letters may be misinterpreted.\n",
    "\n",
    "An interesting challenge would be to refine your model to better distinguish between such closely shaped characters!\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Original Message</font></b></summary>\n",
    "\n",
    "```\n",
    "Dear Laurence,\n",
    "Hope the PyTorch course is going well.\n",
    "Do not forget to keep the labs interesting and engaging.\n",
    "Maybe you could have the students try to decode my messy handwriting.\n",
    "That might be a bit too challenging though.\n",
    "I am impressed you are able to read this.\n",
    "```\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='conclusion'></a>\n",
    "## Conclusion\n",
    "\n",
    "Congratulations, you’ve completed the assignment and built your own letter-recognizing neural network!\n",
    "\n",
    "You’ve worked through every key stage of the deep learning workflow. You began by acquiring and preprocessing the EMNIST “Letters” dataset, calculating its mean and standard deviation, correcting image orientations, and normalizing pixel values. You then organized your data efficiently using PyTorch DataLoaders.\n",
    "\n",
    "Next, you designed and initialized a multi-layer neural network, selected an appropriate loss function and optimizer, and brought your model to life by implementing the training and evaluation loops. You watched its performance improve over epochs and finally put it to the test by decoding a secret message.\n",
    "\n",
    "The skills you’ve practiced here, from handling real image data and applying essential preprocessing steps to building, training, and evaluating neural networks, are foundational. You now have a solid grounding in PyTorch for tackling more advanced architectures, including convolutional neural networks for complex image classification tasks. Well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name='help'></a>\n",
    "\n",
    " ## Need more help with the model's architecture?\n",
    "\n",
    "\n",
    "\n",
    " Run the following cell to see an architecture that works well for the problem at hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# WE STRONGLY RECOMMEND YOU TO TRY YOUR OWN ARCHITECTURES FIRST\n",
    "# AND ONLY RUN THIS CELL IF YOU WISH TO SEE AN ANSWER\n",
    "import base64\n",
    "\n",
    "encoded_answer = \"LSBBIEZsYXR0ZW4gbGF5ZXIKLSBBIExpbmVhciBsYXllciB3aXRoIDc4NCBpbnB1dCBmZWF0dXJlcyBhbmQgMjU2IG91dHB1dCBmZWF0dXJlcwotIEEgUmVMVSBhY3RpdmF0aW9uIGZ1bmN0aW9uCi0gQSBMaW5lYXIgbGF5ZXIgd2l0aCAyNTYgaW5wdXQgZmVhdHVyZXMgYW5kIDEyOCBvdXRwdXQgZmVhdHVyZXMKLSBBIFJlTFUgYWN0aXZhdGlvbiBmdW5jdGlvbgotIEEgTGluZWFyIGxheWVyIHdpdGggMTI4IGlucHV0IGZlYXR1cmVzIGFuZCBudW1fY2xhc3Nlcz0yNiBvdXRwdXQgZmVhdHVyZXMKLSBUcmFpbiB0aGUgbW9kZWwgZm9yIDEwIG9yIG1vcmUgZXBvY2hz\"\n",
    "encoded_answer = encoded_answer.encode('ascii')\n",
    "answer = base64.b64decode(encoded_answer)\n",
    "answer = answer.decode('ascii')\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
