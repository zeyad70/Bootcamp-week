{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e178d8-8b4d-48c9-be61-f0eb4875fabd",
   "metadata": {},
   "source": [
    "# Building Your First Image Classifier\n",
    "\n",
    "Welcome to this hands-on lab where you'll build, train, and evaluate your first image classifier using PyTorch! You've already learned the core concepts behind the deep learning pipeline, and now it's time to put that knowledge into practice. Your goal now is to create a neural network that can recognize handwritten digits from the **MNIST dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6064353",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: '../../assets/mnist_dataset.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: '../../assets/mnist_dataset.png'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: '../../assets/mnist_dataset.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FMT_PNG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_jpeg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: '../../assets/mnist_dataset.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('../../assets/mnist_dataset.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cebf84",
   "metadata": {},
   "source": [
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this notebook, you will have gone through the entire end-to-end process. Specifically, you will:\n",
    "- **Prepare your data:** Load the MNIST dataset, inspect its format, and apply essential transformations.  \n",
    "- **Build your model:** Define a custom neural network using PyTorch's flexible `nn.Module` class.  \n",
    "- **Train your model:** Implement the full training process with a loss function, optimizer, and training loop.  \n",
    "- **Analyze your results:** Evaluate your model on unseen data and visualize its performance.\n",
    "\n",
    "This lab brings together everything you've learned so far: data preparation, model building, training, and evaluation. You'll see how all these pieces work together to create a working image classifier.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1920c-db69-412b-a3ff-00d5600d1b23",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5822c3a-b337-40d3-a01a-e9af8540aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50971f1-3ecb-447e-853b-c7eb34f9b3a1",
   "metadata": {},
   "source": [
    "* The code below selects the best available hardware on your system to speed up model training.\n",
    "    * **CUDA**: Runs on NVIDIA GPUs, which are widely used for deep learning and typically offer the fastest training performance. \n",
    "    * **MPS**: Runs on Apple Silicon GPUs, providing efficient acceleration on modern Mac systems.\n",
    "    * **CPU**: Runs on the Central Processing Unit, the standard processor every computer has. PyTorch will automatically use it if no compatible GPU is detected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d95bb749-e808-4142-9b58-42e17c40ac93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e00efc-e25b-4bf5-a017-039ffd37102c",
   "metadata": {},
   "source": [
    "## MNIST Dataset: Preparing your Data\n",
    "\n",
    "The [MNIST dataset](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html) is a classic benchmark for image classification and is often considered the \"hello world\" of computer vision. It contains 60,000 training images and 10,000 test images. Each image is 28 by 28 pixels, in grayscale, showing a single handwritten digit from 0 to 9.\n",
    "\n",
    "MNIST family (original sources):\n",
    "\n",
    "- [MNIST — LeCun’s page](https://yann.lecun.org/exdb/mnist/index.html). Canonical dataset description, formats, splits, and downloads for handwritten digits.\n",
    "\n",
    "- [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) — ZalandoResearch GitHub. A harder, drop-in MNIST replacement of clothing images with the same split structure.\n",
    "\n",
    "- [EMNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset) — NIST dataset page. MNIST extended to letters and digits, with multiple splits and download details.\n",
    "\n",
    "### Why Data Preparation Matters\n",
    "\n",
    "Before a model can learn from this data, you need to convert the images into numbers that a neural network can process. Each pixel becomes a numerical value representing its brightness, and together these numbers form a **tensor**, which is the format PyTorch models use for computation. \n",
    "\n",
    "**Neural networks are surprisingly picky.** They train much better when all inputs are small numbers, ideally centered around zero. This helps gradients flow more smoothly and makes learning more stable. To achieve this, you will **normalize** the pixel values so they fall within that range.\n",
    "\n",
    "### PyTorch's Data Pipeline\n",
    "\n",
    "PyTorch provides three core data utilities that work together to handle data efficiently:\n",
    "\n",
    "1. **Transforms**: Operations that run on each data point as it's loaded, preparing your data for the model\n",
    "2. **Dataset**: Fetches each sample from disk when it's asked to (doesn't preload everything at once)\n",
    "3. **DataLoader**: Serves the data in batches, making training on large datasets possible\n",
    "\n",
    "This pattern efficiently handles datasets that would otherwise overwhelm your computer's memory. Whether you're working with thousands of delivery records or millions of images, the principle stays the same: **load only what you need, when you need it.**\n",
    "\n",
    "### Inspecting Raw Data\n",
    "\n",
    "To see exactly what this means, you'll start by loading the MNIST training data directly from `torchvision`, PyTorch's library for computer vision tasks. You won't apply any transformations yet. Instead, you'll inspect a raw image to see what the data looks like before any changes, and later compare how that same image appears once it's been prepared for the model.\n",
    "\n",
    "- Define `data_path` that specifies the folder where your dataset will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963b1019-98c0-4727-9220-1b295c06b28a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the path to store the dataset files\n",
    "data_path = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41cd22-e50e-416e-b99e-3b09a3017062",
   "metadata": {},
   "source": [
    "* Load the MNIST dataset using `torchvision.datasets.MNIST`.\n",
    "    * `root`: This tells PyTorch where to save the dataset files. In this case, at the location of the `data_path` you just defined.\n",
    "    * `train`: Setting this to `True` ensures you get the training split of the dataset, which contains 60,000 images.\n",
    "    * `download`: This handy parameter tells PyTorch to automatically download the files if they are not already present in your root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a1ed6c-8d9d-4a19-81f0-2b906e0c3650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_without_transform = torchvision.datasets.MNIST(\n",
    "    root=data_path,     # Path to the directory where the data is/will be stored\n",
    "    train=True,         # Specify that you want the training split of the dataset\n",
    "    download=True       # Download the data if it's not found in the root directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597bb40-e3f7-4c78-964e-40469256ba13",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now that you've loaded the dataset, you can inspect an individual item from it.\n",
    "\n",
    "* You can retrieve any sample from your `Dataset` object just like a Python list by using its index. Here, you'll access the first item at index `0`.\n",
    "* Notice that each item is a **tuple** containing two parts: the image data and its corresponding numerical label.\n",
    "* After running the code, you should see the following:\n",
    "    * The image is a **PIL Image** object, a common Python format for image data.\n",
    "    * Its dimensions are **(28, 28)**, which matches the MNIST image size.\n",
    "    * The label is an integer representing the digit shown in the image.\n",
    "    \n",
    "> **A Note on Labels**: \n",
    ">\n",
    ">    Datasets in PyTorch return labels as **numerical indices**, not text. For the **MNIST dataset**, this is straightforward: index `0` represents the digit `0`, index `1` represents the digit `1`, and so on.\n",
    ">\n",
    ">    This relationship between label indices and their meanings is less direct in other datasets. For example, if you were working with images of cats and dogs, the labels would still be 0 and 1, not the words “cat” or “dog.” In those cases, you might create a list such as class_names = ['cat', 'dog']` to map the numeric labels back to readable names when displaying results or debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03661e45-886b-4a83-8301-4842786dcd2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the first sample (index 0), as a (image, label) tuple\n",
    "image_pil, label = train_dataset_without_transform[0] # Get the first image\n",
    "\n",
    "print(f\"Image type:        {type(image_pil)}\")\n",
    "# Since `image_pil` is a PIL Image object, its dimensions are accessed using the .size attribute.\n",
    "print(f\"Image Dimensions:  {image_pil.size}\")\n",
    "print(f\"Label Type:        {type(label)}\")\n",
    "print(f\"Label value:       {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019b0e1-7ad1-4868-990c-bd8d525ce472",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualizing a Raw Digit\n",
    "\n",
    "Now, you'll visualize the raw image you just loaded. The helper function `display_image` shows the image along with a grid of the numerical pixel values.\n",
    "\n",
    "* Run the code cell below to display the image.\n",
    "* Notice the color bar on the right, which shows the full range of possible brightness values for a pixel, from **0** to **255**. This is what the data looks like *before* any normalization.\n",
    "* Each number overlaid on the digit represents the brightness of that individual pixel. In a grayscale image, **0** corresponds to pure black, **255** to pure white, and values in between are shades of gray.\n",
    "    * You can try running the function again with `show_values=False` to see the image without the numerical grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b92880-c247-49a1-bb9a-995efd1fbb29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the sample image and its corresponding label\n",
    "helper_utils.display_image(image_pil, label, \"MNIST Digit (PIL Image)\", show_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f2221-002a-4758-b2e8-da95dd5ae39a",
   "metadata": {},
   "source": [
    "### The Effect of Transformations\n",
    "\n",
    "You've now seen the raw data: a standard **PIL image** with pixel values ranging from 0 to 255. Next, you'll prepare the data for the model by applying transformations and see how these steps change the image's structure and numerical values.\n",
    "\n",
    "#### Transformations\n",
    "\n",
    "You'll use [transforms.Compose](https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html) to chain together a sequence of preparation steps that are applied to each image as it's loaded. **Compose** just means to do the following things in order. For this task, you'll use two of the most common transformations:\n",
    "\n",
    "* <code>[transforms.ToTensor()](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html)</code>: Converts the PIL image into a PyTorch tensor and scales its pixel values from the original 0–255 range to values between 0 and 1. This is the first step in preparing your data for neural networks.\n",
    "\n",
    "* <code>[transforms.Normalize()](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html)</code>: Adjusts the tensor's values further by centering them around zero, using the mean and standard deviation calculated from the MNIST dataset. \n",
    "    * The values `0.1307` and `0.3081` are the mean and standard deviation of the entire MNIST training set. By normalizing every image with the same values, you make the data more consistent, and that helps the model learn faster and more reliably.\n",
    "    * This transform shifts and scales the values so they're centered around 0, which is exactly what neural networks need for optimal training.\n",
    "\n",
    "Together, these transforms convert your raw images into the format that neural networks expect: small numbers centered around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09504bef-9705-43f7-9fc1-d11bd7b5add5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert images to tensors and normalize pixel values\n",
    "# Pixel values are scaled to [0, 1] and then standardized to mean 0 and std 1\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb461aba-1bc9-4b31-a6b5-2c20cc0fb36a",
   "metadata": {},
   "source": [
    "Now you'll load the dataset again, this time applying the transformations you just defined. The code is nearly identical to before, with one important addition.\n",
    "\n",
    "* The `transform` argument is assigned the `transform` object you created earlier. This tells the `Dataset` to automatically apply your sequence of transformations (`ToTensor` and `Normalize`) to each image as it's loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34a24e-44ea-4e24-9ec2-0224add15ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=data_path,     # Path to the directory where the data is/will be stored\n",
    "    train=True,         # Specify that you want the training split of the dataset\n",
    "    download=True,      # Download the data if it's not found in the root directory\n",
    "    transform=transform # Apply the defined transformations to each image\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf0052-338e-4f53-b4b5-76d1ac962f55",
   "metadata": {},
   "source": [
    "Let's access the same item again, this time from the dataset with transformations applied. Notice the key differences in the output.\n",
    "\n",
    "* The image is no longer a PIL Image because the `ToTensor` transform has converted it into a `torch.Tensor`, the format PyTorch models require.\n",
    "* The shape is now `(1, 28, 28)`. PyTorch structures image tensors as `[channels, height, width]`. Since the images are grayscale, there is only one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21511e-ace8-4ab1-bc3f-0a06cb809f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access the first item again\n",
    "image_tensor, label = train_dataset[0]\n",
    "\n",
    "print(f\"Image Type:                   {type(image_tensor)}\")\n",
    "# Since the `image` is now a PyTorch Tensor, its dimensions are accessed using the .shape attribute.\n",
    "print(f\"Image Shape After Transform:  {image_tensor.shape}\")\n",
    "print(f\"Label Type:                   {type(label)}\")\n",
    "print(f\"Label value:                  {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e0704-d58a-4075-ae31-0307382473db",
   "metadata": {},
   "source": [
    "#### Visualizing a Transformed Digit\n",
    "\n",
    "You can now visualize that same digit again, but this time after the transformations have been applied.\n",
    "\n",
    "* Look closely at the pixel values. They are no longer in the 0-255 range. The Normalize transform has shifted these values so they are now **centered around zero**, giving a new range of approximately **-0.42** to **2.82**.\n",
    "* The color bar on the right confirms this change. Pixels that were originally dark (low values) now have negative numbers, while bright pixels have large positive values.\n",
    "    * You can try running the function again with `show_values=False` to view the image without the numerical grid.\n",
    "    \n",
    "This normalization process is a key step in preparing data for a neural network. It keeps the input values consistent and helps the model train more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea31b08-336f-4769-b969-ecbc423f5b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the transformed image and its label\n",
    "helper_utils.display_image(image_tensor, label, \"MNIST Digit (Tensor)\", show_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01578f33-3121-498f-b739-c7d41858d415",
   "metadata": {},
   "source": [
    "### Completing the Data Pipeline\n",
    "\n",
    "You’ve seen how transformations prepare your raw images for training. Now it’s time to complete the data pipeline by creating a test dataset and wrapping both datasets with a DataLoader.\n",
    "\n",
    "#### Load the Test Dataset\n",
    "\n",
    "In addition to the training data, you need a separate dataset for testing that the model has never seen before. This allows you to evaluate how well the model generalizes to new examples.\n",
    "\n",
    "* You'll load the test set using the same `torchvision.datasets.MNIST` function as before, but with one key difference: set `train=False` to specify that you want the 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db379a85-db1c-4af1-b72c-e66dd690d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=data_path,     # Path to the directory where the data is/will be stored\n",
    "    train=False,        # Specify that you want the testing split of the dataset\n",
    "    download=True,      # Download the data if it's not found in the root directory\n",
    "    transform=transform # Apply the defined transformations to each image\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba6c67-bb52-420d-8678-a2468af66a9f",
   "metadata": {},
   "source": [
    "#### Create Data Loaders\n",
    "\n",
    "The final step in your data pipeline is to create [DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders) instances. A `DataLoader` takes your `Dataset` and serves it in manageable chunks called **batches**. This is essential for training on large datasets, as it avoids loading all the data into memory at once.\n",
    "\n",
    "**Why Batching Matters**\n",
    "\n",
    "Working with data in batches is the practical solution for handling large datasets. Instead of trying to load all 60,000 images at once (which could overwhelm your computer's memory), the DataLoader requests one batch at a time from your dataset. This makes training on massive datasets possible.\n",
    "\n",
    "* **Training Loader**: For the training set, use a `batch_size` of 64 and set `shuffle=True`. \n",
    "    * **Why shuffle?** Datasets often come organized by class. If you don't shuffle, your model might see 6,000 zeros in a row before seeing any ones. It could learn unintended patterns like \"early batches are zeros, late batches are nines\" instead of actually learning what makes a zero look like a zero. Shuffling mixes everything up so that each batch has variety in it. The model learns the actual features of each digit and not just their position in the dataset.\n",
    "    * With `shuffle=True`, for every epoch the model will see the images in a different random order. This prevents the model from memorizing patterns in the dataset order and helps it generalize better.\n",
    "\n",
    "* **Test Loader**: For the test set, you can use a much larger batch size (e.g., 1000) since gradients are not calculated during evaluation. This saves memory and speeds up testing. \n",
    "    * You'll set `shuffle=False` because the model's done learning. You're just checking if it can recognize digits correctly. Order doesn't matter then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07defb-fdd6-45a9-9366-36c78ae4bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader for the training set with shuffling enabled\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create a data loader for the test set with a larger batch size and no shuffling\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ddba20-6b74-48f1-9177-e540b734cbcb",
   "metadata": {},
   "source": [
    "## Building the Neural Network Model\n",
    "\n",
    "With your data pipeline complete, it's time to define the neural network's architecture. You'll create a custom model by creating a class that inherits from PyTorch's [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). This pattern is flexible and gives you full control over the model's structure. You're going beyond `nn.Sequential` to build a custom architecture.\n",
    "\n",
    "Every `nn.Module` class has two essential parts:\n",
    "\n",
    "* The `__init__` method is where you define and initialize the layers your model will use, like gathering your tools before starting a job.\n",
    "* The `forward` method is where you define the exact path the data takes as it flows through those layers.\n",
    "\n",
    "### Define the Model\n",
    "\n",
    "Your model's architecture will include the following key components:\n",
    "\n",
    "* <code>[nn.Flatten()](https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.flatten.Flatten.html)</code>: This layer's job is to transform the 2D image data (28x28 pixels) into a 1D vector of 784 elements (28 * 28 = 784). \n",
    "    * **Why do you need Flatten?** MNIST images arrive as tensors with a specific shape. When PyTorch loads a single MNIST image, it gives you a `(1, 28, 28)`-dimensional tensor (channels, height, width). The one channel means it's grayscale—just a single brightness value for each pixel. But when you're training on batches, PyTorch adds another dimension. So with `batch_size=64`, your data arrives as `(64, 1, 28, 28)`—64 images, each with one channel, and each is 28 by 28 pixels.\n",
    "    * **The problem:** Linear layers expect flat vectors—one long row of numbers per image, not two-dimensional grids. That's what Flatten does. It takes each of your 28×28 images and reshapes them into 784 values in a row. So now your batch, instead of being dimension `(64, 1, 28, 28)`, becomes `(64, 784)`. Without Flatten, you get a shape mismatch error when your image data hits the linear layer.\n",
    "\n",
    "* <code>[nn.Linear()](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html)</code>: These are the core layers of your network. \n",
    "    * The first linear layer (`nn.Linear(784, 128)`) takes those 784 pixel values and transforms them to 128 hidden features.\n",
    "    * The second linear layer (`nn.Linear(128, 10)`) takes the 128 features and turns them into 10 outputs—one for each digit class (0-9).\n",
    "\n",
    "* <code>[nn.ReLU()](https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html)</code>: This is your activation function. It introduces non-linearity, enabling the model to learn complex patterns that simple linear transformations cannot capture. ReLU keeps positive values and zeros out the negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3d73d-1b04-4b75-8f9c-8728844f071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMNISTDNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple deep neural network model for the MNIST dataset.\n",
    "\n",
    "    This model consists of a flatten layer followed by two linear layers\n",
    "    with a ReLU activation function. It is designed for classification tasks\n",
    "    on 28x28 grayscale images.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the neural network.\n",
    "        \"\"\"\n",
    "        super(SimpleMNISTDNN, self).__init__()\n",
    "        # Initializes a layer to flatten the input tensor.\n",
    "        # 28x28 input image to a 784-dimensional vector.\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Initializes the sequential layers of the neural network\n",
    "        self.layers = nn.Sequential(\n",
    "            # Defines the first linear layer with 784 input features and 128 output features.\n",
    "            nn.Linear(784, 128),\n",
    "            # Applies the rectified linear unit activation function.\n",
    "            nn.ReLU(),\n",
    "            # Defines the second linear layer with 128 input features and 10 output features.\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            The output tensor after passing through the network layers.\n",
    "        \"\"\"\n",
    "        # Flattens the input tensor.\n",
    "        x = self.flatten(x)\n",
    "        # Passes the flattened tensor through the sequential layers.\n",
    "        x = self.layers(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0486c-c836-4dad-a5cc-30847399e7b5",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "You've prepared the data and defined the model architecture. However, at this point the model is untrained, so its predictions will be random guesses. If you run the model right now, you'll just be guessing.\n",
    "\n",
    "In this section, you'll bring the model to life by training it on the MNIST data. This involves setting up the necessary tools (a loss function and an optimizer), defining the training and evaluation logic, and running the main training loop.\n",
    "\n",
    "### Initialize Model, Loss Function, and Optimizer\n",
    "\n",
    "Before training, set up three essential components:\n",
    "\n",
    "* **The Model**: First, you'll create an instance of the SimpleMNISTDNN class you defined earlier. \n",
    "    * **Device Management**: It is essential for the model and the data to be on the same device to prevent errors. PyTorch will not move things around for you automatically. If your tensors and models aren't all on the same device, your code may not run and could crash with a device mismatch error. In this notebook, you'll handle this inside the training function itself. By moving both the model and the data to the `device` within that function, you guarantee they are correctly placed right before any computations happen. The key pattern is: choose your device up front, move the model once, and then move the data in every batch.\n",
    "\n",
    "* **The Loss Function**: You'll use <code>[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)</code> as your loss function. This is the standard choice for multi-class classification tasks like MNIST because it is specifically designed to measure the error when a model has to choose one class from several possibilities (i.e., one digit from 0-9). It's perfect for choosing a digit from 0 through 9.\n",
    "\n",
    "* **The Optimizer**: You'll use <code>[optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)</code>, a popular and highly effective optimizer. The optimizer's job is to update the model's weights to minimize the loss. `Adam` is known for adapting its learning rate as it trains by making larger adjustments early when gradients are noisy, and then smaller corrections later as your training stabilizes. This often helps the model learn more quickly and reliably than other optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1fde3-b62b-4d1c-86ef-9db7a574d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network model\n",
    "model = SimpleMNISTDNN()\n",
    "\n",
    "# Define the loss function, suitable for multi-class classification\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set up the Adam optimizer to update the model's parameters with a learning rate of 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23968c03-e9d7-491c-91c5-4fd89ab3481d",
   "metadata": {},
   "source": [
    "### Define the Training Function\n",
    "\n",
    "This function encapsulates all the logic for a **single training epoch**, which is one full pass through the dataset. Each iteration over a **single batch** of data within that epoch is called a **step**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e29c67",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../../assets/epoch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10fe4a",
   "metadata": {},
   "source": [
    "\n",
    "As a reminder, this function performs these key operations:\n",
    "\n",
    "* **Model Preparation**: Before iterating over the data, it prepares the model by moving it to the correct `device` and setting it to training mode with `model.train()`.\n",
    "\n",
    "* **Core Training Loop**: For each batch of data, it executes the essential five-step training sequence: clearing gradients, running a forward pass, calculating loss, performing backpropagation, and updating the model's weights.\n",
    "\n",
    "* **Progress Reporting**: Finally, it tracks the running loss and accuracy, printing periodic updates to monitor how well the model is learning.\n",
    "    * Specifically, with **60,000** training images and a batch size of **64**, the function will report its progress every **134 steps**, giving you **7** consistent updates throughout the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6704e-f32a-42a5-b6c8-28dc0b4b4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loss_function, optimizer, train_loader, device):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model for a single epoch.\n",
    "\n",
    "    This function iterates over the training dataset, performs the forward and\n",
    "    backward passes, and updates the model's weights. It also tracks and\n",
    "    prints the loss and accuracy at specified intervals.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to be trained.\n",
    "        loss_function: The loss function used to calculate the error.\n",
    "        optimizer: The optimizer used to update the model's weights.\n",
    "        train_loader: The DataLoader providing batches of training data.\n",
    "        device: The device (e.g., 'cuda' or 'cpu') on which to perform training.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - model: The model after training for one epoch.\n",
    "        - avg_epoch_loss: The average loss calculated over all batches in the epoch.\n",
    "    \"\"\"\n",
    "    # Ensure the model is on the correct device for training\n",
    "    model = model.to(device)\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize trackers for the entire epoch's loss\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Initialize trackers for periodic progress reporting\n",
    "    running_loss = 0.0\n",
    "    num_correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    # Iterate over the training data in batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move the current batch of data to the specified device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Clear any gradients from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform a forward pass to get model predictions\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate the loss for the current batch\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backpropagation to compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model's weights based on the computed gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss for tracking and reporting\n",
    "        loss_value = loss.item()\n",
    "        epoch_loss += loss_value\n",
    "        running_loss += loss_value\n",
    "        \n",
    "        # Calculate accuracy metrics for the current batch\n",
    "        _, predicted_indices = outputs.max(1)\n",
    "        batch_size = targets.size(0)\n",
    "        total_predictions += batch_size\n",
    "        num_correct_in_batch = predicted_indices.eq(targets).sum().item()\n",
    "        num_correct_predictions += num_correct_in_batch\n",
    "\n",
    "        # Check if it's time to print a progress update\n",
    "        if (batch_idx + 1) % 134 == 0 or (batch_idx + 1) == total_batches:\n",
    "            # Calculate average loss and accuracy for the current interval\n",
    "            avg_running_loss = running_loss / 134\n",
    "            accuracy = 100. * num_correct_predictions / total_predictions\n",
    "            \n",
    "            # Print the progress update\n",
    "            print(f'\\tStep {batch_idx + 1}/{total_batches} - Loss: {avg_running_loss:.3f} | Acc: {accuracy:.2f}%')\n",
    "            \n",
    "            # Reset the trackers for the next reporting interval\n",
    "            running_loss = 0.0\n",
    "            num_correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            \n",
    "    # Calculate the average loss for the entire epoch\n",
    "    avg_epoch_loss = epoch_loss / total_batches\n",
    "    # Return the trained model and the average epoch loss\n",
    "    return model, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32442558-5d1c-4c66-9e0b-ea02bae2662e",
   "metadata": {},
   "source": [
    "### Define the Evaluation Function\n",
    "\n",
    "After training your model, you need a way to measure its performance on data it has never seen before. While the training process shows how well the model is learning the training data, `evaluate` reveals whether it can generalize that learning to new examples. It's similar to the training loop but optimized for inference, with a few key differences:\n",
    "\n",
    "* **Setup for Inference**: The model is set to evaluation mode with `model.eval()` and gradient calculations are disabled using a `torch.no_grad()` block. \n",
    "    * `model.eval()` switches you into evaluation mode. This is important because some layers (like dropout or batch normalization) behave differently during training vs. evaluation.\n",
    "    * `torch.no_grad()` disables gradient calculation. During evaluation, you're not training, so you don't need gradients. This will save memory and speed everything up. These steps are essential for correct results and make the process faster and more memory-efficient.\n",
    "\n",
    "* **Simplified Forward Pass**: The loop is much simpler than in training. It only performs a forward pass to get the model's predictions and calculate accuracy. \n",
    "    * You run each batch through the model, find which class got the highest score with `output.max(1)`, count how many predictions match the true labels, and then return the accuracy percentage.\n",
    "    * There are no loss calculations, backpropagation, or weight updates with an optimizer. No optimizer, no loss tracking, no weight updates—just how many did you get right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54e0a1-af66-47e1-b2b8-d9094e4b68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model's accuracy on a test dataset.\n",
    "\n",
    "    This function sets the model to evaluation mode, iterates through the test data,\n",
    "    and calculates the percentage of correct predictions.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to be evaluated.\n",
    "        test_loader: A data loader containing the test dataset.\n",
    "        device: The device (e.g., 'cpu' or 'cuda') to run the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the model on the test dataset as a percentage.\n",
    "    \"\"\"\n",
    "    # Sets the model to evaluation mode.\n",
    "    model.eval()\n",
    "    # Initializes a counter for correct predictions.\n",
    "    num_correct_predictions = 0\n",
    "    # Initializes a counter for the total number of predictions.\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Disables gradient calculation to reduce memory usage and speed up computations.\n",
    "    with torch.no_grad():\n",
    "        # Iterates over all batches in the test data loader.\n",
    "        for inputs, targets in test_loader:\n",
    "            # Moves the input data and targets to the specified device.\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Performs a forward pass to get the model's output.\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Retrieves the index of the highest value in the output tensor, which represents the predicted class.\n",
    "            _, predicted_indices = outputs.max(1)\n",
    "            \n",
    "            # Gets the size of the current batch.\n",
    "            batch_size = targets.size(0)\n",
    "            # Adds the batch size to the total number of predictions.\n",
    "            total_predictions = total_predictions + batch_size\n",
    "            \n",
    "            # Compares the predicted indices with the actual target values.\n",
    "            correct_predictions = predicted_indices.eq(targets)\n",
    "            # Sums the correct predictions in the current batch.\n",
    "            num_correct_in_batch = correct_predictions.sum().item()\n",
    "            # Adds the correct predictions from the current batch to the total count.\n",
    "            num_correct_predictions = num_correct_predictions + num_correct_in_batch\n",
    "\n",
    "    # Calculates the overall accuracy as a percentage.\n",
    "    accuracy_percentage = (num_correct_predictions / total_predictions) * 100\n",
    "    # Prints the calculated accuracy to the console.\n",
    "    print((f'\\tAccuracy - {accuracy_percentage:.2f}%'))\n",
    "    \n",
    "    return accuracy_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0413b-dd56-4be6-a7df-8476371c66ce",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "\n",
    "This is where all the components you've built come together to train the model. The loop runs for a set number of epochs, where each epoch represents one complete pass through the entire dataset. Training for multiple epochs lets the model see the data repeatedly, gradually adjusting its internal weights to improve accuracy.\n",
    "\n",
    "**What to Expect During Training**\n",
    "\n",
    "With 60,000 training images and a batch size of 64, that's about 938 batches per epoch. You'll see progress updates throughout each epoch showing the loss and accuracy. Notice what's happening: the loss number drops (from around 0.64 to about 0.17 in a single epoch), while your accuracy climbs (from about 81% to about 95% in a single pass through the training set). And look how little code it took to make all of that happen!\n",
    "\n",
    "The logic inside each epoch is straightforward and follows a standard pattern:\n",
    "\n",
    "* First, you call the `train_epoch` function to train the model on all the training data. This performs the essential five-step training sequence: clearing gradients, running a forward pass, calculating loss, performing backpropagation, and updating the model's weights.\n",
    "\n",
    "* Immediately after, you call the `evaluate` function to measure the model's performance on the unseen test data. This is a vital step to check if the model is actually learning to **generalize** or if it's just memorizing the training set. After each training epoch, you evaluate on the test set to see how well it performs on unseen data. This tells you if the model is actually learning generalizable patterns or if it's just memorizing the training data.\n",
    "\n",
    "**When Training is Complete**\n",
    "\n",
    "By the end of training, you'll notice something really satisfying: the loss is tiny and the accuracy is high. When your accuracy stops improving, it's often a sign that your model is done learning for now, so you may not even need all the epochs you set.\n",
    "\n",
    "Finally, the loss and accuracy from each epoch are stored in lists so you can analyze the model's progress over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c786e-498e-4124-858b-1108cffd26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the total number of training epochs (Feel free to set a different number)\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize lists to store metrics from each epoch for later analysis\n",
    "train_loss = []\n",
    "test_acc = []\n",
    "\n",
    "# Begin the training and evaluation process\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\n[Training] Epoch {epoch+1}:')\n",
    "    # Call the training function to train the model for one epoch\n",
    "    trained_model, loss = train_epoch(model, loss_function, optimizer, train_loader, device)\n",
    "    # Store the average training loss for the epoch\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "    print(f'[Testing] Epoch {epoch+1}:')\n",
    "    # Call the evaluation function to measure performance on the test set\n",
    "    accuracy = evaluate(trained_model, test_loader, device)\n",
    "    # Store the test accuracy for the epoch\n",
    "    test_acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba17967-c9b9-45b2-ab05-ee5664c096c0",
   "metadata": {},
   "source": [
    "## Analyzing Model Performance\n",
    "\n",
    "With the training complete, the final step is to analyze your model's performance. You'll do this in two ways: **qualitatively**, by looking at some of its specific predictions, and **quantitatively**, by plotting its performance metrics over time.\n",
    "\n",
    "### Visualizing Predictions\n",
    "\n",
    "Now that the model is trained, let's see it in action. A great way to get a feel for its performance is to visualize the predictions it makes on random samples from the test set. This allows you to see concrete examples of where the model succeeds and where it might be making mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0bf6a9-f1bb-42a2-9cd2-0ed5ba4f68cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize model predictions on a random sample of test images\n",
    "helper_utils.display_predictions(trained_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd4add-96b1-4147-814d-42e7d038bbee",
   "metadata": {},
   "source": [
    "### Plotting Performance Metrics\n",
    "\n",
    "Looking at individual predictions can be helpful, but visualizing overall performance gives you a clearer view of how training progressed. Plotting the training loss and test accuracy across epochs helps you evaluate how effectively the model learned.\n",
    "\n",
    "* **Training Loss**: This plot should show a steady downward trend, indicating that the model is learning from the training data and reducing its error over time.\n",
    "\n",
    "* **Test Accuracy**: This plot should show an upward trend as the model improves its ability to generalize to new, unseen data. A rising test accuracy indicates that the model isn’t just memorizing the training set, but is actually learning patterns that transfer to new examples. When the curve begins to flatten, it often means the model has learned as much as it can from the current setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bebd2-27a9-44e4-bac7-16920ceecf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training loss and test accuracy curves over all epochs\n",
    "helper_utils.plot_metrics(train_loss, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cdf74-fc94-4e5d-9c88-77a5a60ffea6",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations on completing the lab! You have successfully navigated the entire machine learning pipeline to build, train, and analyze your very own image classifier.\n",
    "\n",
    "You started with raw image data and saw firsthand why **data preparation**, transforming and normalizing your inputs, is such a fundamental first step. You then built a custom neural network architecture, gaining practice with the power and flexibility of **`nn.Module`**.\n",
    "\n",
    "Most importantly, you implemented the **core training loop**, the engine that drives the learning process, and used a separate **evaluation function** to get an honest measure of your model's ability to generalize to new, unseen data. The final plots of loss and accuracy provided a clear, quantitative story of your model's journey from random guessing to accurate recognition.\n",
    "\n",
    "The skills you’ve practiced here form the foundation for tackling more advanced deep learning challenges ahead. Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
